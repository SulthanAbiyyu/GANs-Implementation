{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN\n",
    "**Guideline**:\n",
    "1. Pooling layer di replace dengan strided conv\n",
    "2. Pake batchnorm\n",
    "3. Menghilangkan FC hidden layer\n",
    "4. Generator pake ReLU, kecuali outputnya pake hyperbolic tan\n",
    "5. Discriminator pake LReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channel, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.d = nn.Sequential(\n",
    "            # INITIAL LAYER\n",
    "            # kernel size, stride, padding based on paper.\n",
    "            # batch norm diskip\n",
    "\n",
    "            # Input shape: n x channel x 64 x 64\n",
    "            nn.Conv2d(channel, features_d, kernel_size=4, stride=2, padding=1, bias=False),  # 32x32\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # Conv block layer\n",
    "            self._block(features_d, features_d*2, kernel_size=4, stride=2, padding=1), # 16x16\n",
    "            self._block(features_d*2, features_d*2*2, kernel_size=4, stride=2, padding=1), # 8x8\n",
    "            self._block(features_d*2*2, features_d*2*2*2, kernel_size=4, stride=2, padding=1), # 4x4\n",
    "            nn.Conv2d(features_d*2*2*2, 1, kernel_size=4, stride=1, padding=0, bias=False), # Real or fake\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # conv block\n",
    "    def _block(self, in_channels, out_channels,kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            # ga pake bias soalnya mau di batchnorm\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.d(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Ngelakuin kebalikannya dari Discriminator. Alih alih pake conv, kita pake conv transpose \n",
    "    karena kita ingin mengubah gambar menjadi gambar yang lebih besar (upscaling). Alih-alih \n",
    "    gak pake batchnorm waktu diinput, sekarang nggak pake batchnorm di output.\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, channel, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.g = nn.Sequential(\n",
    "            # INITIAL LAYER\n",
    "            # kernel size, stride, padding based on paper.\n",
    "            \n",
    "            # Input shape: n x z_dim x 1 x 1\n",
    "\n",
    "            self._block(z_dim, features_g*16, kernel_size=4, stride=1, padding=0), # 4x4\n",
    "            self._block(features_g*16, features_g*8, kernel_size=4,stride=2, padding=1),  # 8x8\n",
    "            self._block(features_g*8, features_g*4, kernel_size=4,stride=2, padding=1),  # 16x16\n",
    "            self._block(features_g*4, features_g*2, kernel_size=4,stride=2, padding=1),  # 32x32\n",
    "            nn.ConvTranspose2d(features_g*2, channel, kernel_size=4, stride=2, padding=1, bias=False), # 64x64\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels,kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            # ga pake bias soalnya mau di batchnorm\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU() # Sesuai dengan guideline\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.g(x)\n",
    "\n",
    "# weight initialization based on DCGAN paper\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 64\n",
    "CHANNEL = 3\n",
    "Z_DIM = 100\n",
    "EPOCH = 10\n",
    "FEATURES_D = 64\n",
    "FEATURES_G = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5 for _ in range(CHANNEL)],\n",
    "                         [0.5 for _ in range(CHANNEL)])\n",
    "])\n",
    "\n",
    "d = Discriminator(CHANNEL, FEATURES_D).to(DEVICE)\n",
    "g = Generator(Z_DIM, CHANNEL, FEATURES_G).to(DEVICE)\n",
    "initialize_weights(d)\n",
    "initialize_weights(g)\n",
    "\n",
    "dataset = datasets.ImageFolder(root='celeb_dataset', transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "d_optim = optim.Adam(d.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999)) # betas based on DCGAN paper\n",
    "g_optim = optim.Adam(g.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "criteon = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1, device=DEVICE)\n",
    "sum_writer_fake = SummaryWriter(f\"runs/DCGAN/fake_data\")\n",
    "sum_writer_real = SummaryWriter(f\"runs/DCGAN/real_data\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10] Batch 0/1583                       Loss D: 0.0843, loss G: 3.2678\n",
      "Epoch [0/10] Batch 1/1583                       Loss D: 0.0825, loss G: 3.2927\n",
      "Epoch [0/10] Batch 2/1583                       Loss D: 0.0848, loss G: 3.3240\n",
      "Epoch [0/10] Batch 3/1583                       Loss D: 0.0810, loss G: 3.3533\n",
      "Epoch [0/10] Batch 4/1583                       Loss D: 0.0779, loss G: 3.3829\n",
      "Epoch [0/10] Batch 5/1583                       Loss D: 0.0736, loss G: 3.4174\n",
      "Epoch [0/10] Batch 6/1583                       Loss D: 0.0713, loss G: 3.4447\n",
      "Epoch [0/10] Batch 7/1583                       Loss D: 0.0670, loss G: 3.4712\n",
      "Epoch [0/10] Batch 8/1583                       Loss D: 0.0656, loss G: 3.4949\n",
      "Epoch [0/10] Batch 9/1583                       Loss D: 0.0654, loss G: 3.5181\n",
      "Epoch [0/10] Batch 10/1583                       Loss D: 0.0628, loss G: 3.5387\n",
      "Epoch [0/10] Batch 11/1583                       Loss D: 0.0614, loss G: 3.5514\n",
      "Epoch [0/10] Batch 12/1583                       Loss D: 0.0624, loss G: 3.5736\n",
      "Epoch [0/10] Batch 13/1583                       Loss D: 0.0614, loss G: 3.5974\n",
      "Epoch [0/10] Batch 14/1583                       Loss D: 0.0593, loss G: 3.6118\n",
      "Epoch [0/10] Batch 15/1583                       Loss D: 0.0604, loss G: 3.6263\n",
      "Epoch [0/10] Batch 16/1583                       Loss D: 0.0636, loss G: 3.6423\n",
      "Epoch [0/10] Batch 17/1583                       Loss D: 0.0622, loss G: 3.6776\n",
      "Epoch [0/10] Batch 18/1583                       Loss D: 0.0569, loss G: 3.7249\n",
      "Epoch [0/10] Batch 19/1583                       Loss D: 0.0531, loss G: 3.7492\n",
      "Epoch [0/10] Batch 20/1583                       Loss D: 0.0511, loss G: 3.7587\n",
      "Epoch [0/10] Batch 21/1583                       Loss D: 0.0517, loss G: 3.7729\n",
      "Epoch [0/10] Batch 22/1583                       Loss D: 0.0488, loss G: 3.7872\n",
      "Epoch [0/10] Batch 23/1583                       Loss D: 0.0486, loss G: 3.8044\n",
      "Epoch [0/10] Batch 24/1583                       Loss D: 0.0479, loss G: 3.8267\n",
      "Epoch [0/10] Batch 25/1583                       Loss D: 0.0462, loss G: 3.8546\n",
      "Epoch [0/10] Batch 26/1583                       Loss D: 0.0470, loss G: 3.8845\n",
      "Epoch [0/10] Batch 27/1583                       Loss D: 0.0450, loss G: 3.8999\n",
      "Epoch [0/10] Batch 28/1583                       Loss D: 0.0476, loss G: 3.9227\n",
      "Epoch [0/10] Batch 29/1583                       Loss D: 0.0482, loss G: 3.9203\n",
      "Epoch [0/10] Batch 30/1583                       Loss D: 0.0588, loss G: 3.8930\n",
      "Epoch [0/10] Batch 31/1583                       Loss D: 0.1088, loss G: 3.4623\n",
      "Epoch [0/10] Batch 32/1583                       Loss D: 0.4495, loss G: 4.2047\n",
      "Epoch [0/10] Batch 33/1583                       Loss D: 0.6763, loss G: 4.1587\n",
      "Epoch [0/10] Batch 34/1583                       Loss D: 0.1331, loss G: 4.1141\n",
      "Epoch [0/10] Batch 35/1583                       Loss D: 0.0632, loss G: 4.0470\n",
      "Epoch [0/10] Batch 36/1583                       Loss D: 0.0517, loss G: 3.9392\n",
      "Epoch [0/10] Batch 37/1583                       Loss D: 0.0528, loss G: 3.7430\n",
      "Epoch [0/10] Batch 38/1583                       Loss D: 0.0657, loss G: 3.4837\n",
      "Epoch [0/10] Batch 39/1583                       Loss D: 0.0915, loss G: 3.3825\n",
      "Epoch [0/10] Batch 40/1583                       Loss D: 0.1053, loss G: 3.5182\n",
      "Epoch [0/10] Batch 41/1583                       Loss D: 0.0862, loss G: 3.6224\n",
      "Epoch [0/10] Batch 42/1583                       Loss D: 0.0900, loss G: 3.6342\n",
      "Epoch [0/10] Batch 43/1583                       Loss D: 0.1155, loss G: 3.7712\n",
      "Epoch [0/10] Batch 44/1583                       Loss D: 0.0987, loss G: 3.8996\n",
      "Epoch [0/10] Batch 45/1583                       Loss D: 0.0902, loss G: 3.9001\n",
      "Epoch [0/10] Batch 46/1583                       Loss D: 0.0793, loss G: 3.9300\n",
      "Epoch [0/10] Batch 47/1583                       Loss D: 0.0675, loss G: 4.0223\n",
      "Epoch [0/10] Batch 48/1583                       Loss D: 0.0591, loss G: 4.0586\n",
      "Epoch [0/10] Batch 49/1583                       Loss D: 0.0599, loss G: 4.0732\n",
      "Epoch [0/10] Batch 50/1583                       Loss D: 0.0665, loss G: 4.1412\n",
      "Epoch [0/10] Batch 51/1583                       Loss D: 0.0712, loss G: 4.1610\n",
      "Epoch [0/10] Batch 52/1583                       Loss D: 0.0568, loss G: 4.2370\n",
      "Epoch [0/10] Batch 53/1583                       Loss D: 0.0545, loss G: 4.2515\n",
      "Epoch [0/10] Batch 54/1583                       Loss D: 0.0529, loss G: 4.2291\n",
      "Epoch [0/10] Batch 55/1583                       Loss D: 0.0578, loss G: 4.2775\n",
      "Epoch [0/10] Batch 56/1583                       Loss D: 0.0533, loss G: 4.3669\n",
      "Epoch [0/10] Batch 57/1583                       Loss D: 0.0511, loss G: 4.3255\n",
      "Epoch [0/10] Batch 58/1583                       Loss D: 0.0499, loss G: 4.3429\n",
      "Epoch [0/10] Batch 59/1583                       Loss D: 0.0467, loss G: 4.3803\n",
      "Epoch [0/10] Batch 60/1583                       Loss D: 0.0418, loss G: 4.4001\n",
      "Epoch [0/10] Batch 61/1583                       Loss D: 0.0357, loss G: 4.4180\n",
      "Epoch [0/10] Batch 62/1583                       Loss D: 0.0369, loss G: 4.4014\n",
      "Epoch [0/10] Batch 63/1583                       Loss D: 0.0328, loss G: 4.4020\n",
      "Epoch [0/10] Batch 64/1583                       Loss D: 0.0356, loss G: 4.3950\n",
      "Epoch [0/10] Batch 65/1583                       Loss D: 0.0347, loss G: 4.4110\n",
      "Epoch [0/10] Batch 66/1583                       Loss D: 0.0335, loss G: 4.4356\n",
      "Epoch [0/10] Batch 67/1583                       Loss D: 0.0345, loss G: 4.4411\n",
      "Epoch [0/10] Batch 68/1583                       Loss D: 0.0340, loss G: 4.4500\n",
      "Epoch [0/10] Batch 69/1583                       Loss D: 0.0308, loss G: 4.5068\n",
      "Epoch [0/10] Batch 70/1583                       Loss D: 0.0316, loss G: 4.5440\n",
      "Epoch [0/10] Batch 71/1583                       Loss D: 0.0280, loss G: 4.5773\n",
      "Epoch [0/10] Batch 72/1583                       Loss D: 0.0296, loss G: 4.5844\n",
      "Epoch [0/10] Batch 73/1583                       Loss D: 0.0270, loss G: 4.5782\n",
      "Epoch [0/10] Batch 74/1583                       Loss D: 0.0285, loss G: 4.5469\n",
      "Epoch [0/10] Batch 75/1583                       Loss D: 0.0299, loss G: 4.5022\n",
      "Epoch [0/10] Batch 76/1583                       Loss D: 0.0287, loss G: 4.5096\n",
      "Epoch [0/10] Batch 77/1583                       Loss D: 0.0278, loss G: 4.5472\n",
      "Epoch [0/10] Batch 78/1583                       Loss D: 0.0292, loss G: 4.5582\n",
      "Epoch [0/10] Batch 79/1583                       Loss D: 0.0319, loss G: 4.5149\n",
      "Epoch [0/10] Batch 80/1583                       Loss D: 0.0298, loss G: 4.5179\n",
      "Epoch [0/10] Batch 81/1583                       Loss D: 0.0287, loss G: 4.5689\n",
      "Epoch [0/10] Batch 82/1583                       Loss D: 0.0257, loss G: 4.6287\n",
      "Epoch [0/10] Batch 83/1583                       Loss D: 0.0259, loss G: 4.6368\n",
      "Epoch [0/10] Batch 84/1583                       Loss D: 0.0274, loss G: 4.6005\n",
      "Epoch [0/10] Batch 85/1583                       Loss D: 0.0255, loss G: 4.5969\n",
      "Epoch [0/10] Batch 86/1583                       Loss D: 0.0264, loss G: 4.6182\n",
      "Epoch [0/10] Batch 87/1583                       Loss D: 0.0252, loss G: 4.6497\n",
      "Epoch [0/10] Batch 88/1583                       Loss D: 0.0261, loss G: 4.6614\n",
      "Epoch [0/10] Batch 89/1583                       Loss D: 0.0227, loss G: 4.7130\n",
      "Epoch [0/10] Batch 90/1583                       Loss D: 0.0212, loss G: 4.7705\n",
      "Epoch [0/10] Batch 91/1583                       Loss D: 0.0215, loss G: 4.8131\n",
      "Epoch [0/10] Batch 92/1583                       Loss D: 0.0200, loss G: 4.8033\n",
      "Epoch [0/10] Batch 93/1583                       Loss D: 0.0188, loss G: 4.7944\n",
      "Epoch [0/10] Batch 94/1583                       Loss D: 0.0185, loss G: 4.7976\n",
      "Epoch [0/10] Batch 95/1583                       Loss D: 0.0223, loss G: 4.7777\n",
      "Epoch [0/10] Batch 96/1583                       Loss D: 0.0216, loss G: 4.7753\n",
      "Epoch [0/10] Batch 97/1583                       Loss D: 0.0201, loss G: 4.8190\n",
      "Epoch [0/10] Batch 98/1583                       Loss D: 0.0210, loss G: 4.8609\n",
      "Epoch [0/10] Batch 99/1583                       Loss D: 0.0189, loss G: 4.8826\n",
      "Epoch [0/10] Batch 100/1583                       Loss D: 0.0177, loss G: 4.8853\n",
      "Epoch [0/10] Batch 101/1583                       Loss D: 0.0177, loss G: 4.8818\n",
      "Epoch [0/10] Batch 102/1583                       Loss D: 0.0164, loss G: 4.8742\n",
      "Epoch [0/10] Batch 103/1583                       Loss D: 0.0174, loss G: 4.8454\n",
      "Epoch [0/10] Batch 104/1583                       Loss D: 0.0191, loss G: 4.8152\n",
      "Epoch [0/10] Batch 105/1583                       Loss D: 0.0166, loss G: 4.8172\n",
      "Epoch [0/10] Batch 106/1583                       Loss D: 0.0171, loss G: 4.8364\n",
      "Epoch [0/10] Batch 107/1583                       Loss D: 0.0162, loss G: 4.8629\n",
      "Epoch [0/10] Batch 108/1583                       Loss D: 0.0161, loss G: 4.8763\n",
      "Epoch [0/10] Batch 109/1583                       Loss D: 0.0175, loss G: 4.8646\n",
      "Epoch [0/10] Batch 110/1583                       Loss D: 0.0170, loss G: 4.8446\n",
      "Epoch [0/10] Batch 111/1583                       Loss D: 0.0165, loss G: 4.8358\n",
      "Epoch [0/10] Batch 112/1583                       Loss D: 0.0172, loss G: 4.8315\n",
      "Epoch [0/10] Batch 113/1583                       Loss D: 0.0170, loss G: 4.8465\n",
      "Epoch [0/10] Batch 114/1583                       Loss D: 0.0172, loss G: 4.8913\n",
      "Epoch [0/10] Batch 115/1583                       Loss D: 0.0198, loss G: 4.9116\n",
      "Epoch [0/10] Batch 116/1583                       Loss D: 0.0196, loss G: 4.9918\n",
      "Epoch [0/10] Batch 117/1583                       Loss D: 0.0199, loss G: 5.0282\n",
      "Epoch [0/10] Batch 118/1583                       Loss D: 0.0173, loss G: 5.0869\n",
      "Epoch [0/10] Batch 119/1583                       Loss D: 0.0159, loss G: 5.1162\n",
      "Epoch [0/10] Batch 120/1583                       Loss D: 0.0151, loss G: 5.1217\n",
      "Epoch [0/10] Batch 121/1583                       Loss D: 0.0148, loss G: 5.1159\n",
      "Epoch [0/10] Batch 122/1583                       Loss D: 0.0135, loss G: 5.0976\n",
      "Epoch [0/10] Batch 123/1583                       Loss D: 0.0143, loss G: 5.0590\n",
      "Epoch [0/10] Batch 124/1583                       Loss D: 0.0152, loss G: 5.0192\n",
      "Epoch [0/10] Batch 125/1583                       Loss D: 0.0151, loss G: 5.0067\n",
      "Epoch [0/10] Batch 126/1583                       Loss D: 0.0157, loss G: 5.0147\n",
      "Epoch [0/10] Batch 127/1583                       Loss D: 0.0153, loss G: 5.0305\n",
      "Epoch [0/10] Batch 128/1583                       Loss D: 0.0156, loss G: 5.0349\n",
      "Epoch [0/10] Batch 129/1583                       Loss D: 0.0146, loss G: 5.0466\n",
      "Epoch [0/10] Batch 130/1583                       Loss D: 0.0132, loss G: 5.0777\n",
      "Epoch [0/10] Batch 131/1583                       Loss D: 0.0128, loss G: 5.0978\n",
      "Epoch [0/10] Batch 132/1583                       Loss D: 0.0133, loss G: 5.1035\n",
      "Epoch [0/10] Batch 133/1583                       Loss D: 0.0142, loss G: 5.0892\n",
      "Epoch [0/10] Batch 134/1583                       Loss D: 0.0141, loss G: 5.0679\n",
      "Epoch [0/10] Batch 135/1583                       Loss D: 0.0125, loss G: 5.0797\n",
      "Epoch [0/10] Batch 136/1583                       Loss D: 0.0141, loss G: 5.0791\n",
      "Epoch [0/10] Batch 137/1583                       Loss D: 0.0139, loss G: 5.0642\n",
      "Epoch [0/10] Batch 138/1583                       Loss D: 0.0143, loss G: 5.0309\n",
      "Epoch [0/10] Batch 139/1583                       Loss D: 0.0134, loss G: 5.0674\n",
      "Epoch [0/10] Batch 140/1583                       Loss D: 0.0136, loss G: 5.1144\n",
      "Epoch [0/10] Batch 141/1583                       Loss D: 0.0147, loss G: 5.0889\n",
      "Epoch [0/10] Batch 142/1583                       Loss D: 0.0131, loss G: 5.0920\n",
      "Epoch [0/10] Batch 143/1583                       Loss D: 0.0134, loss G: 5.0964\n",
      "Epoch [0/10] Batch 144/1583                       Loss D: 0.0143, loss G: 5.0767\n",
      "Epoch [0/10] Batch 145/1583                       Loss D: 0.0138, loss G: 5.0664\n",
      "Epoch [0/10] Batch 146/1583                       Loss D: 0.0141, loss G: 5.0746\n",
      "Epoch [0/10] Batch 147/1583                       Loss D: 0.0146, loss G: 5.0884\n",
      "Epoch [0/10] Batch 148/1583                       Loss D: 0.0139, loss G: 5.1380\n",
      "Epoch [0/10] Batch 149/1583                       Loss D: 0.0137, loss G: 5.1631\n",
      "Epoch [0/10] Batch 150/1583                       Loss D: 0.0139, loss G: 5.1553\n",
      "Epoch [0/10] Batch 151/1583                       Loss D: 0.0152, loss G: 5.0988\n",
      "Epoch [0/10] Batch 152/1583                       Loss D: 0.0146, loss G: 5.1149\n",
      "Epoch [0/10] Batch 153/1583                       Loss D: 0.0166, loss G: 5.0657\n",
      "Epoch [0/10] Batch 154/1583                       Loss D: 0.0163, loss G: 5.0372\n",
      "Epoch [0/10] Batch 155/1583                       Loss D: 0.0176, loss G: 5.0980\n",
      "Epoch [0/10] Batch 156/1583                       Loss D: 0.0202, loss G: 4.9407\n",
      "Epoch [0/10] Batch 157/1583                       Loss D: 0.0260, loss G: 4.9695\n",
      "Epoch [0/10] Batch 158/1583                       Loss D: 0.0176, loss G: 5.4536\n",
      "Epoch [0/10] Batch 159/1583                       Loss D: 0.0248, loss G: 4.9629\n",
      "Epoch [0/10] Batch 160/1583                       Loss D: 0.0280, loss G: 4.7965\n",
      "Epoch [0/10] Batch 161/1583                       Loss D: 0.0207, loss G: 5.7587\n",
      "Epoch [0/10] Batch 162/1583                       Loss D: 0.0102, loss G: 5.9189\n",
      "Epoch [0/10] Batch 163/1583                       Loss D: 0.0109, loss G: 5.8118\n",
      "Epoch [0/10] Batch 164/1583                       Loss D: 0.0091, loss G: 5.6471\n",
      "Epoch [0/10] Batch 165/1583                       Loss D: 0.0095, loss G: 5.4823\n",
      "Epoch [0/10] Batch 166/1583                       Loss D: 0.0082, loss G: 5.4500\n",
      "Epoch [0/10] Batch 167/1583                       Loss D: 0.0092, loss G: 5.4755\n",
      "Epoch [0/10] Batch 168/1583                       Loss D: 0.0088, loss G: 5.5276\n",
      "Epoch [0/10] Batch 169/1583                       Loss D: 0.0086, loss G: 5.5638\n",
      "Epoch [0/10] Batch 170/1583                       Loss D: 0.0087, loss G: 5.5623\n",
      "Epoch [0/10] Batch 171/1583                       Loss D: 0.0082, loss G: 5.5632\n",
      "Epoch [0/10] Batch 172/1583                       Loss D: 0.0083, loss G: 5.5579\n",
      "Epoch [0/10] Batch 173/1583                       Loss D: 0.0086, loss G: 5.5540\n",
      "Epoch [0/10] Batch 174/1583                       Loss D: 0.0083, loss G: 5.5682\n",
      "Epoch [0/10] Batch 175/1583                       Loss D: 0.0085, loss G: 5.5727\n",
      "Epoch [0/10] Batch 176/1583                       Loss D: 0.0080, loss G: 5.5882\n",
      "Epoch [0/10] Batch 177/1583                       Loss D: 0.0077, loss G: 5.6039\n",
      "Epoch [0/10] Batch 178/1583                       Loss D: 0.0079, loss G: 5.5927\n",
      "Epoch [0/10] Batch 179/1583                       Loss D: 0.0087, loss G: 5.5614\n",
      "Epoch [0/10] Batch 180/1583                       Loss D: 0.0083, loss G: 5.5285\n",
      "Epoch [0/10] Batch 181/1583                       Loss D: 0.0076, loss G: 5.5450\n",
      "Epoch [0/10] Batch 182/1583                       Loss D: 0.0095, loss G: 5.5157\n",
      "Epoch [0/10] Batch 183/1583                       Loss D: 0.0072, loss G: 5.5941\n",
      "Epoch [0/10] Batch 184/1583                       Loss D: 0.0091, loss G: 5.4826\n",
      "Epoch [0/10] Batch 185/1583                       Loss D: 0.0090, loss G: 5.4616\n",
      "Epoch [0/10] Batch 186/1583                       Loss D: 0.0095, loss G: 5.4922\n",
      "Epoch [0/10] Batch 187/1583                       Loss D: 0.0096, loss G: 5.4841\n",
      "Epoch [0/10] Batch 188/1583                       Loss D: 0.0099, loss G: 5.4811\n",
      "Epoch [0/10] Batch 189/1583                       Loss D: 0.0093, loss G: 5.5105\n",
      "Epoch [0/10] Batch 190/1583                       Loss D: 0.0090, loss G: 5.5507\n",
      "Epoch [0/10] Batch 191/1583                       Loss D: 0.0088, loss G: 5.5841\n",
      "Epoch [0/10] Batch 192/1583                       Loss D: 0.0082, loss G: 5.6307\n",
      "Epoch [0/10] Batch 193/1583                       Loss D: 0.0086, loss G: 5.6486\n",
      "Epoch [0/10] Batch 194/1583                       Loss D: 0.0083, loss G: 5.6579\n",
      "Epoch [0/10] Batch 195/1583                       Loss D: 0.0074, loss G: 5.6661\n",
      "Epoch [0/10] Batch 196/1583                       Loss D: 0.0069, loss G: 5.6869\n",
      "Epoch [0/10] Batch 197/1583                       Loss D: 0.0082, loss G: 5.6681\n",
      "Epoch [0/10] Batch 198/1583                       Loss D: 0.0073, loss G: 5.6828\n",
      "Epoch [0/10] Batch 199/1583                       Loss D: 0.0070, loss G: 5.7167\n",
      "Epoch [0/10] Batch 200/1583                       Loss D: 0.0068, loss G: 5.7493\n",
      "Epoch [0/10] Batch 201/1583                       Loss D: 0.0070, loss G: 5.7611\n",
      "Epoch [0/10] Batch 202/1583                       Loss D: 0.0077, loss G: 5.7193\n",
      "Epoch [0/10] Batch 203/1583                       Loss D: 0.0065, loss G: 5.7203\n",
      "Epoch [0/10] Batch 204/1583                       Loss D: 0.0073, loss G: 5.7204\n",
      "Epoch [0/10] Batch 205/1583                       Loss D: 0.0074, loss G: 5.7033\n",
      "Epoch [0/10] Batch 206/1583                       Loss D: 0.0070, loss G: 5.6974\n",
      "Epoch [0/10] Batch 207/1583                       Loss D: 0.0063, loss G: 5.7274\n",
      "Epoch [0/10] Batch 208/1583                       Loss D: 0.0064, loss G: 5.7518\n",
      "Epoch [0/10] Batch 209/1583                       Loss D: 0.0073, loss G: 5.7222\n",
      "Epoch [0/10] Batch 210/1583                       Loss D: 0.0074, loss G: 5.6812\n",
      "Epoch [0/10] Batch 211/1583                       Loss D: 0.0063, loss G: 5.7006\n",
      "Epoch [0/10] Batch 212/1583                       Loss D: 0.0060, loss G: 5.7473\n",
      "Epoch [0/10] Batch 213/1583                       Loss D: 0.0069, loss G: 5.7318\n",
      "Epoch [0/10] Batch 214/1583                       Loss D: 0.0067, loss G: 5.7198\n",
      "Epoch [0/10] Batch 215/1583                       Loss D: 0.0063, loss G: 5.7313\n",
      "Epoch [0/10] Batch 216/1583                       Loss D: 0.0061, loss G: 5.7539\n",
      "Epoch [0/10] Batch 217/1583                       Loss D: 0.0064, loss G: 5.7613\n",
      "Epoch [0/10] Batch 218/1583                       Loss D: 0.0060, loss G: 5.7833\n",
      "Epoch [0/10] Batch 219/1583                       Loss D: 0.0064, loss G: 5.7827\n",
      "Epoch [0/10] Batch 220/1583                       Loss D: 0.0057, loss G: 5.8087\n",
      "Epoch [0/10] Batch 221/1583                       Loss D: 0.0057, loss G: 5.8263\n",
      "Epoch [0/10] Batch 222/1583                       Loss D: 0.0058, loss G: 5.8295\n",
      "Epoch [0/10] Batch 223/1583                       Loss D: 0.0060, loss G: 5.8190\n",
      "Epoch [0/10] Batch 224/1583                       Loss D: 0.0060, loss G: 5.8303\n",
      "Epoch [0/10] Batch 225/1583                       Loss D: 0.0054, loss G: 5.8635\n",
      "Epoch [0/10] Batch 226/1583                       Loss D: 0.0057, loss G: 5.8906\n",
      "Epoch [0/10] Batch 227/1583                       Loss D: 0.0061, loss G: 5.8834\n",
      "Epoch [0/10] Batch 228/1583                       Loss D: 0.0051, loss G: 5.8960\n",
      "Epoch [0/10] Batch 229/1583                       Loss D: 0.0055, loss G: 5.8995\n",
      "Epoch [0/10] Batch 230/1583                       Loss D: 0.0053, loss G: 5.8907\n",
      "Epoch [0/10] Batch 231/1583                       Loss D: 0.0049, loss G: 5.9102\n",
      "Epoch [0/10] Batch 232/1583                       Loss D: 0.0050, loss G: 5.9378\n",
      "Epoch [0/10] Batch 233/1583                       Loss D: 0.0051, loss G: 5.9562\n",
      "Epoch [0/10] Batch 234/1583                       Loss D: 0.0047, loss G: 5.9746\n",
      "Epoch [0/10] Batch 235/1583                       Loss D: 0.0050, loss G: 5.9705\n",
      "Epoch [0/10] Batch 236/1583                       Loss D: 0.0051, loss G: 5.9629\n",
      "Epoch [0/10] Batch 237/1583                       Loss D: 0.0055, loss G: 5.9521\n",
      "Epoch [0/10] Batch 238/1583                       Loss D: 0.0048, loss G: 5.9576\n",
      "Epoch [0/10] Batch 239/1583                       Loss D: 0.0047, loss G: 5.9830\n",
      "Epoch [0/10] Batch 240/1583                       Loss D: 0.0050, loss G: 5.9524\n",
      "Epoch [0/10] Batch 241/1583                       Loss D: 0.0048, loss G: 5.9538\n",
      "Epoch [0/10] Batch 242/1583                       Loss D: 0.0050, loss G: 5.9530\n",
      "Epoch [0/10] Batch 243/1583                       Loss D: 0.0050, loss G: 5.9598\n",
      "Epoch [0/10] Batch 244/1583                       Loss D: 0.0051, loss G: 5.9471\n",
      "Epoch [0/10] Batch 245/1583                       Loss D: 0.0048, loss G: 5.9532\n",
      "Epoch [0/10] Batch 246/1583                       Loss D: 0.0046, loss G: 5.9820\n",
      "Epoch [0/10] Batch 247/1583                       Loss D: 0.0047, loss G: 6.0055\n",
      "Epoch [0/10] Batch 248/1583                       Loss D: 0.0044, loss G: 6.0390\n",
      "Epoch [0/10] Batch 249/1583                       Loss D: 0.0044, loss G: 6.0545\n",
      "Epoch [0/10] Batch 250/1583                       Loss D: 0.0042, loss G: 6.0620\n",
      "Epoch [0/10] Batch 251/1583                       Loss D: 0.0040, loss G: 6.0813\n",
      "Epoch [0/10] Batch 252/1583                       Loss D: 0.0041, loss G: 6.0957\n",
      "Epoch [0/10] Batch 253/1583                       Loss D: 0.0042, loss G: 6.1018\n",
      "Epoch [0/10] Batch 254/1583                       Loss D: 0.0045, loss G: 6.0843\n",
      "Epoch [0/10] Batch 255/1583                       Loss D: 0.0041, loss G: 6.0826\n",
      "Epoch [0/10] Batch 256/1583                       Loss D: 0.0048, loss G: 6.0706\n",
      "Epoch [0/10] Batch 257/1583                       Loss D: 0.0043, loss G: 6.0850\n",
      "Epoch [0/10] Batch 258/1583                       Loss D: 0.0038, loss G: 6.1358\n",
      "Epoch [0/10] Batch 259/1583                       Loss D: 0.0041, loss G: 6.1222\n",
      "Epoch [0/10] Batch 260/1583                       Loss D: 0.0038, loss G: 6.1178\n",
      "Epoch [0/10] Batch 261/1583                       Loss D: 0.0041, loss G: 6.1189\n",
      "Epoch [0/10] Batch 262/1583                       Loss D: 0.0039, loss G: 6.1349\n",
      "Epoch [0/10] Batch 263/1583                       Loss D: 0.0038, loss G: 6.1543\n",
      "Epoch [0/10] Batch 264/1583                       Loss D: 0.0037, loss G: 6.1759\n",
      "Epoch [0/10] Batch 265/1583                       Loss D: 0.0040, loss G: 6.1828\n",
      "Epoch [0/10] Batch 266/1583                       Loss D: 0.0039, loss G: 6.1794\n",
      "Epoch [0/10] Batch 267/1583                       Loss D: 0.0039, loss G: 6.1777\n",
      "Epoch [0/10] Batch 268/1583                       Loss D: 0.0037, loss G: 6.1880\n",
      "Epoch [0/10] Batch 269/1583                       Loss D: 0.0035, loss G: 6.2009\n",
      "Epoch [0/10] Batch 270/1583                       Loss D: 0.0034, loss G: 6.2234\n",
      "Epoch [0/10] Batch 271/1583                       Loss D: 0.0037, loss G: 6.2339\n",
      "Epoch [0/10] Batch 272/1583                       Loss D: 0.0035, loss G: 6.2315\n",
      "Epoch [0/10] Batch 273/1583                       Loss D: 0.0033, loss G: 6.2302\n",
      "Epoch [0/10] Batch 274/1583                       Loss D: 0.0037, loss G: 6.2277\n",
      "Epoch [0/10] Batch 275/1583                       Loss D: 0.0035, loss G: 6.2323\n",
      "Epoch [0/10] Batch 276/1583                       Loss D: 0.0036, loss G: 6.2352\n",
      "Epoch [0/10] Batch 277/1583                       Loss D: 0.0035, loss G: 6.2352\n",
      "Epoch [0/10] Batch 278/1583                       Loss D: 0.0034, loss G: 6.2371\n",
      "Epoch [0/10] Batch 279/1583                       Loss D: 0.0033, loss G: 6.2368\n",
      "Epoch [0/10] Batch 280/1583                       Loss D: 0.0036, loss G: 6.2304\n",
      "Epoch [0/10] Batch 281/1583                       Loss D: 0.0039, loss G: 6.2202\n",
      "Epoch [0/10] Batch 282/1583                       Loss D: 0.0034, loss G: 6.2226\n",
      "Epoch [0/10] Batch 283/1583                       Loss D: 0.0033, loss G: 6.2260\n",
      "Epoch [0/10] Batch 284/1583                       Loss D: 0.0035, loss G: 6.2338\n",
      "Epoch [0/10] Batch 285/1583                       Loss D: 0.0033, loss G: 6.2344\n",
      "Epoch [0/10] Batch 286/1583                       Loss D: 0.0038, loss G: 6.2236\n",
      "Epoch [0/10] Batch 287/1583                       Loss D: 0.0036, loss G: 6.2110\n",
      "Epoch [0/10] Batch 288/1583                       Loss D: 0.0034, loss G: 6.2173\n",
      "Epoch [0/10] Batch 289/1583                       Loss D: 0.0034, loss G: 6.2296\n",
      "Epoch [0/10] Batch 290/1583                       Loss D: 0.0034, loss G: 6.2474\n",
      "Epoch [0/10] Batch 291/1583                       Loss D: 0.0033, loss G: 6.2674\n",
      "Epoch [0/10] Batch 292/1583                       Loss D: 0.0034, loss G: 6.2802\n",
      "Epoch [0/10] Batch 293/1583                       Loss D: 0.0031, loss G: 6.2986\n",
      "Epoch [0/10] Batch 294/1583                       Loss D: 0.0031, loss G: 6.3184\n",
      "Epoch [0/10] Batch 295/1583                       Loss D: 0.0031, loss G: 6.3270\n",
      "Epoch [0/10] Batch 296/1583                       Loss D: 0.0030, loss G: 6.3396\n",
      "Epoch [0/10] Batch 297/1583                       Loss D: 0.0031, loss G: 6.3555\n",
      "Epoch [0/10] Batch 298/1583                       Loss D: 0.0030, loss G: 6.3746\n",
      "Epoch [0/10] Batch 299/1583                       Loss D: 0.0030, loss G: 6.3956\n",
      "Epoch [0/10] Batch 300/1583                       Loss D: 0.0028, loss G: 6.4204\n",
      "Epoch [0/10] Batch 301/1583                       Loss D: 0.0027, loss G: 6.4400\n",
      "Epoch [0/10] Batch 302/1583                       Loss D: 0.0028, loss G: 6.4526\n",
      "Epoch [0/10] Batch 303/1583                       Loss D: 0.0028, loss G: 6.4607\n",
      "Epoch [0/10] Batch 304/1583                       Loss D: 0.0028, loss G: 6.4761\n",
      "Epoch [0/10] Batch 305/1583                       Loss D: 0.0027, loss G: 6.4897\n",
      "Epoch [0/10] Batch 306/1583                       Loss D: 0.0028, loss G: 6.5043\n",
      "Epoch [0/10] Batch 307/1583                       Loss D: 0.0030, loss G: 6.4956\n",
      "Epoch [0/10] Batch 308/1583                       Loss D: 0.0026, loss G: 6.4983\n",
      "Epoch [0/10] Batch 309/1583                       Loss D: 0.0026, loss G: 6.5102\n",
      "Epoch [0/10] Batch 310/1583                       Loss D: 0.0025, loss G: 6.5308\n",
      "Epoch [0/10] Batch 311/1583                       Loss D: 0.0029, loss G: 6.5416\n",
      "Epoch [0/10] Batch 312/1583                       Loss D: 0.0026, loss G: 6.5543\n",
      "Epoch [0/10] Batch 313/1583                       Loss D: 0.0026, loss G: 6.5671\n",
      "Epoch [0/10] Batch 314/1583                       Loss D: 0.0025, loss G: 6.5766\n",
      "Epoch [0/10] Batch 315/1583                       Loss D: 0.0024, loss G: 6.5908\n",
      "Epoch [0/10] Batch 316/1583                       Loss D: 0.0025, loss G: 6.6041\n",
      "Epoch [0/10] Batch 317/1583                       Loss D: 0.0024, loss G: 6.6057\n",
      "Epoch [0/10] Batch 318/1583                       Loss D: 0.0025, loss G: 6.6000\n",
      "Epoch [0/10] Batch 319/1583                       Loss D: 0.0023, loss G: 6.6029\n",
      "Epoch [0/10] Batch 320/1583                       Loss D: 0.0024, loss G: 6.6079\n",
      "Epoch [0/10] Batch 321/1583                       Loss D: 0.0023, loss G: 6.6177\n",
      "Epoch [0/10] Batch 322/1583                       Loss D: 0.0025, loss G: 6.6209\n",
      "Epoch [0/10] Batch 323/1583                       Loss D: 0.0024, loss G: 6.6216\n",
      "Epoch [0/10] Batch 324/1583                       Loss D: 0.0023, loss G: 6.6143\n",
      "Epoch [0/10] Batch 325/1583                       Loss D: 0.0023, loss G: 6.6113\n",
      "Epoch [0/10] Batch 326/1583                       Loss D: 0.0023, loss G: 6.6120\n",
      "Epoch [0/10] Batch 327/1583                       Loss D: 0.0023, loss G: 6.6097\n",
      "Epoch [0/10] Batch 328/1583                       Loss D: 0.0023, loss G: 6.6051\n",
      "Epoch [0/10] Batch 329/1583                       Loss D: 0.0024, loss G: 6.5973\n",
      "Epoch [0/10] Batch 330/1583                       Loss D: 0.0026, loss G: 6.5942\n",
      "Epoch [0/10] Batch 331/1583                       Loss D: 0.0024, loss G: 6.5907\n",
      "Epoch [0/10] Batch 332/1583                       Loss D: 0.0024, loss G: 6.5916\n",
      "Epoch [0/10] Batch 333/1583                       Loss D: 0.0024, loss G: 6.5718\n",
      "Epoch [0/10] Batch 334/1583                       Loss D: 0.0024, loss G: 6.5505\n",
      "Epoch [0/10] Batch 335/1583                       Loss D: 0.0026, loss G: 6.5393\n",
      "Epoch [0/10] Batch 336/1583                       Loss D: 0.0023, loss G: 6.5360\n",
      "Epoch [0/10] Batch 337/1583                       Loss D: 0.0027, loss G: 6.5198\n",
      "Epoch [0/10] Batch 338/1583                       Loss D: 0.0023, loss G: 6.5072\n",
      "Epoch [0/10] Batch 339/1583                       Loss D: 0.0027, loss G: 6.4946\n",
      "Epoch [0/10] Batch 340/1583                       Loss D: 0.0027, loss G: 6.4941\n",
      "Epoch [0/10] Batch 341/1583                       Loss D: 0.0024, loss G: 6.5077\n",
      "Epoch [0/10] Batch 342/1583                       Loss D: 0.0025, loss G: 6.5272\n",
      "Epoch [0/10] Batch 343/1583                       Loss D: 0.0025, loss G: 6.4745\n",
      "Epoch [0/10] Batch 344/1583                       Loss D: 0.0028, loss G: 6.4545\n",
      "Epoch [0/10] Batch 345/1583                       Loss D: 0.0026, loss G: 6.4439\n",
      "Epoch [0/10] Batch 346/1583                       Loss D: 0.0025, loss G: 6.4546\n",
      "Epoch [0/10] Batch 347/1583                       Loss D: 0.0024, loss G: 6.4738\n",
      "Epoch [0/10] Batch 348/1583                       Loss D: 0.0025, loss G: 6.5007\n",
      "Epoch [0/10] Batch 349/1583                       Loss D: 0.0025, loss G: 6.4784\n",
      "Epoch [0/10] Batch 350/1583                       Loss D: 0.0025, loss G: 6.4802\n",
      "Epoch [0/10] Batch 351/1583                       Loss D: 0.0025, loss G: 6.4690\n",
      "Epoch [0/10] Batch 352/1583                       Loss D: 0.0027, loss G: 6.4681\n",
      "Epoch [0/10] Batch 353/1583                       Loss D: 0.0026, loss G: 6.4772\n",
      "Epoch [0/10] Batch 354/1583                       Loss D: 0.0024, loss G: 6.5018\n",
      "Epoch [0/10] Batch 355/1583                       Loss D: 0.0024, loss G: 6.4899\n",
      "Epoch [0/10] Batch 356/1583                       Loss D: 0.0025, loss G: 6.4866\n",
      "Epoch [0/10] Batch 357/1583                       Loss D: 0.0026, loss G: 6.4906\n",
      "Epoch [0/10] Batch 358/1583                       Loss D: 0.0025, loss G: 6.5210\n",
      "Epoch [0/10] Batch 359/1583                       Loss D: 0.0025, loss G: 6.5488\n",
      "Epoch [0/10] Batch 360/1583                       Loss D: 0.0023, loss G: 6.5575\n",
      "Epoch [0/10] Batch 361/1583                       Loss D: 0.0024, loss G: 6.5409\n",
      "Epoch [0/10] Batch 362/1583                       Loss D: 0.0024, loss G: 6.5401\n",
      "Epoch [0/10] Batch 363/1583                       Loss D: 0.0023, loss G: 6.5526\n",
      "Epoch [0/10] Batch 364/1583                       Loss D: 0.0023, loss G: 6.5739\n",
      "Epoch [0/10] Batch 365/1583                       Loss D: 0.0023, loss G: 6.5947\n",
      "Epoch [0/10] Batch 366/1583                       Loss D: 0.0025, loss G: 6.6001\n",
      "Epoch [0/10] Batch 367/1583                       Loss D: 0.0023, loss G: 6.6071\n",
      "Epoch [0/10] Batch 368/1583                       Loss D: 0.0023, loss G: 6.6187\n",
      "Epoch [0/10] Batch 369/1583                       Loss D: 0.0022, loss G: 6.6451\n",
      "Epoch [0/10] Batch 370/1583                       Loss D: 0.0021, loss G: 6.6669\n",
      "Epoch [0/10] Batch 371/1583                       Loss D: 0.0021, loss G: 6.6680\n",
      "Epoch [0/10] Batch 372/1583                       Loss D: 0.0022, loss G: 6.6607\n",
      "Epoch [0/10] Batch 373/1583                       Loss D: 0.0022, loss G: 6.6631\n",
      "Epoch [0/10] Batch 374/1583                       Loss D: 0.0022, loss G: 6.6729\n",
      "Epoch [0/10] Batch 375/1583                       Loss D: 0.0020, loss G: 6.6916\n",
      "Epoch [0/10] Batch 376/1583                       Loss D: 0.0019, loss G: 6.7135\n",
      "Epoch [0/10] Batch 377/1583                       Loss D: 0.0021, loss G: 6.7280\n",
      "Epoch [0/10] Batch 378/1583                       Loss D: 0.0020, loss G: 6.7376\n",
      "Epoch [0/10] Batch 379/1583                       Loss D: 0.0020, loss G: 6.7453\n",
      "Epoch [0/10] Batch 380/1583                       Loss D: 0.0019, loss G: 6.7556\n",
      "Epoch [0/10] Batch 381/1583                       Loss D: 0.0019, loss G: 6.7709\n",
      "Epoch [0/10] Batch 382/1583                       Loss D: 0.0019, loss G: 6.7890\n",
      "Epoch [0/10] Batch 383/1583                       Loss D: 0.0019, loss G: 6.8004\n",
      "Epoch [0/10] Batch 384/1583                       Loss D: 0.0019, loss G: 6.7986\n",
      "Epoch [0/10] Batch 385/1583                       Loss D: 0.0019, loss G: 6.8003\n",
      "Epoch [0/10] Batch 386/1583                       Loss D: 0.0018, loss G: 6.8114\n",
      "Epoch [0/10] Batch 387/1583                       Loss D: 0.0019, loss G: 6.8210\n",
      "Epoch [0/10] Batch 388/1583                       Loss D: 0.0019, loss G: 6.8295\n",
      "Epoch [0/10] Batch 389/1583                       Loss D: 0.0019, loss G: 6.8312\n",
      "Epoch [0/10] Batch 390/1583                       Loss D: 0.0018, loss G: 6.8364\n",
      "Epoch [0/10] Batch 391/1583                       Loss D: 0.0019, loss G: 6.8409\n",
      "Epoch [0/10] Batch 392/1583                       Loss D: 0.0018, loss G: 6.8505\n",
      "Epoch [0/10] Batch 393/1583                       Loss D: 0.0019, loss G: 6.8558\n",
      "Epoch [0/10] Batch 394/1583                       Loss D: 0.0018, loss G: 6.8629\n",
      "Epoch [0/10] Batch 395/1583                       Loss D: 0.0018, loss G: 6.8667\n",
      "Epoch [0/10] Batch 396/1583                       Loss D: 0.0018, loss G: 6.8693\n",
      "Epoch [0/10] Batch 397/1583                       Loss D: 0.0018, loss G: 6.8778\n",
      "Epoch [0/10] Batch 398/1583                       Loss D: 0.0017, loss G: 6.8895\n",
      "Epoch [0/10] Batch 399/1583                       Loss D: 0.0017, loss G: 6.8995\n",
      "Epoch [0/10] Batch 400/1583                       Loss D: 0.0018, loss G: 6.9084\n",
      "Epoch [0/10] Batch 401/1583                       Loss D: 0.0016, loss G: 6.9161\n",
      "Epoch [0/10] Batch 402/1583                       Loss D: 0.0017, loss G: 6.9202\n",
      "Epoch [0/10] Batch 403/1583                       Loss D: 0.0016, loss G: 6.9232\n",
      "Epoch [0/10] Batch 404/1583                       Loss D: 0.0018, loss G: 6.9252\n",
      "Epoch [0/10] Batch 405/1583                       Loss D: 0.0016, loss G: 6.9280\n",
      "Epoch [0/10] Batch 406/1583                       Loss D: 0.0018, loss G: 6.9275\n",
      "Epoch [0/10] Batch 407/1583                       Loss D: 0.0016, loss G: 6.9360\n",
      "Epoch [0/10] Batch 408/1583                       Loss D: 0.0016, loss G: 6.9488\n",
      "Epoch [0/10] Batch 409/1583                       Loss D: 0.0016, loss G: 6.9619\n",
      "Epoch [0/10] Batch 410/1583                       Loss D: 0.0016, loss G: 6.9713\n",
      "Epoch [0/10] Batch 411/1583                       Loss D: 0.0018, loss G: 6.9651\n",
      "Epoch [0/10] Batch 412/1583                       Loss D: 0.0016, loss G: 6.9635\n",
      "Epoch [0/10] Batch 413/1583                       Loss D: 0.0017, loss G: 6.9644\n",
      "Epoch [0/10] Batch 414/1583                       Loss D: 0.0015, loss G: 6.9715\n",
      "Epoch [0/10] Batch 415/1583                       Loss D: 0.0016, loss G: 6.9796\n",
      "Epoch [0/10] Batch 416/1583                       Loss D: 0.0016, loss G: 6.9928\n",
      "Epoch [0/10] Batch 417/1583                       Loss D: 0.0016, loss G: 7.0041\n",
      "Epoch [0/10] Batch 418/1583                       Loss D: 0.0015, loss G: 7.0101\n",
      "Epoch [0/10] Batch 419/1583                       Loss D: 0.0014, loss G: 7.0144\n",
      "Epoch [0/10] Batch 420/1583                       Loss D: 0.0016, loss G: 7.0122\n",
      "Epoch [0/10] Batch 421/1583                       Loss D: 0.0016, loss G: 7.0149\n",
      "Epoch [0/10] Batch 422/1583                       Loss D: 0.0015, loss G: 7.0185\n",
      "Epoch [0/10] Batch 423/1583                       Loss D: 0.0015, loss G: 7.0289\n",
      "Epoch [0/10] Batch 424/1583                       Loss D: 0.0015, loss G: 7.0381\n",
      "Epoch [0/10] Batch 425/1583                       Loss D: 0.0014, loss G: 7.0477\n",
      "Epoch [0/10] Batch 426/1583                       Loss D: 0.0015, loss G: 7.0512\n",
      "Epoch [0/10] Batch 427/1583                       Loss D: 0.0015, loss G: 7.0550\n",
      "Epoch [0/10] Batch 428/1583                       Loss D: 0.0015, loss G: 7.0557\n",
      "Epoch [0/10] Batch 429/1583                       Loss D: 0.0015, loss G: 7.0567\n",
      "Epoch [0/10] Batch 430/1583                       Loss D: 0.0014, loss G: 7.0611\n",
      "Epoch [0/10] Batch 431/1583                       Loss D: 0.0014, loss G: 7.0649\n",
      "Epoch [0/10] Batch 432/1583                       Loss D: 0.0015, loss G: 7.0623\n",
      "Epoch [0/10] Batch 433/1583                       Loss D: 0.0015, loss G: 7.0570\n",
      "Epoch [0/10] Batch 434/1583                       Loss D: 0.0014, loss G: 7.0530\n",
      "Epoch [0/10] Batch 435/1583                       Loss D: 0.0015, loss G: 7.0514\n",
      "Epoch [0/10] Batch 436/1583                       Loss D: 0.0014, loss G: 7.0539\n",
      "Epoch [0/10] Batch 437/1583                       Loss D: 0.0015, loss G: 7.0511\n",
      "Epoch [0/10] Batch 438/1583                       Loss D: 0.0014, loss G: 7.0480\n",
      "Epoch [0/10] Batch 439/1583                       Loss D: 0.0015, loss G: 7.0391\n",
      "Epoch [0/10] Batch 440/1583                       Loss D: 0.0014, loss G: 7.0338\n",
      "Epoch [0/10] Batch 441/1583                       Loss D: 0.0014, loss G: 7.0362\n",
      "Epoch [0/10] Batch 442/1583                       Loss D: 0.0014, loss G: 7.0435\n",
      "Epoch [0/10] Batch 443/1583                       Loss D: 0.0015, loss G: 7.0497\n",
      "Epoch [0/10] Batch 444/1583                       Loss D: 0.0014, loss G: 7.0529\n",
      "Epoch [0/10] Batch 445/1583                       Loss D: 0.0015, loss G: 7.0459\n",
      "Epoch [0/10] Batch 446/1583                       Loss D: 0.0014, loss G: 7.0404\n",
      "Epoch [0/10] Batch 447/1583                       Loss D: 0.0014, loss G: 7.0363\n",
      "Epoch [0/10] Batch 448/1583                       Loss D: 0.0015, loss G: 7.0360\n",
      "Epoch [0/10] Batch 449/1583                       Loss D: 0.0015, loss G: 7.0366\n",
      "Epoch [0/10] Batch 450/1583                       Loss D: 0.0014, loss G: 7.0424\n",
      "Epoch [0/10] Batch 451/1583                       Loss D: 0.0014, loss G: 7.0508\n",
      "Epoch [0/10] Batch 452/1583                       Loss D: 0.0014, loss G: 7.0629\n",
      "Epoch [0/10] Batch 453/1583                       Loss D: 0.0014, loss G: 7.0671\n",
      "Epoch [0/10] Batch 454/1583                       Loss D: 0.0014, loss G: 7.0732\n",
      "Epoch [0/10] Batch 455/1583                       Loss D: 0.0014, loss G: 7.0747\n",
      "Epoch [0/10] Batch 456/1583                       Loss D: 0.0015, loss G: 7.0772\n",
      "Epoch [0/10] Batch 457/1583                       Loss D: 0.0013, loss G: 7.0856\n",
      "Epoch [0/10] Batch 458/1583                       Loss D: 0.0013, loss G: 7.0966\n",
      "Epoch [0/10] Batch 459/1583                       Loss D: 0.0013, loss G: 7.1028\n",
      "Epoch [0/10] Batch 460/1583                       Loss D: 0.0014, loss G: 7.1118\n",
      "Epoch [0/10] Batch 461/1583                       Loss D: 0.0015, loss G: 7.1149\n",
      "Epoch [0/10] Batch 462/1583                       Loss D: 0.0014, loss G: 7.1201\n",
      "Epoch [0/10] Batch 463/1583                       Loss D: 0.0013, loss G: 7.1280\n",
      "Epoch [0/10] Batch 464/1583                       Loss D: 0.0013, loss G: 7.1390\n",
      "Epoch [0/10] Batch 465/1583                       Loss D: 0.0014, loss G: 7.1494\n",
      "Epoch [0/10] Batch 466/1583                       Loss D: 0.0013, loss G: 7.1563\n",
      "Epoch [0/10] Batch 467/1583                       Loss D: 0.0013, loss G: 7.1576\n",
      "Epoch [0/10] Batch 468/1583                       Loss D: 0.0013, loss G: 7.1597\n",
      "Epoch [0/10] Batch 469/1583                       Loss D: 0.0013, loss G: 7.1626\n",
      "Epoch [0/10] Batch 470/1583                       Loss D: 0.0013, loss G: 7.1714\n",
      "Epoch [0/10] Batch 471/1583                       Loss D: 0.0013, loss G: 7.1810\n",
      "Epoch [0/10] Batch 472/1583                       Loss D: 0.0013, loss G: 7.1883\n",
      "Epoch [0/10] Batch 473/1583                       Loss D: 0.0013, loss G: 7.1961\n",
      "Epoch [0/10] Batch 474/1583                       Loss D: 0.0012, loss G: 7.1990\n",
      "Epoch [0/10] Batch 475/1583                       Loss D: 0.0012, loss G: 7.1996\n",
      "Epoch [0/10] Batch 476/1583                       Loss D: 0.0012, loss G: 7.2002\n",
      "Epoch [0/10] Batch 477/1583                       Loss D: 0.0013, loss G: 7.2023\n",
      "Epoch [0/10] Batch 478/1583                       Loss D: 0.0013, loss G: 7.2061\n",
      "Epoch [0/10] Batch 479/1583                       Loss D: 0.0013, loss G: 7.2142\n",
      "Epoch [0/10] Batch 480/1583                       Loss D: 0.0012, loss G: 7.2190\n",
      "Epoch [0/10] Batch 481/1583                       Loss D: 0.0012, loss G: 7.2184\n",
      "Epoch [0/10] Batch 482/1583                       Loss D: 0.0012, loss G: 7.2172\n",
      "Epoch [0/10] Batch 483/1583                       Loss D: 0.0012, loss G: 7.2158\n",
      "Epoch [0/10] Batch 484/1583                       Loss D: 0.0013, loss G: 7.2128\n",
      "Epoch [0/10] Batch 485/1583                       Loss D: 0.0012, loss G: 7.2122\n",
      "Epoch [0/10] Batch 486/1583                       Loss D: 0.0012, loss G: 7.2140\n",
      "Epoch [0/10] Batch 487/1583                       Loss D: 0.0013, loss G: 7.2103\n",
      "Epoch [0/10] Batch 488/1583                       Loss D: 0.0013, loss G: 7.2077\n",
      "Epoch [0/10] Batch 489/1583                       Loss D: 0.0012, loss G: 7.2031\n",
      "Epoch [0/10] Batch 490/1583                       Loss D: 0.0012, loss G: 7.2011\n",
      "Epoch [0/10] Batch 491/1583                       Loss D: 0.0012, loss G: 7.2001\n",
      "Epoch [0/10] Batch 492/1583                       Loss D: 0.0013, loss G: 7.2001\n",
      "Epoch [0/10] Batch 493/1583                       Loss D: 0.0012, loss G: 7.2015\n",
      "Epoch [0/10] Batch 494/1583                       Loss D: 0.0012, loss G: 7.2094\n",
      "Epoch [0/10] Batch 495/1583                       Loss D: 0.0012, loss G: 7.2067\n",
      "Epoch [0/10] Batch 496/1583                       Loss D: 0.0012, loss G: 7.2046\n",
      "Epoch [0/10] Batch 497/1583                       Loss D: 0.0011, loss G: 7.2042\n",
      "Epoch [0/10] Batch 498/1583                       Loss D: 0.0012, loss G: 7.2060\n",
      "Epoch [0/10] Batch 499/1583                       Loss D: 0.0012, loss G: 7.2061\n",
      "Epoch [0/10] Batch 500/1583                       Loss D: 0.0013, loss G: 7.2004\n",
      "Epoch [0/10] Batch 501/1583                       Loss D: 0.0012, loss G: 7.1949\n",
      "Epoch [0/10] Batch 502/1583                       Loss D: 0.0012, loss G: 7.1906\n",
      "Epoch [0/10] Batch 503/1583                       Loss D: 0.0013, loss G: 7.1848\n",
      "Epoch [0/10] Batch 504/1583                       Loss D: 0.0013, loss G: 7.1874\n",
      "Epoch [0/10] Batch 505/1583                       Loss D: 0.0013, loss G: 7.1941\n",
      "Epoch [0/10] Batch 506/1583                       Loss D: 0.0012, loss G: 7.1981\n",
      "Epoch [0/10] Batch 507/1583                       Loss D: 0.0013, loss G: 7.1938\n",
      "Epoch [0/10] Batch 508/1583                       Loss D: 0.0012, loss G: 7.1935\n",
      "Epoch [0/10] Batch 509/1583                       Loss D: 0.0012, loss G: 7.1983\n",
      "Epoch [0/10] Batch 510/1583                       Loss D: 0.0012, loss G: 7.2082\n",
      "Epoch [0/10] Batch 511/1583                       Loss D: 0.0012, loss G: 7.2203\n",
      "Epoch [0/10] Batch 512/1583                       Loss D: 0.0012, loss G: 7.2234\n",
      "Epoch [0/10] Batch 513/1583                       Loss D: 0.0011, loss G: 7.2223\n",
      "Epoch [0/10] Batch 514/1583                       Loss D: 0.0012, loss G: 7.2208\n",
      "Epoch [0/10] Batch 515/1583                       Loss D: 0.0012, loss G: 7.2245\n",
      "Epoch [0/10] Batch 516/1583                       Loss D: 0.0011, loss G: 7.2312\n",
      "Epoch [0/10] Batch 517/1583                       Loss D: 0.0012, loss G: 7.2368\n",
      "Epoch [0/10] Batch 518/1583                       Loss D: 0.0011, loss G: 7.2454\n",
      "Epoch [0/10] Batch 519/1583                       Loss D: 0.0012, loss G: 7.2529\n",
      "Epoch [0/10] Batch 520/1583                       Loss D: 0.0011, loss G: 7.2569\n",
      "Epoch [0/10] Batch 521/1583                       Loss D: 0.0012, loss G: 7.2582\n",
      "Epoch [0/10] Batch 522/1583                       Loss D: 0.0012, loss G: 7.2635\n",
      "Epoch [0/10] Batch 523/1583                       Loss D: 0.0011, loss G: 7.2690\n",
      "Epoch [0/10] Batch 524/1583                       Loss D: 0.0011, loss G: 7.2781\n",
      "Epoch [0/10] Batch 525/1583                       Loss D: 0.0012, loss G: 7.2846\n",
      "Epoch [0/10] Batch 526/1583                       Loss D: 0.0011, loss G: 7.2851\n",
      "Epoch [0/10] Batch 527/1583                       Loss D: 0.0011, loss G: 7.2875\n",
      "Epoch [0/10] Batch 528/1583                       Loss D: 0.0011, loss G: 7.2900\n",
      "Epoch [0/10] Batch 529/1583                       Loss D: 0.0011, loss G: 7.2960\n",
      "Epoch [0/10] Batch 530/1583                       Loss D: 0.0011, loss G: 7.3032\n",
      "Epoch [0/10] Batch 531/1583                       Loss D: 0.0011, loss G: 7.3106\n",
      "Epoch [0/10] Batch 532/1583                       Loss D: 0.0010, loss G: 7.3174\n",
      "Epoch [0/10] Batch 533/1583                       Loss D: 0.0011, loss G: 7.3187\n",
      "Epoch [0/10] Batch 534/1583                       Loss D: 0.0011, loss G: 7.3233\n",
      "Epoch [0/10] Batch 535/1583                       Loss D: 0.0010, loss G: 7.3335\n",
      "Epoch [0/10] Batch 536/1583                       Loss D: 0.0011, loss G: 7.3429\n",
      "Epoch [0/10] Batch 537/1583                       Loss D: 0.0011, loss G: 7.3485\n",
      "Epoch [0/10] Batch 538/1583                       Loss D: 0.0011, loss G: 7.3544\n",
      "Epoch [0/10] Batch 539/1583                       Loss D: 0.0010, loss G: 7.3570\n",
      "Epoch [0/10] Batch 540/1583                       Loss D: 0.0010, loss G: 7.3645\n",
      "Epoch [0/10] Batch 541/1583                       Loss D: 0.0011, loss G: 7.3705\n",
      "Epoch [0/10] Batch 542/1583                       Loss D: 0.0010, loss G: 7.3793\n",
      "Epoch [0/10] Batch 543/1583                       Loss D: 0.0010, loss G: 7.3884\n",
      "Epoch [0/10] Batch 544/1583                       Loss D: 0.0010, loss G: 7.3936\n",
      "Epoch [0/10] Batch 545/1583                       Loss D: 0.0010, loss G: 7.3979\n",
      "Epoch [0/10] Batch 546/1583                       Loss D: 0.0010, loss G: 7.4038\n",
      "Epoch [0/10] Batch 547/1583                       Loss D: 0.0010, loss G: 7.4117\n",
      "Epoch [0/10] Batch 548/1583                       Loss D: 0.0010, loss G: 7.4230\n",
      "Epoch [0/10] Batch 549/1583                       Loss D: 0.0009, loss G: 7.4315\n",
      "Epoch [0/10] Batch 550/1583                       Loss D: 0.0009, loss G: 7.4401\n",
      "Epoch [0/10] Batch 551/1583                       Loss D: 0.0010, loss G: 7.4430\n",
      "Epoch [0/10] Batch 552/1583                       Loss D: 0.0010, loss G: 7.4408\n",
      "Epoch [0/10] Batch 553/1583                       Loss D: 0.0010, loss G: 7.4405\n",
      "Epoch [0/10] Batch 554/1583                       Loss D: 0.0009, loss G: 7.4452\n",
      "Epoch [0/10] Batch 555/1583                       Loss D: 0.0009, loss G: 7.4532\n",
      "Epoch [0/10] Batch 556/1583                       Loss D: 0.0010, loss G: 7.4643\n",
      "Epoch [0/10] Batch 557/1583                       Loss D: 0.0009, loss G: 7.4746\n",
      "Epoch [0/10] Batch 558/1583                       Loss D: 0.0010, loss G: 7.4811\n",
      "Epoch [0/10] Batch 559/1583                       Loss D: 0.0009, loss G: 7.4837\n",
      "Epoch [0/10] Batch 560/1583                       Loss D: 0.0009, loss G: 7.4849\n",
      "Epoch [0/10] Batch 561/1583                       Loss D: 0.0010, loss G: 7.4883\n",
      "Epoch [0/10] Batch 562/1583                       Loss D: 0.0010, loss G: 7.4926\n",
      "Epoch [0/10] Batch 563/1583                       Loss D: 0.0009, loss G: 7.4985\n",
      "Epoch [0/10] Batch 564/1583                       Loss D: 0.0009, loss G: 7.5063\n",
      "Epoch [0/10] Batch 565/1583                       Loss D: 0.0009, loss G: 7.5127\n",
      "Epoch [0/10] Batch 566/1583                       Loss D: 0.0009, loss G: 7.5134\n",
      "Epoch [0/10] Batch 567/1583                       Loss D: 0.0009, loss G: 7.5155\n",
      "Epoch [0/10] Batch 568/1583                       Loss D: 0.0009, loss G: 7.5181\n",
      "Epoch [0/10] Batch 569/1583                       Loss D: 0.0009, loss G: 7.5236\n",
      "Epoch [0/10] Batch 570/1583                       Loss D: 0.0011, loss G: 7.5299\n",
      "Epoch [0/10] Batch 571/1583                       Loss D: 0.0009, loss G: 7.5395\n",
      "Epoch [0/10] Batch 572/1583                       Loss D: 0.0009, loss G: 7.5455\n",
      "Epoch [0/10] Batch 573/1583                       Loss D: 0.0009, loss G: 7.5479\n",
      "Epoch [0/10] Batch 574/1583                       Loss D: 0.0009, loss G: 7.5505\n",
      "Epoch [0/10] Batch 575/1583                       Loss D: 0.0009, loss G: 7.5484\n",
      "Epoch [0/10] Batch 576/1583                       Loss D: 0.0009, loss G: 7.5535\n",
      "Epoch [0/10] Batch 577/1583                       Loss D: 0.0009, loss G: 7.5582\n",
      "Epoch [0/10] Batch 578/1583                       Loss D: 0.0010, loss G: 7.5616\n",
      "Epoch [0/10] Batch 579/1583                       Loss D: 0.0009, loss G: 7.5667\n",
      "Epoch [0/10] Batch 580/1583                       Loss D: 0.0009, loss G: 7.5745\n",
      "Epoch [0/10] Batch 581/1583                       Loss D: 0.0009, loss G: 7.5759\n",
      "Epoch [0/10] Batch 582/1583                       Loss D: 0.0008, loss G: 7.5772\n",
      "Epoch [0/10] Batch 583/1583                       Loss D: 0.0008, loss G: 7.5775\n",
      "Epoch [0/10] Batch 584/1583                       Loss D: 0.0008, loss G: 7.5808\n",
      "Epoch [0/10] Batch 585/1583                       Loss D: 0.0009, loss G: 7.5868\n",
      "Epoch [0/10] Batch 586/1583                       Loss D: 0.0008, loss G: 7.5922\n",
      "Epoch [0/10] Batch 587/1583                       Loss D: 0.0009, loss G: 7.5972\n",
      "Epoch [0/10] Batch 588/1583                       Loss D: 0.0009, loss G: 7.5994\n",
      "Epoch [0/10] Batch 589/1583                       Loss D: 0.0008, loss G: 7.6000\n",
      "Epoch [0/10] Batch 590/1583                       Loss D: 0.0008, loss G: 7.6010\n",
      "Epoch [0/10] Batch 591/1583                       Loss D: 0.0008, loss G: 7.6028\n",
      "Epoch [0/10] Batch 592/1583                       Loss D: 0.0008, loss G: 7.6077\n",
      "Epoch [0/10] Batch 593/1583                       Loss D: 0.0008, loss G: 7.6138\n",
      "Epoch [0/10] Batch 594/1583                       Loss D: 0.0008, loss G: 7.6212\n",
      "Epoch [0/10] Batch 595/1583                       Loss D: 0.0008, loss G: 7.6252\n",
      "Epoch [0/10] Batch 596/1583                       Loss D: 0.0008, loss G: 7.6279\n",
      "Epoch [0/10] Batch 597/1583                       Loss D: 0.0009, loss G: 7.6308\n",
      "Epoch [0/10] Batch 598/1583                       Loss D: 0.0008, loss G: 7.6330\n",
      "Epoch [0/10] Batch 599/1583                       Loss D: 0.0008, loss G: 7.6362\n",
      "Epoch [0/10] Batch 600/1583                       Loss D: 0.0009, loss G: 7.6376\n",
      "Epoch [0/10] Batch 601/1583                       Loss D: 0.0008, loss G: 7.6422\n",
      "Epoch [0/10] Batch 602/1583                       Loss D: 0.0008, loss G: 7.6405\n",
      "Epoch [0/10] Batch 603/1583                       Loss D: 0.0008, loss G: 7.6408\n",
      "Epoch [0/10] Batch 604/1583                       Loss D: 0.0008, loss G: 7.6407\n",
      "Epoch [0/10] Batch 605/1583                       Loss D: 0.0008, loss G: 7.6412\n",
      "Epoch [0/10] Batch 606/1583                       Loss D: 0.0008, loss G: 7.6440\n",
      "Epoch [0/10] Batch 607/1583                       Loss D: 0.0008, loss G: 7.6461\n",
      "Epoch [0/10] Batch 608/1583                       Loss D: 0.0008, loss G: 7.6559\n",
      "Epoch [0/10] Batch 609/1583                       Loss D: 0.0009, loss G: 7.6622\n",
      "Epoch [0/10] Batch 610/1583                       Loss D: 0.0008, loss G: 7.6652\n",
      "Epoch [0/10] Batch 611/1583                       Loss D: 0.0008, loss G: 7.6628\n",
      "Epoch [0/10] Batch 612/1583                       Loss D: 0.0008, loss G: 7.6619\n",
      "Epoch [0/10] Batch 613/1583                       Loss D: 0.0008, loss G: 7.6603\n",
      "Epoch [0/10] Batch 614/1583                       Loss D: 0.0008, loss G: 7.6625\n",
      "Epoch [0/10] Batch 615/1583                       Loss D: 0.0008, loss G: 7.6653\n",
      "Epoch [0/10] Batch 616/1583                       Loss D: 0.0008, loss G: 7.6727\n",
      "Epoch [0/10] Batch 617/1583                       Loss D: 0.0008, loss G: 7.6754\n",
      "Epoch [0/10] Batch 618/1583                       Loss D: 0.0008, loss G: 7.6718\n",
      "Epoch [0/10] Batch 619/1583                       Loss D: 0.0008, loss G: 7.6686\n",
      "Epoch [0/10] Batch 620/1583                       Loss D: 0.0008, loss G: 7.6753\n",
      "Epoch [0/10] Batch 621/1583                       Loss D: 0.0007, loss G: 7.6869\n",
      "Epoch [0/10] Batch 622/1583                       Loss D: 0.0008, loss G: 7.6929\n",
      "Epoch [0/10] Batch 623/1583                       Loss D: 0.0007, loss G: 7.6917\n",
      "Epoch [0/10] Batch 624/1583                       Loss D: 0.0008, loss G: 7.6892\n",
      "Epoch [0/10] Batch 625/1583                       Loss D: 0.0008, loss G: 7.6865\n",
      "Epoch [0/10] Batch 626/1583                       Loss D: 0.0008, loss G: 7.6921\n",
      "Epoch [0/10] Batch 627/1583                       Loss D: 0.0008, loss G: 7.6981\n",
      "Epoch [0/10] Batch 628/1583                       Loss D: 0.0008, loss G: 7.7029\n",
      "Epoch [0/10] Batch 629/1583                       Loss D: 0.0008, loss G: 7.7031\n",
      "Epoch [0/10] Batch 630/1583                       Loss D: 0.0008, loss G: 7.7010\n",
      "Epoch [0/10] Batch 631/1583                       Loss D: 0.0008, loss G: 7.6997\n",
      "Epoch [0/10] Batch 632/1583                       Loss D: 0.0008, loss G: 7.7014\n",
      "Epoch [0/10] Batch 633/1583                       Loss D: 0.0008, loss G: 7.7054\n",
      "Epoch [0/10] Batch 634/1583                       Loss D: 0.0008, loss G: 7.7099\n",
      "Epoch [0/10] Batch 635/1583                       Loss D: 0.0007, loss G: 7.7171\n",
      "Epoch [0/10] Batch 636/1583                       Loss D: 0.0008, loss G: 7.7179\n",
      "Epoch [0/10] Batch 637/1583                       Loss D: 0.0007, loss G: 7.7192\n",
      "Epoch [0/10] Batch 638/1583                       Loss D: 0.0007, loss G: 7.7251\n",
      "Epoch [0/10] Batch 639/1583                       Loss D: 0.0007, loss G: 7.7336\n",
      "Epoch [0/10] Batch 640/1583                       Loss D: 0.0007, loss G: 7.7438\n",
      "Epoch [0/10] Batch 641/1583                       Loss D: 0.0007, loss G: 7.7481\n",
      "Epoch [0/10] Batch 642/1583                       Loss D: 0.0007, loss G: 7.7508\n",
      "Epoch [0/10] Batch 643/1583                       Loss D: 0.0008, loss G: 7.7472\n",
      "Epoch [0/10] Batch 644/1583                       Loss D: 0.0007, loss G: 7.7489\n",
      "Epoch [0/10] Batch 645/1583                       Loss D: 0.0008, loss G: 7.7496\n",
      "Epoch [0/10] Batch 646/1583                       Loss D: 0.0007, loss G: 7.7534\n",
      "Epoch [0/10] Batch 647/1583                       Loss D: 0.0008, loss G: 7.7596\n",
      "Epoch [0/10] Batch 648/1583                       Loss D: 0.0007, loss G: 7.7712\n",
      "Epoch [0/10] Batch 649/1583                       Loss D: 0.0007, loss G: 7.7778\n",
      "Epoch [0/10] Batch 650/1583                       Loss D: 0.0007, loss G: 7.7855\n",
      "Epoch [0/10] Batch 651/1583                       Loss D: 0.0008, loss G: 7.7858\n",
      "Epoch [0/10] Batch 652/1583                       Loss D: 0.0007, loss G: 7.7872\n",
      "Epoch [0/10] Batch 653/1583                       Loss D: 0.0007, loss G: 7.7884\n",
      "Epoch [0/10] Batch 654/1583                       Loss D: 0.0007, loss G: 7.7891\n",
      "Epoch [0/10] Batch 655/1583                       Loss D: 0.0007, loss G: 7.7882\n",
      "Epoch [0/10] Batch 656/1583                       Loss D: 0.0007, loss G: 7.7917\n",
      "Epoch [0/10] Batch 657/1583                       Loss D: 0.0007, loss G: 7.7934\n",
      "Epoch [0/10] Batch 658/1583                       Loss D: 0.0007, loss G: 7.7935\n",
      "Epoch [0/10] Batch 659/1583                       Loss D: 0.0007, loss G: 7.7979\n",
      "Epoch [0/10] Batch 660/1583                       Loss D: 0.0007, loss G: 7.8012\n",
      "Epoch [0/10] Batch 661/1583                       Loss D: 0.0007, loss G: 7.8056\n",
      "Epoch [0/10] Batch 662/1583                       Loss D: 0.0007, loss G: 7.8118\n",
      "Epoch [0/10] Batch 663/1583                       Loss D: 0.0007, loss G: 7.8188\n",
      "Epoch [0/10] Batch 664/1583                       Loss D: 0.0007, loss G: 7.8217\n",
      "Epoch [0/10] Batch 665/1583                       Loss D: 0.0007, loss G: 7.8256\n",
      "Epoch [0/10] Batch 666/1583                       Loss D: 0.0007, loss G: 7.8294\n",
      "Epoch [0/10] Batch 667/1583                       Loss D: 0.0007, loss G: 7.8306\n",
      "Epoch [0/10] Batch 668/1583                       Loss D: 0.0007, loss G: 7.8306\n",
      "Epoch [0/10] Batch 669/1583                       Loss D: 0.0007, loss G: 7.8318\n",
      "Epoch [0/10] Batch 670/1583                       Loss D: 0.0007, loss G: 7.8351\n",
      "Epoch [0/10] Batch 671/1583                       Loss D: 0.0007, loss G: 7.8386\n",
      "Epoch [0/10] Batch 672/1583                       Loss D: 0.0007, loss G: 7.8448\n",
      "Epoch [0/10] Batch 673/1583                       Loss D: 0.0007, loss G: 7.8464\n",
      "Epoch [0/10] Batch 674/1583                       Loss D: 0.0007, loss G: 7.8478\n",
      "Epoch [0/10] Batch 675/1583                       Loss D: 0.0007, loss G: 7.8494\n",
      "Epoch [0/10] Batch 676/1583                       Loss D: 0.0007, loss G: 7.8541\n",
      "Epoch [0/10] Batch 677/1583                       Loss D: 0.0006, loss G: 7.8613\n",
      "Epoch [0/10] Batch 678/1583                       Loss D: 0.0007, loss G: 7.8688\n",
      "Epoch [0/10] Batch 679/1583                       Loss D: 0.0006, loss G: 7.8708\n",
      "Epoch [0/10] Batch 680/1583                       Loss D: 0.0006, loss G: 7.8679\n",
      "Epoch [0/10] Batch 681/1583                       Loss D: 0.0007, loss G: 7.8662\n",
      "Epoch [0/10] Batch 682/1583                       Loss D: 0.0007, loss G: 7.8707\n",
      "Epoch [0/10] Batch 683/1583                       Loss D: 0.0007, loss G: 7.8785\n",
      "Epoch [0/10] Batch 684/1583                       Loss D: 0.0006, loss G: 7.8904\n",
      "Epoch [0/10] Batch 685/1583                       Loss D: 0.0007, loss G: 7.8954\n",
      "Epoch [0/10] Batch 686/1583                       Loss D: 0.0007, loss G: 7.8944\n",
      "Epoch [0/10] Batch 687/1583                       Loss D: 0.0007, loss G: 7.8947\n",
      "Epoch [0/10] Batch 688/1583                       Loss D: 0.0006, loss G: 7.8934\n",
      "Epoch [0/10] Batch 689/1583                       Loss D: 0.0006, loss G: 7.8952\n",
      "Epoch [0/10] Batch 690/1583                       Loss D: 0.0006, loss G: 7.8958\n",
      "Epoch [0/10] Batch 691/1583                       Loss D: 0.0006, loss G: 7.9044\n",
      "Epoch [0/10] Batch 692/1583                       Loss D: 0.0007, loss G: 7.9104\n",
      "Epoch [0/10] Batch 693/1583                       Loss D: 0.0006, loss G: 7.9201\n",
      "Epoch [0/10] Batch 694/1583                       Loss D: 0.0006, loss G: 7.9224\n",
      "Epoch [0/10] Batch 695/1583                       Loss D: 0.0006, loss G: 7.9242\n",
      "Epoch [0/10] Batch 696/1583                       Loss D: 0.0006, loss G: 7.9256\n",
      "Epoch [0/10] Batch 697/1583                       Loss D: 0.0006, loss G: 7.9235\n",
      "Epoch [0/10] Batch 698/1583                       Loss D: 0.0006, loss G: 7.9259\n",
      "Epoch [0/10] Batch 699/1583                       Loss D: 0.0006, loss G: 7.9293\n",
      "Epoch [0/10] Batch 700/1583                       Loss D: 0.0006, loss G: 7.9334\n",
      "Epoch [0/10] Batch 701/1583                       Loss D: 0.0007, loss G: 7.9369\n",
      "Epoch [0/10] Batch 702/1583                       Loss D: 0.0006, loss G: 7.9396\n",
      "Epoch [0/10] Batch 703/1583                       Loss D: 0.0006, loss G: 7.9402\n",
      "Epoch [0/10] Batch 704/1583                       Loss D: 0.0006, loss G: 7.9425\n",
      "Epoch [0/10] Batch 705/1583                       Loss D: 0.0006, loss G: 7.9495\n",
      "Epoch [0/10] Batch 706/1583                       Loss D: 0.0006, loss G: 7.9572\n",
      "Epoch [0/10] Batch 707/1583                       Loss D: 0.0006, loss G: 7.9613\n",
      "Epoch [0/10] Batch 708/1583                       Loss D: 0.0006, loss G: 7.9599\n",
      "Epoch [0/10] Batch 709/1583                       Loss D: 0.0006, loss G: 7.9613\n",
      "Epoch [0/10] Batch 710/1583                       Loss D: 0.0006, loss G: 7.9661\n",
      "Epoch [0/10] Batch 711/1583                       Loss D: 0.0006, loss G: 7.9720\n",
      "Epoch [0/10] Batch 712/1583                       Loss D: 0.0006, loss G: 7.9768\n",
      "Epoch [0/10] Batch 713/1583                       Loss D: 0.0006, loss G: 7.9850\n",
      "Epoch [0/10] Batch 714/1583                       Loss D: 0.0006, loss G: 7.9956\n",
      "Epoch [0/10] Batch 715/1583                       Loss D: 0.0006, loss G: 7.9978\n",
      "Epoch [0/10] Batch 716/1583                       Loss D: 0.0006, loss G: 7.9971\n",
      "Epoch [0/10] Batch 717/1583                       Loss D: 0.0006, loss G: 8.0004\n",
      "Epoch [0/10] Batch 718/1583                       Loss D: 0.0006, loss G: 8.0055\n",
      "Epoch [0/10] Batch 719/1583                       Loss D: 0.0006, loss G: 8.0088\n",
      "Epoch [0/10] Batch 720/1583                       Loss D: 0.0006, loss G: 8.0126\n",
      "Epoch [0/10] Batch 721/1583                       Loss D: 0.0006, loss G: 8.0138\n",
      "Epoch [0/10] Batch 722/1583                       Loss D: 0.0006, loss G: 8.0150\n",
      "Epoch [0/10] Batch 723/1583                       Loss D: 0.0005, loss G: 8.0200\n",
      "Epoch [0/10] Batch 724/1583                       Loss D: 0.0005, loss G: 8.0248\n",
      "Epoch [0/10] Batch 725/1583                       Loss D: 0.0006, loss G: 8.0294\n",
      "Epoch [0/10] Batch 726/1583                       Loss D: 0.0006, loss G: 8.0314\n",
      "Epoch [0/10] Batch 727/1583                       Loss D: 0.0006, loss G: 8.0355\n",
      "Epoch [0/10] Batch 728/1583                       Loss D: 0.0005, loss G: 8.0425\n",
      "Epoch [0/10] Batch 729/1583                       Loss D: 0.0006, loss G: 8.0422\n",
      "Epoch [0/10] Batch 730/1583                       Loss D: 0.0006, loss G: 8.0436\n",
      "Epoch [0/10] Batch 731/1583                       Loss D: 0.0006, loss G: 8.0489\n",
      "Epoch [0/10] Batch 732/1583                       Loss D: 0.0006, loss G: 8.0536\n",
      "Epoch [0/10] Batch 733/1583                       Loss D: 0.0006, loss G: 8.0573\n",
      "Epoch [0/10] Batch 734/1583                       Loss D: 0.0005, loss G: 8.0614\n",
      "Epoch [0/10] Batch 735/1583                       Loss D: 0.0005, loss G: 8.0648\n",
      "Epoch [0/10] Batch 736/1583                       Loss D: 0.0006, loss G: 8.0662\n",
      "Epoch [0/10] Batch 737/1583                       Loss D: 0.0006, loss G: 8.0688\n",
      "Epoch [0/10] Batch 738/1583                       Loss D: 0.0006, loss G: 8.0716\n",
      "Epoch [0/10] Batch 739/1583                       Loss D: 0.0006, loss G: 8.0745\n",
      "Epoch [0/10] Batch 740/1583                       Loss D: 0.0005, loss G: 8.0758\n",
      "Epoch [0/10] Batch 741/1583                       Loss D: 0.0005, loss G: 8.0783\n",
      "Epoch [0/10] Batch 742/1583                       Loss D: 0.0005, loss G: 8.0811\n",
      "Epoch [0/10] Batch 743/1583                       Loss D: 0.0005, loss G: 8.0834\n",
      "Epoch [0/10] Batch 744/1583                       Loss D: 0.0005, loss G: 8.0888\n",
      "Epoch [0/10] Batch 745/1583                       Loss D: 0.0006, loss G: 8.0918\n",
      "Epoch [0/10] Batch 746/1583                       Loss D: 0.0006, loss G: 8.0931\n",
      "Epoch [0/10] Batch 747/1583                       Loss D: 0.0005, loss G: 8.0978\n",
      "Epoch [0/10] Batch 748/1583                       Loss D: 0.0005, loss G: 8.1031\n",
      "Epoch [0/10] Batch 749/1583                       Loss D: 0.0005, loss G: 8.1035\n",
      "Epoch [0/10] Batch 750/1583                       Loss D: 0.0005, loss G: 8.1021\n",
      "Epoch [0/10] Batch 751/1583                       Loss D: 0.0005, loss G: 8.0984\n",
      "Epoch [0/10] Batch 752/1583                       Loss D: 0.0005, loss G: 8.0966\n",
      "Epoch [0/10] Batch 753/1583                       Loss D: 0.0006, loss G: 8.0940\n",
      "Epoch [0/10] Batch 754/1583                       Loss D: 0.0005, loss G: 8.0946\n",
      "Epoch [0/10] Batch 755/1583                       Loss D: 0.0005, loss G: 8.0985\n",
      "Epoch [0/10] Batch 756/1583                       Loss D: 0.0005, loss G: 8.0981\n",
      "Epoch [0/10] Batch 757/1583                       Loss D: 0.0005, loss G: 8.0977\n",
      "Epoch [0/10] Batch 758/1583                       Loss D: 0.0005, loss G: 8.0939\n",
      "Epoch [0/10] Batch 759/1583                       Loss D: 0.0005, loss G: 8.0933\n",
      "Epoch [0/10] Batch 760/1583                       Loss D: 0.0005, loss G: 8.0921\n",
      "Epoch [0/10] Batch 761/1583                       Loss D: 0.0005, loss G: 8.0888\n",
      "Epoch [0/10] Batch 762/1583                       Loss D: 0.0005, loss G: 8.0891\n",
      "Epoch [0/10] Batch 763/1583                       Loss D: 0.0006, loss G: 8.0876\n",
      "Epoch [0/10] Batch 764/1583                       Loss D: 0.0005, loss G: 8.0901\n",
      "Epoch [0/10] Batch 765/1583                       Loss D: 0.0005, loss G: 8.0932\n",
      "Epoch [0/10] Batch 766/1583                       Loss D: 0.0005, loss G: 8.0931\n",
      "Epoch [0/10] Batch 767/1583                       Loss D: 0.0005, loss G: 8.0954\n",
      "Epoch [0/10] Batch 768/1583                       Loss D: 0.0005, loss G: 8.0944\n",
      "Epoch [0/10] Batch 769/1583                       Loss D: 0.0005, loss G: 8.0942\n",
      "Epoch [0/10] Batch 770/1583                       Loss D: 0.0005, loss G: 8.0973\n",
      "Epoch [0/10] Batch 771/1583                       Loss D: 0.0005, loss G: 8.1025\n",
      "Epoch [0/10] Batch 772/1583                       Loss D: 0.0005, loss G: 8.1023\n",
      "Epoch [0/10] Batch 773/1583                       Loss D: 0.0006, loss G: 8.1013\n",
      "Epoch [0/10] Batch 774/1583                       Loss D: 0.0005, loss G: 8.0991\n",
      "Epoch [0/10] Batch 775/1583                       Loss D: 0.0005, loss G: 8.1041\n",
      "Epoch [0/10] Batch 776/1583                       Loss D: 0.0006, loss G: 8.1072\n",
      "Epoch [0/10] Batch 777/1583                       Loss D: 0.0005, loss G: 8.1144\n",
      "Epoch [0/10] Batch 778/1583                       Loss D: 0.0005, loss G: 8.1227\n",
      "Epoch [0/10] Batch 779/1583                       Loss D: 0.0005, loss G: 8.1286\n",
      "Epoch [0/10] Batch 780/1583                       Loss D: 0.0005, loss G: 8.1283\n",
      "Epoch [0/10] Batch 781/1583                       Loss D: 0.0005, loss G: 8.1238\n",
      "Epoch [0/10] Batch 782/1583                       Loss D: 0.0005, loss G: 8.1238\n",
      "Epoch [0/10] Batch 783/1583                       Loss D: 0.0005, loss G: 8.1286\n",
      "Epoch [0/10] Batch 784/1583                       Loss D: 0.0005, loss G: 8.1386\n",
      "Epoch [0/10] Batch 785/1583                       Loss D: 0.0005, loss G: 8.1463\n",
      "Epoch [0/10] Batch 786/1583                       Loss D: 0.0005, loss G: 8.1504\n",
      "Epoch [0/10] Batch 787/1583                       Loss D: 0.0005, loss G: 8.1516\n",
      "Epoch [0/10] Batch 788/1583                       Loss D: 0.0005, loss G: 8.1521\n",
      "Epoch [0/10] Batch 789/1583                       Loss D: 0.0005, loss G: 8.1535\n",
      "Epoch [0/10] Batch 790/1583                       Loss D: 0.0005, loss G: 8.1582\n",
      "Epoch [0/10] Batch 791/1583                       Loss D: 0.0005, loss G: 8.1660\n",
      "Epoch [0/10] Batch 792/1583                       Loss D: 0.0005, loss G: 8.1730\n",
      "Epoch [0/10] Batch 793/1583                       Loss D: 0.0005, loss G: 8.1758\n",
      "Epoch [0/10] Batch 794/1583                       Loss D: 0.0005, loss G: 8.1756\n",
      "Epoch [0/10] Batch 795/1583                       Loss D: 0.0005, loss G: 8.1770\n",
      "Epoch [0/10] Batch 796/1583                       Loss D: 0.0005, loss G: 8.1778\n",
      "Epoch [0/10] Batch 797/1583                       Loss D: 0.0005, loss G: 8.1808\n",
      "Epoch [0/10] Batch 798/1583                       Loss D: 0.0005, loss G: 8.1840\n",
      "Epoch [0/10] Batch 799/1583                       Loss D: 0.0005, loss G: 8.1918\n",
      "Epoch [0/10] Batch 800/1583                       Loss D: 0.0005, loss G: 8.1914\n",
      "Epoch [0/10] Batch 801/1583                       Loss D: 0.0005, loss G: 8.1953\n",
      "Epoch [0/10] Batch 802/1583                       Loss D: 0.0005, loss G: 8.1963\n",
      "Epoch [0/10] Batch 803/1583                       Loss D: 0.0005, loss G: 8.1982\n",
      "Epoch [0/10] Batch 804/1583                       Loss D: 0.0005, loss G: 8.2004\n",
      "Epoch [0/10] Batch 805/1583                       Loss D: 0.0005, loss G: 8.2046\n",
      "Epoch [0/10] Batch 806/1583                       Loss D: 0.0005, loss G: 8.2086\n",
      "Epoch [0/10] Batch 807/1583                       Loss D: 0.0005, loss G: 8.2118\n",
      "Epoch [0/10] Batch 808/1583                       Loss D: 0.0005, loss G: 8.2159\n",
      "Epoch [0/10] Batch 809/1583                       Loss D: 0.0005, loss G: 8.2107\n",
      "Epoch [0/10] Batch 810/1583                       Loss D: 0.0005, loss G: 8.2122\n",
      "Epoch [0/10] Batch 811/1583                       Loss D: 0.0005, loss G: 8.2134\n",
      "Epoch [0/10] Batch 812/1583                       Loss D: 0.0005, loss G: 8.2188\n",
      "Epoch [0/10] Batch 813/1583                       Loss D: 0.0004, loss G: 8.2252\n",
      "Epoch [0/10] Batch 814/1583                       Loss D: 0.0005, loss G: 8.2255\n",
      "Epoch [0/10] Batch 815/1583                       Loss D: 0.0005, loss G: 8.2251\n",
      "Epoch [0/10] Batch 816/1583                       Loss D: 0.0005, loss G: 8.2210\n",
      "Epoch [0/10] Batch 817/1583                       Loss D: 0.0005, loss G: 8.2233\n",
      "Epoch [0/10] Batch 818/1583                       Loss D: 0.0005, loss G: 8.2244\n",
      "Epoch [0/10] Batch 819/1583                       Loss D: 0.0005, loss G: 8.2255\n",
      "Epoch [0/10] Batch 820/1583                       Loss D: 0.0005, loss G: 8.2260\n",
      "Epoch [0/10] Batch 821/1583                       Loss D: 0.0004, loss G: 8.2298\n",
      "Epoch [0/10] Batch 822/1583                       Loss D: 0.0005, loss G: 8.2315\n",
      "Epoch [0/10] Batch 823/1583                       Loss D: 0.0005, loss G: 8.2341\n",
      "Epoch [0/10] Batch 824/1583                       Loss D: 0.0005, loss G: 8.2324\n",
      "Epoch [0/10] Batch 825/1583                       Loss D: 0.0005, loss G: 8.2346\n",
      "Epoch [0/10] Batch 826/1583                       Loss D: 0.0005, loss G: 8.2332\n",
      "Epoch [0/10] Batch 827/1583                       Loss D: 0.0004, loss G: 8.2355\n",
      "Epoch [0/10] Batch 828/1583                       Loss D: 0.0005, loss G: 8.2338\n",
      "Epoch [0/10] Batch 829/1583                       Loss D: 0.0005, loss G: 8.2306\n",
      "Epoch [0/10] Batch 830/1583                       Loss D: 0.0005, loss G: 8.2311\n",
      "Epoch [0/10] Batch 831/1583                       Loss D: 0.0004, loss G: 8.2336\n",
      "Epoch [0/10] Batch 832/1583                       Loss D: 0.0005, loss G: 8.2380\n",
      "Epoch [0/10] Batch 833/1583                       Loss D: 0.0004, loss G: 8.2412\n",
      "Epoch [0/10] Batch 834/1583                       Loss D: 0.0004, loss G: 8.2437\n",
      "Epoch [0/10] Batch 835/1583                       Loss D: 0.0004, loss G: 8.2441\n",
      "Epoch [0/10] Batch 836/1583                       Loss D: 0.0005, loss G: 8.2416\n",
      "Epoch [0/10] Batch 837/1583                       Loss D: 0.0005, loss G: 8.2415\n",
      "Epoch [0/10] Batch 838/1583                       Loss D: 0.0005, loss G: 8.2436\n",
      "Epoch [0/10] Batch 839/1583                       Loss D: 0.0005, loss G: 8.2444\n",
      "Epoch [0/10] Batch 840/1583                       Loss D: 0.0005, loss G: 8.2448\n",
      "Epoch [0/10] Batch 841/1583                       Loss D: 0.0004, loss G: 8.2457\n",
      "Epoch [0/10] Batch 842/1583                       Loss D: 0.0005, loss G: 8.2449\n",
      "Epoch [0/10] Batch 843/1583                       Loss D: 0.0005, loss G: 8.2446\n",
      "Epoch [0/10] Batch 844/1583                       Loss D: 0.0005, loss G: 8.2448\n",
      "Epoch [0/10] Batch 845/1583                       Loss D: 0.0004, loss G: 8.2481\n",
      "Epoch [0/10] Batch 846/1583                       Loss D: 0.0004, loss G: 8.2485\n",
      "Epoch [0/10] Batch 847/1583                       Loss D: 0.0005, loss G: 8.2507\n",
      "Epoch [0/10] Batch 848/1583                       Loss D: 0.0005, loss G: 8.2507\n",
      "Epoch [0/10] Batch 849/1583                       Loss D: 0.0005, loss G: 8.2532\n",
      "Epoch [0/10] Batch 850/1583                       Loss D: 0.0004, loss G: 8.2532\n",
      "Epoch [0/10] Batch 851/1583                       Loss D: 0.0004, loss G: 8.2570\n",
      "Epoch [0/10] Batch 852/1583                       Loss D: 0.0004, loss G: 8.2630\n",
      "Epoch [0/10] Batch 853/1583                       Loss D: 0.0004, loss G: 8.2662\n",
      "Epoch [0/10] Batch 854/1583                       Loss D: 0.0004, loss G: 8.2704\n",
      "Epoch [0/10] Batch 855/1583                       Loss D: 0.0005, loss G: 8.2711\n",
      "Epoch [0/10] Batch 856/1583                       Loss D: 0.0005, loss G: 8.2710\n",
      "Epoch [0/10] Batch 857/1583                       Loss D: 0.0004, loss G: 8.2722\n",
      "Epoch [0/10] Batch 858/1583                       Loss D: 0.0004, loss G: 8.2715\n",
      "Epoch [0/10] Batch 859/1583                       Loss D: 0.0004, loss G: 8.2743\n",
      "Epoch [0/10] Batch 860/1583                       Loss D: 0.0004, loss G: 8.2776\n",
      "Epoch [0/10] Batch 861/1583                       Loss D: 0.0005, loss G: 8.2802\n",
      "Epoch [0/10] Batch 862/1583                       Loss D: 0.0004, loss G: 8.2805\n",
      "Epoch [0/10] Batch 863/1583                       Loss D: 0.0004, loss G: 8.2834\n",
      "Epoch [0/10] Batch 864/1583                       Loss D: 0.0004, loss G: 8.2880\n",
      "Epoch [0/10] Batch 865/1583                       Loss D: 0.0004, loss G: 8.2911\n",
      "Epoch [0/10] Batch 866/1583                       Loss D: 0.0004, loss G: 8.2971\n",
      "Epoch [0/10] Batch 867/1583                       Loss D: 0.0004, loss G: 8.2981\n",
      "Epoch [0/10] Batch 868/1583                       Loss D: 0.0005, loss G: 8.2970\n",
      "Epoch [0/10] Batch 869/1583                       Loss D: 0.0004, loss G: 8.2956\n",
      "Epoch [0/10] Batch 870/1583                       Loss D: 0.0004, loss G: 8.2955\n",
      "Epoch [0/10] Batch 871/1583                       Loss D: 0.0005, loss G: 8.2973\n",
      "Epoch [0/10] Batch 872/1583                       Loss D: 0.0004, loss G: 8.3030\n",
      "Epoch [0/10] Batch 873/1583                       Loss D: 0.0004, loss G: 8.3126\n",
      "Epoch [0/10] Batch 874/1583                       Loss D: 0.0004, loss G: 8.3165\n",
      "Epoch [0/10] Batch 875/1583                       Loss D: 0.0004, loss G: 8.3168\n",
      "Epoch [0/10] Batch 876/1583                       Loss D: 0.0005, loss G: 8.3163\n",
      "Epoch [0/10] Batch 877/1583                       Loss D: 0.0004, loss G: 8.3215\n",
      "Epoch [0/10] Batch 878/1583                       Loss D: 0.0005, loss G: 8.3221\n",
      "Epoch [0/10] Batch 879/1583                       Loss D: 0.0004, loss G: 8.3268\n",
      "Epoch [0/10] Batch 880/1583                       Loss D: 0.0004, loss G: 8.3269\n",
      "Epoch [0/10] Batch 881/1583                       Loss D: 0.0004, loss G: 8.3244\n",
      "Epoch [0/10] Batch 882/1583                       Loss D: 0.0004, loss G: 8.3219\n",
      "Epoch [0/10] Batch 883/1583                       Loss D: 0.0004, loss G: 8.3231\n",
      "Epoch [0/10] Batch 884/1583                       Loss D: 0.0004, loss G: 8.3265\n",
      "Epoch [0/10] Batch 885/1583                       Loss D: 0.0004, loss G: 8.3281\n",
      "Epoch [0/10] Batch 886/1583                       Loss D: 0.0004, loss G: 8.3267\n",
      "Epoch [0/10] Batch 887/1583                       Loss D: 0.0004, loss G: 8.3253\n",
      "Epoch [0/10] Batch 888/1583                       Loss D: 0.0004, loss G: 8.3243\n",
      "Epoch [0/10] Batch 889/1583                       Loss D: 0.0004, loss G: 8.3259\n",
      "Epoch [0/10] Batch 890/1583                       Loss D: 0.0004, loss G: 8.3317\n",
      "Epoch [0/10] Batch 891/1583                       Loss D: 0.0004, loss G: 8.3363\n",
      "Epoch [0/10] Batch 892/1583                       Loss D: 0.0004, loss G: 8.3407\n",
      "Epoch [0/10] Batch 893/1583                       Loss D: 0.0004, loss G: 8.3450\n",
      "Epoch [0/10] Batch 894/1583                       Loss D: 0.0004, loss G: 8.3453\n",
      "Epoch [0/10] Batch 895/1583                       Loss D: 0.0004, loss G: 8.3471\n",
      "Epoch [0/10] Batch 896/1583                       Loss D: 0.0004, loss G: 8.3478\n",
      "Epoch [0/10] Batch 897/1583                       Loss D: 0.0004, loss G: 8.3479\n",
      "Epoch [0/10] Batch 898/1583                       Loss D: 0.0004, loss G: 8.3509\n",
      "Epoch [0/10] Batch 899/1583                       Loss D: 0.0004, loss G: 8.3513\n",
      "Epoch [0/10] Batch 900/1583                       Loss D: 0.0004, loss G: 8.3503\n",
      "Epoch [0/10] Batch 901/1583                       Loss D: 0.0004, loss G: 8.3530\n",
      "Epoch [0/10] Batch 902/1583                       Loss D: 0.0004, loss G: 8.3564\n",
      "Epoch [0/10] Batch 903/1583                       Loss D: 0.0004, loss G: 8.3613\n",
      "Epoch [0/10] Batch 904/1583                       Loss D: 0.0004, loss G: 8.3644\n",
      "Epoch [0/10] Batch 905/1583                       Loss D: 0.0004, loss G: 8.3657\n",
      "Epoch [0/10] Batch 906/1583                       Loss D: 0.0004, loss G: 8.3632\n",
      "Epoch [0/10] Batch 907/1583                       Loss D: 0.0004, loss G: 8.3654\n",
      "Epoch [0/10] Batch 908/1583                       Loss D: 0.0004, loss G: 8.3699\n",
      "Epoch [0/10] Batch 909/1583                       Loss D: 0.0004, loss G: 8.3761\n",
      "Epoch [0/10] Batch 910/1583                       Loss D: 0.0004, loss G: 8.3787\n",
      "Epoch [0/10] Batch 911/1583                       Loss D: 0.0004, loss G: 8.3831\n",
      "Epoch [0/10] Batch 912/1583                       Loss D: 0.0004, loss G: 8.3828\n",
      "Epoch [0/10] Batch 913/1583                       Loss D: 0.0004, loss G: 8.3823\n",
      "Epoch [0/10] Batch 914/1583                       Loss D: 0.0004, loss G: 8.3831\n",
      "Epoch [0/10] Batch 915/1583                       Loss D: 0.0004, loss G: 8.3875\n",
      "Epoch [0/10] Batch 916/1583                       Loss D: 0.0004, loss G: 8.3923\n",
      "Epoch [0/10] Batch 917/1583                       Loss D: 0.0004, loss G: 8.3987\n",
      "Epoch [0/10] Batch 918/1583                       Loss D: 0.0004, loss G: 8.4058\n",
      "Epoch [0/10] Batch 919/1583                       Loss D: 0.0004, loss G: 8.4096\n",
      "Epoch [0/10] Batch 920/1583                       Loss D: 0.0004, loss G: 8.4128\n",
      "Epoch [0/10] Batch 921/1583                       Loss D: 0.0004, loss G: 8.4110\n",
      "Epoch [0/10] Batch 922/1583                       Loss D: 0.0004, loss G: 8.4089\n",
      "Epoch [0/10] Batch 923/1583                       Loss D: 0.0004, loss G: 8.4095\n",
      "Epoch [0/10] Batch 924/1583                       Loss D: 0.0004, loss G: 8.4100\n",
      "Epoch [0/10] Batch 925/1583                       Loss D: 0.0004, loss G: 8.4095\n",
      "Epoch [0/10] Batch 926/1583                       Loss D: 0.0004, loss G: 8.4105\n",
      "Epoch [0/10] Batch 927/1583                       Loss D: 0.0004, loss G: 8.4171\n",
      "Epoch [0/10] Batch 928/1583                       Loss D: 0.0004, loss G: 8.4263\n",
      "Epoch [0/10] Batch 929/1583                       Loss D: 0.0004, loss G: 8.4340\n",
      "Epoch [0/10] Batch 930/1583                       Loss D: 0.0004, loss G: 8.4363\n",
      "Epoch [0/10] Batch 931/1583                       Loss D: 0.0004, loss G: 8.4386\n",
      "Epoch [0/10] Batch 932/1583                       Loss D: 0.0004, loss G: 8.4410\n",
      "Epoch [0/10] Batch 933/1583                       Loss D: 0.0004, loss G: 8.4400\n",
      "Epoch [0/10] Batch 934/1583                       Loss D: 0.0004, loss G: 8.4426\n",
      "Epoch [0/10] Batch 935/1583                       Loss D: 0.0004, loss G: 8.4467\n",
      "Epoch [0/10] Batch 936/1583                       Loss D: 0.0004, loss G: 8.4517\n",
      "Epoch [0/10] Batch 937/1583                       Loss D: 0.0004, loss G: 8.4515\n",
      "Epoch [0/10] Batch 938/1583                       Loss D: 0.0004, loss G: 8.4525\n",
      "Epoch [0/10] Batch 939/1583                       Loss D: 0.0004, loss G: 8.4550\n",
      "Epoch [0/10] Batch 940/1583                       Loss D: 0.0004, loss G: 8.4564\n",
      "Epoch [0/10] Batch 941/1583                       Loss D: 0.0004, loss G: 8.4635\n",
      "Epoch [0/10] Batch 942/1583                       Loss D: 0.0004, loss G: 8.4708\n",
      "Epoch [0/10] Batch 943/1583                       Loss D: 0.0004, loss G: 8.4738\n",
      "Epoch [0/10] Batch 944/1583                       Loss D: 0.0004, loss G: 8.4730\n",
      "Epoch [0/10] Batch 945/1583                       Loss D: 0.0004, loss G: 8.4699\n",
      "Epoch [0/10] Batch 946/1583                       Loss D: 0.0004, loss G: 8.4683\n",
      "Epoch [0/10] Batch 947/1583                       Loss D: 0.0004, loss G: 8.4709\n",
      "Epoch [0/10] Batch 948/1583                       Loss D: 0.0004, loss G: 8.4771\n",
      "Epoch [0/10] Batch 949/1583                       Loss D: 0.0004, loss G: 8.4863\n",
      "Epoch [0/10] Batch 950/1583                       Loss D: 0.0003, loss G: 8.4945\n",
      "Epoch [0/10] Batch 951/1583                       Loss D: 0.0004, loss G: 8.4974\n",
      "Epoch [0/10] Batch 952/1583                       Loss D: 0.0004, loss G: 8.4963\n",
      "Epoch [0/10] Batch 953/1583                       Loss D: 0.0004, loss G: 8.4938\n",
      "Epoch [0/10] Batch 954/1583                       Loss D: 0.0004, loss G: 8.4936\n",
      "Epoch [0/10] Batch 955/1583                       Loss D: 0.0004, loss G: 8.4973\n",
      "Epoch [0/10] Batch 956/1583                       Loss D: 0.0004, loss G: 8.5021\n",
      "Epoch [0/10] Batch 957/1583                       Loss D: 0.0004, loss G: 8.5058\n",
      "Epoch [0/10] Batch 958/1583                       Loss D: 0.0004, loss G: 8.5109\n",
      "Epoch [0/10] Batch 959/1583                       Loss D: 0.0004, loss G: 8.5158\n",
      "Epoch [0/10] Batch 960/1583                       Loss D: 0.0003, loss G: 8.5220\n",
      "Epoch [0/10] Batch 961/1583                       Loss D: 0.0004, loss G: 8.5243\n",
      "Epoch [0/10] Batch 962/1583                       Loss D: 0.0004, loss G: 8.5256\n",
      "Epoch [0/10] Batch 963/1583                       Loss D: 0.0003, loss G: 8.5276\n",
      "Epoch [0/10] Batch 964/1583                       Loss D: 0.0004, loss G: 8.5317\n",
      "Epoch [0/10] Batch 965/1583                       Loss D: 0.0004, loss G: 8.5332\n",
      "Epoch [0/10] Batch 966/1583                       Loss D: 0.0004, loss G: 8.5368\n",
      "Epoch [0/10] Batch 967/1583                       Loss D: 0.0004, loss G: 8.5394\n",
      "Epoch [0/10] Batch 968/1583                       Loss D: 0.0004, loss G: 8.5459\n",
      "Epoch [0/10] Batch 969/1583                       Loss D: 0.0004, loss G: 8.5512\n",
      "Epoch [0/10] Batch 970/1583                       Loss D: 0.0004, loss G: 8.5540\n",
      "Epoch [0/10] Batch 971/1583                       Loss D: 0.0003, loss G: 8.5581\n",
      "Epoch [0/10] Batch 972/1583                       Loss D: 0.0003, loss G: 8.5625\n",
      "Epoch [0/10] Batch 973/1583                       Loss D: 0.0004, loss G: 8.5661\n",
      "Epoch [0/10] Batch 974/1583                       Loss D: 0.0003, loss G: 8.5762\n",
      "Epoch [0/10] Batch 975/1583                       Loss D: 0.0003, loss G: 8.5787\n",
      "Epoch [0/10] Batch 976/1583                       Loss D: 0.0003, loss G: 8.5811\n",
      "Epoch [0/10] Batch 977/1583                       Loss D: 0.0003, loss G: 8.5833\n",
      "Epoch [0/10] Batch 978/1583                       Loss D: 0.0004, loss G: 8.5861\n",
      "Epoch [0/10] Batch 979/1583                       Loss D: 0.0004, loss G: 8.5885\n",
      "Epoch [0/10] Batch 980/1583                       Loss D: 0.0003, loss G: 8.5942\n",
      "Epoch [0/10] Batch 981/1583                       Loss D: 0.0003, loss G: 8.5988\n",
      "Epoch [0/10] Batch 982/1583                       Loss D: 0.0003, loss G: 8.6045\n",
      "Epoch [0/10] Batch 983/1583                       Loss D: 0.0003, loss G: 8.6075\n",
      "Epoch [0/10] Batch 984/1583                       Loss D: 0.0003, loss G: 8.6076\n",
      "Epoch [0/10] Batch 985/1583                       Loss D: 0.0004, loss G: 8.6065\n",
      "Epoch [0/10] Batch 986/1583                       Loss D: 0.0003, loss G: 8.6078\n",
      "Epoch [0/10] Batch 987/1583                       Loss D: 0.0003, loss G: 8.6113\n",
      "Epoch [0/10] Batch 988/1583                       Loss D: 0.0003, loss G: 8.6160\n",
      "Epoch [0/10] Batch 989/1583                       Loss D: 0.0003, loss G: 8.6211\n",
      "Epoch [0/10] Batch 990/1583                       Loss D: 0.0003, loss G: 8.6271\n",
      "Epoch [0/10] Batch 991/1583                       Loss D: 0.0003, loss G: 8.6315\n",
      "Epoch [0/10] Batch 992/1583                       Loss D: 0.0003, loss G: 8.6337\n",
      "Epoch [0/10] Batch 993/1583                       Loss D: 0.0003, loss G: 8.6361\n",
      "Epoch [0/10] Batch 994/1583                       Loss D: 0.0004, loss G: 8.6364\n",
      "Epoch [0/10] Batch 995/1583                       Loss D: 0.0003, loss G: 8.6401\n",
      "Epoch [0/10] Batch 996/1583                       Loss D: 0.0003, loss G: 8.6444\n",
      "Epoch [0/10] Batch 997/1583                       Loss D: 0.0003, loss G: 8.6509\n",
      "Epoch [0/10] Batch 998/1583                       Loss D: 0.0003, loss G: 8.6525\n",
      "Epoch [0/10] Batch 999/1583                       Loss D: 0.0004, loss G: 8.6517\n",
      "Epoch [0/10] Batch 1000/1583                       Loss D: 0.0003, loss G: 8.6514\n",
      "Epoch [0/10] Batch 1001/1583                       Loss D: 0.0003, loss G: 8.6506\n",
      "Epoch [0/10] Batch 1002/1583                       Loss D: 0.0003, loss G: 8.6536\n",
      "Epoch [0/10] Batch 1003/1583                       Loss D: 0.0003, loss G: 8.6580\n",
      "Epoch [0/10] Batch 1004/1583                       Loss D: 0.0003, loss G: 8.6613\n",
      "Epoch [0/10] Batch 1005/1583                       Loss D: 0.0003, loss G: 8.6631\n",
      "Epoch [0/10] Batch 1006/1583                       Loss D: 0.0003, loss G: 8.6636\n",
      "Epoch [0/10] Batch 1007/1583                       Loss D: 0.0003, loss G: 8.6658\n",
      "Epoch [0/10] Batch 1008/1583                       Loss D: 0.0003, loss G: 8.6694\n",
      "Epoch [0/10] Batch 1009/1583                       Loss D: 0.0003, loss G: 8.6757\n",
      "Epoch [0/10] Batch 1010/1583                       Loss D: 0.0003, loss G: 8.6820\n",
      "Epoch [0/10] Batch 1011/1583                       Loss D: 0.0003, loss G: 8.6865\n",
      "Epoch [0/10] Batch 1012/1583                       Loss D: 0.0003, loss G: 8.6872\n",
      "Epoch [0/10] Batch 1013/1583                       Loss D: 0.0003, loss G: 8.6874\n",
      "Epoch [0/10] Batch 1014/1583                       Loss D: 0.0003, loss G: 8.6878\n",
      "Epoch [0/10] Batch 1015/1583                       Loss D: 0.0003, loss G: 8.6917\n",
      "Epoch [0/10] Batch 1016/1583                       Loss D: 0.0003, loss G: 8.6957\n",
      "Epoch [0/10] Batch 1017/1583                       Loss D: 0.0003, loss G: 8.7017\n",
      "Epoch [0/10] Batch 1018/1583                       Loss D: 0.0003, loss G: 8.7065\n",
      "Epoch [0/10] Batch 1019/1583                       Loss D: 0.0003, loss G: 8.7102\n",
      "Epoch [0/10] Batch 1020/1583                       Loss D: 0.0004, loss G: 8.7052\n",
      "Epoch [0/10] Batch 1021/1583                       Loss D: 0.0003, loss G: 8.7050\n",
      "Epoch [0/10] Batch 1022/1583                       Loss D: 0.0003, loss G: 8.7070\n",
      "Epoch [0/10] Batch 1023/1583                       Loss D: 0.0003, loss G: 8.7092\n",
      "Epoch [0/10] Batch 1024/1583                       Loss D: 0.0003, loss G: 8.7116\n",
      "Epoch [0/10] Batch 1025/1583                       Loss D: 0.0003, loss G: 8.7129\n",
      "Epoch [0/10] Batch 1026/1583                       Loss D: 0.0003, loss G: 8.7142\n",
      "Epoch [0/10] Batch 1027/1583                       Loss D: 0.0003, loss G: 8.7169\n",
      "Epoch [0/10] Batch 1028/1583                       Loss D: 0.0003, loss G: 8.7193\n",
      "Epoch [0/10] Batch 1029/1583                       Loss D: 0.0003, loss G: 8.7227\n",
      "Epoch [0/10] Batch 1030/1583                       Loss D: 0.0003, loss G: 8.7269\n",
      "Epoch [0/10] Batch 1031/1583                       Loss D: 0.0003, loss G: 8.7308\n",
      "Epoch [0/10] Batch 1032/1583                       Loss D: 0.0003, loss G: 8.7333\n",
      "Epoch [0/10] Batch 1033/1583                       Loss D: 0.0003, loss G: 8.7316\n",
      "Epoch [0/10] Batch 1034/1583                       Loss D: 0.0003, loss G: 8.7327\n",
      "Epoch [0/10] Batch 1035/1583                       Loss D: 0.0003, loss G: 8.7353\n",
      "Epoch [0/10] Batch 1036/1583                       Loss D: 0.0003, loss G: 8.7391\n",
      "Epoch [0/10] Batch 1037/1583                       Loss D: 0.0003, loss G: 8.7407\n",
      "Epoch [0/10] Batch 1038/1583                       Loss D: 0.0003, loss G: 8.7444\n",
      "Epoch [0/10] Batch 1039/1583                       Loss D: 0.0003, loss G: 8.7457\n",
      "Epoch [0/10] Batch 1040/1583                       Loss D: 0.0003, loss G: 8.7482\n",
      "Epoch [0/10] Batch 1041/1583                       Loss D: 0.0003, loss G: 8.7498\n",
      "Epoch [0/10] Batch 1042/1583                       Loss D: 0.0003, loss G: 8.7516\n",
      "Epoch [0/10] Batch 1043/1583                       Loss D: 0.0003, loss G: 8.7537\n",
      "Epoch [0/10] Batch 1044/1583                       Loss D: 0.0003, loss G: 8.7553\n",
      "Epoch [0/10] Batch 1045/1583                       Loss D: 0.0003, loss G: 8.7566\n",
      "Epoch [0/10] Batch 1046/1583                       Loss D: 0.0003, loss G: 8.7560\n",
      "Epoch [0/10] Batch 1047/1583                       Loss D: 0.0003, loss G: 8.7571\n",
      "Epoch [0/10] Batch 1048/1583                       Loss D: 0.0003, loss G: 8.7563\n",
      "Epoch [0/10] Batch 1049/1583                       Loss D: 0.0003, loss G: 8.7574\n",
      "Epoch [0/10] Batch 1050/1583                       Loss D: 0.0003, loss G: 8.7575\n",
      "Epoch [0/10] Batch 1051/1583                       Loss D: 0.0003, loss G: 8.7586\n",
      "Epoch [0/10] Batch 1052/1583                       Loss D: 0.0003, loss G: 8.7584\n",
      "Epoch [0/10] Batch 1053/1583                       Loss D: 0.0003, loss G: 8.7608\n",
      "Epoch [0/10] Batch 1054/1583                       Loss D: 0.0003, loss G: 8.7621\n",
      "Epoch [0/10] Batch 1055/1583                       Loss D: 0.0003, loss G: 8.7614\n",
      "Epoch [0/10] Batch 1056/1583                       Loss D: 0.0003, loss G: 8.7593\n",
      "Epoch [0/10] Batch 1057/1583                       Loss D: 0.0003, loss G: 8.7561\n",
      "Epoch [0/10] Batch 1058/1583                       Loss D: 0.0003, loss G: 8.7528\n",
      "Epoch [0/10] Batch 1059/1583                       Loss D: 0.0003, loss G: 8.7525\n",
      "Epoch [0/10] Batch 1060/1583                       Loss D: 0.0003, loss G: 8.7544\n",
      "Epoch [0/10] Batch 1061/1583                       Loss D: 0.0003, loss G: 8.7558\n",
      "Epoch [0/10] Batch 1062/1583                       Loss D: 0.0003, loss G: 8.7552\n",
      "Epoch [0/10] Batch 1063/1583                       Loss D: 0.0003, loss G: 8.7527\n",
      "Epoch [0/10] Batch 1064/1583                       Loss D: 0.0003, loss G: 8.7518\n",
      "Epoch [0/10] Batch 1065/1583                       Loss D: 0.0003, loss G: 8.7517\n",
      "Epoch [0/10] Batch 1066/1583                       Loss D: 0.0003, loss G: 8.7528\n",
      "Epoch [0/10] Batch 1067/1583                       Loss D: 0.0003, loss G: 8.7526\n",
      "Epoch [0/10] Batch 1068/1583                       Loss D: 0.0003, loss G: 8.7531\n",
      "Epoch [0/10] Batch 1069/1583                       Loss D: 0.0003, loss G: 8.7541\n",
      "Epoch [0/10] Batch 1070/1583                       Loss D: 0.0003, loss G: 8.7529\n",
      "Epoch [0/10] Batch 1071/1583                       Loss D: 0.0003, loss G: 8.7528\n",
      "Epoch [0/10] Batch 1072/1583                       Loss D: 0.0003, loss G: 8.7530\n",
      "Epoch [0/10] Batch 1073/1583                       Loss D: 0.0003, loss G: 8.7538\n",
      "Epoch [0/10] Batch 1074/1583                       Loss D: 0.0003, loss G: 8.7554\n",
      "Epoch [0/10] Batch 1075/1583                       Loss D: 0.0003, loss G: 8.7557\n",
      "Epoch [0/10] Batch 1076/1583                       Loss D: 0.0003, loss G: 8.7579\n",
      "Epoch [0/10] Batch 1077/1583                       Loss D: 0.0003, loss G: 8.7606\n",
      "Epoch [0/10] Batch 1078/1583                       Loss D: 0.0003, loss G: 8.7641\n",
      "Epoch [0/10] Batch 1079/1583                       Loss D: 0.0003, loss G: 8.7745\n",
      "Epoch [0/10] Batch 1080/1583                       Loss D: 0.0003, loss G: 8.7742\n",
      "Epoch [0/10] Batch 1081/1583                       Loss D: 0.0003, loss G: 8.7736\n",
      "Epoch [0/10] Batch 1082/1583                       Loss D: 0.0003, loss G: 8.7727\n",
      "Epoch [0/10] Batch 1083/1583                       Loss D: 0.0003, loss G: 8.7689\n",
      "Epoch [0/10] Batch 1084/1583                       Loss D: 0.0003, loss G: 8.7690\n",
      "Epoch [0/10] Batch 1085/1583                       Loss D: 0.0003, loss G: 8.7705\n",
      "Epoch [0/10] Batch 1086/1583                       Loss D: 0.0003, loss G: 8.7752\n",
      "Epoch [0/10] Batch 1087/1583                       Loss D: 0.0003, loss G: 8.7810\n",
      "Epoch [0/10] Batch 1088/1583                       Loss D: 0.0003, loss G: 8.7823\n",
      "Epoch [0/10] Batch 1089/1583                       Loss D: 0.0003, loss G: 8.7807\n",
      "Epoch [0/10] Batch 1090/1583                       Loss D: 0.0003, loss G: 8.7785\n",
      "Epoch [0/10] Batch 1091/1583                       Loss D: 0.0003, loss G: 8.7780\n",
      "Epoch [0/10] Batch 1092/1583                       Loss D: 0.0003, loss G: 8.7790\n",
      "Epoch [0/10] Batch 1093/1583                       Loss D: 0.0003, loss G: 8.7818\n",
      "Epoch [0/10] Batch 1094/1583                       Loss D: 0.0003, loss G: 8.7828\n",
      "Epoch [0/10] Batch 1095/1583                       Loss D: 0.0003, loss G: 8.7861\n",
      "Epoch [0/10] Batch 1096/1583                       Loss D: 0.0003, loss G: 8.7860\n",
      "Epoch [0/10] Batch 1097/1583                       Loss D: 0.0003, loss G: 8.7854\n",
      "Epoch [0/10] Batch 1098/1583                       Loss D: 0.0003, loss G: 8.7844\n",
      "Epoch [0/10] Batch 1099/1583                       Loss D: 0.0003, loss G: 8.7849\n",
      "Epoch [0/10] Batch 1100/1583                       Loss D: 0.0003, loss G: 8.7869\n",
      "Epoch [0/10] Batch 1101/1583                       Loss D: 0.0003, loss G: 8.7884\n",
      "Epoch [0/10] Batch 1102/1583                       Loss D: 0.0003, loss G: 8.7880\n",
      "Epoch [0/10] Batch 1103/1583                       Loss D: 0.0003, loss G: 8.7879\n",
      "Epoch [0/10] Batch 1104/1583                       Loss D: 0.0003, loss G: 8.7885\n",
      "Epoch [0/10] Batch 1105/1583                       Loss D: 0.0003, loss G: 8.7883\n",
      "Epoch [0/10] Batch 1106/1583                       Loss D: 0.0003, loss G: 8.7889\n",
      "Epoch [0/10] Batch 1107/1583                       Loss D: 0.0003, loss G: 8.7891\n",
      "Epoch [0/10] Batch 1108/1583                       Loss D: 0.0003, loss G: 8.7896\n",
      "Epoch [0/10] Batch 1109/1583                       Loss D: 0.0003, loss G: 8.7891\n",
      "Epoch [0/10] Batch 1110/1583                       Loss D: 0.0003, loss G: 8.7882\n",
      "Epoch [0/10] Batch 1111/1583                       Loss D: 0.0003, loss G: 8.7879\n",
      "Epoch [0/10] Batch 1112/1583                       Loss D: 0.0003, loss G: 8.7861\n",
      "Epoch [0/10] Batch 1113/1583                       Loss D: 0.0003, loss G: 8.7853\n",
      "Epoch [0/10] Batch 1114/1583                       Loss D: 0.0003, loss G: 8.7852\n",
      "Epoch [0/10] Batch 1115/1583                       Loss D: 0.0003, loss G: 8.7847\n",
      "Epoch [0/10] Batch 1116/1583                       Loss D: 0.0003, loss G: 8.7842\n",
      "Epoch [0/10] Batch 1117/1583                       Loss D: 0.0003, loss G: 8.7832\n",
      "Epoch [0/10] Batch 1118/1583                       Loss D: 0.0002, loss G: 8.7811\n",
      "Epoch [0/10] Batch 1119/1583                       Loss D: 0.0003, loss G: 8.7774\n",
      "Epoch [0/10] Batch 1120/1583                       Loss D: 0.0003, loss G: 8.7744\n",
      "Epoch [0/10] Batch 1121/1583                       Loss D: 0.0003, loss G: 8.7731\n",
      "Epoch [0/10] Batch 1122/1583                       Loss D: 0.0003, loss G: 8.7713\n",
      "Epoch [0/10] Batch 1123/1583                       Loss D: 0.0003, loss G: 8.7671\n",
      "Epoch [0/10] Batch 1124/1583                       Loss D: 0.0003, loss G: 8.7629\n",
      "Epoch [0/10] Batch 1125/1583                       Loss D: 0.0003, loss G: 8.7582\n",
      "Epoch [0/10] Batch 1126/1583                       Loss D: 0.0003, loss G: 8.7567\n",
      "Epoch [0/10] Batch 1127/1583                       Loss D: 0.0003, loss G: 8.7564\n",
      "Epoch [0/10] Batch 1128/1583                       Loss D: 0.0003, loss G: 8.7539\n",
      "Epoch [0/10] Batch 1129/1583                       Loss D: 0.0003, loss G: 8.7510\n",
      "Epoch [0/10] Batch 1130/1583                       Loss D: 0.0003, loss G: 8.7497\n",
      "Epoch [0/10] Batch 1131/1583                       Loss D: 0.0003, loss G: 8.7484\n",
      "Epoch [0/10] Batch 1132/1583                       Loss D: 0.0003, loss G: 8.7469\n",
      "Epoch [0/10] Batch 1133/1583                       Loss D: 0.0003, loss G: 8.7462\n",
      "Epoch [0/10] Batch 1134/1583                       Loss D: 0.0003, loss G: 8.7459\n",
      "Epoch [0/10] Batch 1135/1583                       Loss D: 0.0003, loss G: 8.7434\n",
      "Epoch [0/10] Batch 1136/1583                       Loss D: 0.0003, loss G: 8.7427\n",
      "Epoch [0/10] Batch 1137/1583                       Loss D: 0.0003, loss G: 8.7453\n",
      "Epoch [0/10] Batch 1138/1583                       Loss D: 0.0003, loss G: 8.7457\n",
      "Epoch [0/10] Batch 1139/1583                       Loss D: 0.0003, loss G: 8.7450\n",
      "Epoch [0/10] Batch 1140/1583                       Loss D: 0.0003, loss G: 8.7432\n",
      "Epoch [0/10] Batch 1141/1583                       Loss D: 0.0003, loss G: 8.7426\n",
      "Epoch [0/10] Batch 1142/1583                       Loss D: 0.0003, loss G: 8.7426\n",
      "Epoch [0/10] Batch 1143/1583                       Loss D: 0.0003, loss G: 8.7424\n",
      "Epoch [0/10] Batch 1144/1583                       Loss D: 0.0003, loss G: 8.7461\n",
      "Epoch [0/10] Batch 1145/1583                       Loss D: 0.0003, loss G: 8.7512\n",
      "Epoch [0/10] Batch 1146/1583                       Loss D: 0.0003, loss G: 8.7540\n",
      "Epoch [0/10] Batch 1147/1583                       Loss D: 0.0003, loss G: 8.7555\n",
      "Epoch [0/10] Batch 1148/1583                       Loss D: 0.0003, loss G: 8.7529\n",
      "Epoch [0/10] Batch 1149/1583                       Loss D: 0.0003, loss G: 8.7508\n",
      "Epoch [0/10] Batch 1150/1583                       Loss D: 0.0003, loss G: 8.7527\n",
      "Epoch [0/10] Batch 1151/1583                       Loss D: 0.0003, loss G: 8.7539\n",
      "Epoch [0/10] Batch 1152/1583                       Loss D: 0.0003, loss G: 8.7584\n",
      "Epoch [0/10] Batch 1153/1583                       Loss D: 0.0003, loss G: 8.7661\n",
      "Epoch [0/10] Batch 1154/1583                       Loss D: 0.0003, loss G: 8.7717\n",
      "Epoch [0/10] Batch 1155/1583                       Loss D: 0.0003, loss G: 8.7732\n",
      "Epoch [0/10] Batch 1156/1583                       Loss D: 0.0003, loss G: 8.7722\n",
      "Epoch [0/10] Batch 1157/1583                       Loss D: 0.0003, loss G: 8.7727\n",
      "Epoch [0/10] Batch 1158/1583                       Loss D: 0.0003, loss G: 8.7773\n",
      "Epoch [0/10] Batch 1159/1583                       Loss D: 0.0003, loss G: 8.7820\n",
      "Epoch [0/10] Batch 1160/1583                       Loss D: 0.0003, loss G: 8.7852\n",
      "Epoch [0/10] Batch 1161/1583                       Loss D: 0.0002, loss G: 8.7858\n",
      "Epoch [0/10] Batch 1162/1583                       Loss D: 0.0003, loss G: 8.7852\n",
      "Epoch [0/10] Batch 1163/1583                       Loss D: 0.0002, loss G: 8.7857\n",
      "Epoch [0/10] Batch 1164/1583                       Loss D: 0.0002, loss G: 8.7887\n",
      "Epoch [0/10] Batch 1165/1583                       Loss D: 0.0003, loss G: 8.7919\n",
      "Epoch [0/10] Batch 1166/1583                       Loss D: 0.0003, loss G: 8.7953\n",
      "Epoch [0/10] Batch 1167/1583                       Loss D: 0.0003, loss G: 8.8009\n",
      "Epoch [0/10] Batch 1168/1583                       Loss D: 0.0003, loss G: 8.8077\n",
      "Epoch [0/10] Batch 1169/1583                       Loss D: 0.0002, loss G: 8.8132\n",
      "Epoch [0/10] Batch 1170/1583                       Loss D: 0.0003, loss G: 8.8154\n",
      "Epoch [0/10] Batch 1171/1583                       Loss D: 0.0003, loss G: 8.8167\n",
      "Epoch [0/10] Batch 1172/1583                       Loss D: 0.0002, loss G: 8.8178\n",
      "Epoch [0/10] Batch 1173/1583                       Loss D: 0.0003, loss G: 8.8197\n",
      "Epoch [0/10] Batch 1174/1583                       Loss D: 0.0003, loss G: 8.8237\n",
      "Epoch [0/10] Batch 1175/1583                       Loss D: 0.0003, loss G: 8.8263\n",
      "Epoch [0/10] Batch 1176/1583                       Loss D: 0.0003, loss G: 8.8318\n",
      "Epoch [0/10] Batch 1177/1583                       Loss D: 0.0002, loss G: 8.8382\n",
      "Epoch [0/10] Batch 1178/1583                       Loss D: 0.0002, loss G: 8.8410\n",
      "Epoch [0/10] Batch 1179/1583                       Loss D: 0.0003, loss G: 8.8433\n",
      "Epoch [0/10] Batch 1180/1583                       Loss D: 0.0003, loss G: 8.8461\n",
      "Epoch [0/10] Batch 1181/1583                       Loss D: 0.0002, loss G: 8.8489\n",
      "Epoch [0/10] Batch 1182/1583                       Loss D: 0.0003, loss G: 8.8498\n",
      "Epoch [0/10] Batch 1183/1583                       Loss D: 0.0002, loss G: 8.8526\n",
      "Epoch [0/10] Batch 1184/1583                       Loss D: 0.0002, loss G: 8.8559\n",
      "Epoch [0/10] Batch 1185/1583                       Loss D: 0.0003, loss G: 8.8589\n",
      "Epoch [0/10] Batch 1186/1583                       Loss D: 0.0002, loss G: 8.8612\n",
      "Epoch [0/10] Batch 1187/1583                       Loss D: 0.0002, loss G: 8.8664\n",
      "Epoch [0/10] Batch 1188/1583                       Loss D: 0.0002, loss G: 8.8725\n",
      "Epoch [0/10] Batch 1189/1583                       Loss D: 0.0003, loss G: 8.8739\n",
      "Epoch [0/10] Batch 1190/1583                       Loss D: 0.0002, loss G: 8.8731\n",
      "Epoch [0/10] Batch 1191/1583                       Loss D: 0.0002, loss G: 8.8740\n",
      "Epoch [0/10] Batch 1192/1583                       Loss D: 0.0003, loss G: 8.8756\n",
      "Epoch [0/10] Batch 1193/1583                       Loss D: 0.0002, loss G: 8.8794\n",
      "Epoch [0/10] Batch 1194/1583                       Loss D: 0.0002, loss G: 8.8872\n",
      "Epoch [0/10] Batch 1195/1583                       Loss D: 0.0002, loss G: 8.8939\n",
      "Epoch [0/10] Batch 1196/1583                       Loss D: 0.0002, loss G: 8.8952\n",
      "Epoch [0/10] Batch 1197/1583                       Loss D: 0.0002, loss G: 8.8961\n",
      "Epoch [0/10] Batch 1198/1583                       Loss D: 0.0002, loss G: 8.8981\n",
      "Epoch [0/10] Batch 1199/1583                       Loss D: 0.0003, loss G: 8.9006\n",
      "Epoch [0/10] Batch 1200/1583                       Loss D: 0.0002, loss G: 8.9045\n",
      "Epoch [0/10] Batch 1201/1583                       Loss D: 0.0002, loss G: 8.9113\n",
      "Epoch [0/10] Batch 1202/1583                       Loss D: 0.0002, loss G: 8.9180\n",
      "Epoch [0/10] Batch 1203/1583                       Loss D: 0.0002, loss G: 8.9234\n",
      "Epoch [0/10] Batch 1204/1583                       Loss D: 0.0002, loss G: 8.9263\n",
      "Epoch [0/10] Batch 1205/1583                       Loss D: 0.0002, loss G: 8.9270\n",
      "Epoch [0/10] Batch 1206/1583                       Loss D: 0.0002, loss G: 8.9305\n",
      "Epoch [0/10] Batch 1207/1583                       Loss D: 0.0002, loss G: 8.9364\n",
      "Epoch [0/10] Batch 1208/1583                       Loss D: 0.0002, loss G: 8.9415\n",
      "Epoch [0/10] Batch 1209/1583                       Loss D: 0.0002, loss G: 8.9465\n",
      "Epoch [0/10] Batch 1210/1583                       Loss D: 0.0002, loss G: 8.9499\n",
      "Epoch [0/10] Batch 1211/1583                       Loss D: 0.0002, loss G: 8.9537\n",
      "Epoch [0/10] Batch 1212/1583                       Loss D: 0.0002, loss G: 8.9570\n",
      "Epoch [0/10] Batch 1213/1583                       Loss D: 0.0002, loss G: 8.9599\n",
      "Epoch [0/10] Batch 1214/1583                       Loss D: 0.0003, loss G: 8.9605\n",
      "Epoch [0/10] Batch 1215/1583                       Loss D: 0.0002, loss G: 8.9637\n",
      "Epoch [0/10] Batch 1216/1583                       Loss D: 0.0002, loss G: 8.9686\n",
      "Epoch [0/10] Batch 1217/1583                       Loss D: 0.0002, loss G: 8.9736\n",
      "Epoch [0/10] Batch 1218/1583                       Loss D: 0.0002, loss G: 8.9777\n",
      "Epoch [0/10] Batch 1219/1583                       Loss D: 0.0002, loss G: 8.9822\n",
      "Epoch [0/10] Batch 1220/1583                       Loss D: 0.0002, loss G: 8.9862\n",
      "Epoch [0/10] Batch 1221/1583                       Loss D: 0.0002, loss G: 8.9905\n",
      "Epoch [0/10] Batch 1222/1583                       Loss D: 0.0002, loss G: 8.9945\n",
      "Epoch [0/10] Batch 1223/1583                       Loss D: 0.0002, loss G: 8.9976\n",
      "Epoch [0/10] Batch 1224/1583                       Loss D: 0.0002, loss G: 9.0028\n",
      "Epoch [0/10] Batch 1225/1583                       Loss D: 0.0002, loss G: 9.0071\n",
      "Epoch [0/10] Batch 1226/1583                       Loss D: 0.0002, loss G: 9.0105\n",
      "Epoch [0/10] Batch 1227/1583                       Loss D: 0.0002, loss G: 9.0144\n",
      "Epoch [0/10] Batch 1228/1583                       Loss D: 0.0002, loss G: 9.0179\n",
      "Epoch [0/10] Batch 1229/1583                       Loss D: 0.0002, loss G: 9.0207\n",
      "Epoch [0/10] Batch 1230/1583                       Loss D: 0.0002, loss G: 9.0253\n",
      "Epoch [0/10] Batch 1231/1583                       Loss D: 0.0002, loss G: 9.0283\n",
      "Epoch [0/10] Batch 1232/1583                       Loss D: 0.0002, loss G: 9.0329\n",
      "Epoch [0/10] Batch 1233/1583                       Loss D: 0.0002, loss G: 9.0381\n",
      "Epoch [0/10] Batch 1234/1583                       Loss D: 0.0002, loss G: 9.0433\n",
      "Epoch [0/10] Batch 1235/1583                       Loss D: 0.0002, loss G: 9.0459\n",
      "Epoch [0/10] Batch 1236/1583                       Loss D: 0.0002, loss G: 9.0470\n",
      "Epoch [0/10] Batch 1237/1583                       Loss D: 0.0002, loss G: 9.0483\n",
      "Epoch [0/10] Batch 1238/1583                       Loss D: 0.0002, loss G: 9.0516\n",
      "Epoch [0/10] Batch 1239/1583                       Loss D: 0.0002, loss G: 9.0551\n",
      "Epoch [0/10] Batch 1240/1583                       Loss D: 0.0002, loss G: 9.0594\n",
      "Epoch [0/10] Batch 1241/1583                       Loss D: 0.0002, loss G: 9.0649\n",
      "Epoch [0/10] Batch 1242/1583                       Loss D: 0.0002, loss G: 9.0699\n",
      "Epoch [0/10] Batch 1243/1583                       Loss D: 0.0002, loss G: 9.0738\n",
      "Epoch [0/10] Batch 1244/1583                       Loss D: 0.0002, loss G: 9.0772\n",
      "Epoch [0/10] Batch 1245/1583                       Loss D: 0.0002, loss G: 9.0811\n",
      "Epoch [0/10] Batch 1246/1583                       Loss D: 0.0002, loss G: 9.0861\n",
      "Epoch [0/10] Batch 1247/1583                       Loss D: 0.0002, loss G: 9.0903\n",
      "Epoch [0/10] Batch 1248/1583                       Loss D: 0.0002, loss G: 9.0954\n",
      "Epoch [0/10] Batch 1249/1583                       Loss D: 0.0002, loss G: 9.0998\n",
      "Epoch [0/10] Batch 1250/1583                       Loss D: 0.0002, loss G: 9.1017\n",
      "Epoch [0/10] Batch 1251/1583                       Loss D: 0.0002, loss G: 9.1014\n",
      "Epoch [0/10] Batch 1252/1583                       Loss D: 0.0002, loss G: 9.1013\n",
      "Epoch [0/10] Batch 1253/1583                       Loss D: 0.0002, loss G: 9.1030\n",
      "Epoch [0/10] Batch 1254/1583                       Loss D: 0.0002, loss G: 9.1057\n",
      "Epoch [0/10] Batch 1255/1583                       Loss D: 0.0002, loss G: 9.1101\n",
      "Epoch [0/10] Batch 1256/1583                       Loss D: 0.0002, loss G: 9.1160\n",
      "Epoch [0/10] Batch 1257/1583                       Loss D: 0.0002, loss G: 9.1184\n",
      "Epoch [0/10] Batch 1258/1583                       Loss D: 0.0002, loss G: 9.1216\n",
      "Epoch [0/10] Batch 1259/1583                       Loss D: 0.0002, loss G: 9.1243\n",
      "Epoch [0/10] Batch 1260/1583                       Loss D: 0.0002, loss G: 9.1276\n",
      "Epoch [0/10] Batch 1261/1583                       Loss D: 0.0002, loss G: 9.1321\n",
      "Epoch [0/10] Batch 1262/1583                       Loss D: 0.0002, loss G: 9.1372\n",
      "Epoch [0/10] Batch 1263/1583                       Loss D: 0.0002, loss G: 9.1394\n",
      "Epoch [0/10] Batch 1264/1583                       Loss D: 0.0002, loss G: 9.1405\n",
      "Epoch [0/10] Batch 1265/1583                       Loss D: 0.0002, loss G: 9.1436\n",
      "Epoch [0/10] Batch 1266/1583                       Loss D: 0.0002, loss G: 9.1471\n",
      "Epoch [0/10] Batch 1267/1583                       Loss D: 0.0002, loss G: 9.1513\n",
      "Epoch [0/10] Batch 1268/1583                       Loss D: 0.0002, loss G: 9.1561\n",
      "Epoch [0/10] Batch 1269/1583                       Loss D: 0.0002, loss G: 9.1590\n",
      "Epoch [0/10] Batch 1270/1583                       Loss D: 0.0002, loss G: 9.1633\n",
      "Epoch [0/10] Batch 1271/1583                       Loss D: 0.0002, loss G: 9.1675\n",
      "Epoch [0/10] Batch 1272/1583                       Loss D: 0.0002, loss G: 9.1735\n",
      "Epoch [0/10] Batch 1273/1583                       Loss D: 0.0002, loss G: 9.1757\n",
      "Epoch [0/10] Batch 1274/1583                       Loss D: 0.0002, loss G: 9.1790\n",
      "Epoch [0/10] Batch 1275/1583                       Loss D: 0.0002, loss G: 9.1796\n",
      "Epoch [0/10] Batch 1276/1583                       Loss D: 0.0002, loss G: 9.1813\n",
      "Epoch [0/10] Batch 1277/1583                       Loss D: 0.0002, loss G: 9.1831\n",
      "Epoch [0/10] Batch 1278/1583                       Loss D: 0.0002, loss G: 9.1857\n",
      "Epoch [0/10] Batch 1279/1583                       Loss D: 0.0002, loss G: 9.1868\n",
      "Epoch [0/10] Batch 1280/1583                       Loss D: 0.0002, loss G: 9.1905\n",
      "Epoch [0/10] Batch 1281/1583                       Loss D: 0.0002, loss G: 9.1951\n",
      "Epoch [0/10] Batch 1282/1583                       Loss D: 0.0002, loss G: 9.1973\n",
      "Epoch [0/10] Batch 1283/1583                       Loss D: 0.0002, loss G: 9.1980\n",
      "Epoch [0/10] Batch 1284/1583                       Loss D: 0.0002, loss G: 9.1998\n",
      "Epoch [0/10] Batch 1285/1583                       Loss D: 0.0002, loss G: 9.2026\n",
      "Epoch [0/10] Batch 1286/1583                       Loss D: 0.0002, loss G: 9.2066\n",
      "Epoch [0/10] Batch 1287/1583                       Loss D: 0.0002, loss G: 9.2087\n",
      "Epoch [0/10] Batch 1288/1583                       Loss D: 0.0002, loss G: 9.2115\n",
      "Epoch [0/10] Batch 1289/1583                       Loss D: 0.0002, loss G: 9.2134\n",
      "Epoch [0/10] Batch 1290/1583                       Loss D: 0.0002, loss G: 9.2159\n",
      "Epoch [0/10] Batch 1291/1583                       Loss D: 0.0002, loss G: 9.2188\n",
      "Epoch [0/10] Batch 1292/1583                       Loss D: 0.0002, loss G: 9.2225\n",
      "Epoch [0/10] Batch 1293/1583                       Loss D: 0.0002, loss G: 9.2254\n",
      "Epoch [0/10] Batch 1294/1583                       Loss D: 0.0002, loss G: 9.2280\n",
      "Epoch [0/10] Batch 1295/1583                       Loss D: 0.0002, loss G: 9.2301\n",
      "Epoch [0/10] Batch 1296/1583                       Loss D: 0.0002, loss G: 9.2319\n",
      "Epoch [0/10] Batch 1297/1583                       Loss D: 0.0002, loss G: 9.2347\n",
      "Epoch [0/10] Batch 1298/1583                       Loss D: 0.0002, loss G: 9.2368\n",
      "Epoch [0/10] Batch 1299/1583                       Loss D: 0.0002, loss G: 9.2395\n",
      "Epoch [0/10] Batch 1300/1583                       Loss D: 0.0002, loss G: 9.2424\n",
      "Epoch [0/10] Batch 1301/1583                       Loss D: 0.0002, loss G: 9.2451\n",
      "Epoch [0/10] Batch 1302/1583                       Loss D: 0.0002, loss G: 9.2461\n",
      "Epoch [0/10] Batch 1303/1583                       Loss D: 0.0002, loss G: 9.2480\n",
      "Epoch [0/10] Batch 1304/1583                       Loss D: 0.0002, loss G: 9.2493\n",
      "Epoch [0/10] Batch 1305/1583                       Loss D: 0.0002, loss G: 9.2506\n",
      "Epoch [0/10] Batch 1306/1583                       Loss D: 0.0002, loss G: 9.2531\n",
      "Epoch [0/10] Batch 1307/1583                       Loss D: 0.0002, loss G: 9.2558\n",
      "Epoch [0/10] Batch 1308/1583                       Loss D: 0.0002, loss G: 9.2577\n",
      "Epoch [0/10] Batch 1309/1583                       Loss D: 0.0002, loss G: 9.2599\n",
      "Epoch [0/10] Batch 1310/1583                       Loss D: 0.0002, loss G: 9.2638\n",
      "Epoch [0/10] Batch 1311/1583                       Loss D: 0.0002, loss G: 9.2660\n",
      "Epoch [0/10] Batch 1312/1583                       Loss D: 0.0002, loss G: 9.2675\n",
      "Epoch [0/10] Batch 1313/1583                       Loss D: 0.0002, loss G: 9.2703\n",
      "Epoch [0/10] Batch 1314/1583                       Loss D: 0.0002, loss G: 9.2712\n",
      "Epoch [0/10] Batch 1315/1583                       Loss D: 0.0002, loss G: 9.2738\n",
      "Epoch [0/10] Batch 1316/1583                       Loss D: 0.0002, loss G: 9.2745\n",
      "Epoch [0/10] Batch 1317/1583                       Loss D: 0.0002, loss G: 9.2767\n",
      "Epoch [0/10] Batch 1318/1583                       Loss D: 0.0002, loss G: 9.2781\n",
      "Epoch [0/10] Batch 1319/1583                       Loss D: 0.0002, loss G: 9.2812\n",
      "Epoch [0/10] Batch 1320/1583                       Loss D: 0.0002, loss G: 9.2850\n",
      "Epoch [0/10] Batch 1321/1583                       Loss D: 0.0002, loss G: 9.2875\n",
      "Epoch [0/10] Batch 1322/1583                       Loss D: 0.0002, loss G: 9.2883\n",
      "Epoch [0/10] Batch 1323/1583                       Loss D: 0.0002, loss G: 9.2902\n",
      "Epoch [0/10] Batch 1324/1583                       Loss D: 0.0002, loss G: 9.2926\n",
      "Epoch [0/10] Batch 1325/1583                       Loss D: 0.0002, loss G: 9.2943\n",
      "Epoch [0/10] Batch 1326/1583                       Loss D: 0.0002, loss G: 9.2960\n",
      "Epoch [0/10] Batch 1327/1583                       Loss D: 0.0002, loss G: 9.2977\n",
      "Epoch [0/10] Batch 1328/1583                       Loss D: 0.0002, loss G: 9.2990\n",
      "Epoch [0/10] Batch 1329/1583                       Loss D: 0.0002, loss G: 9.3009\n",
      "Epoch [0/10] Batch 1330/1583                       Loss D: 0.0002, loss G: 9.3033\n",
      "Epoch [0/10] Batch 1331/1583                       Loss D: 0.0002, loss G: 9.3051\n",
      "Epoch [0/10] Batch 1332/1583                       Loss D: 0.0002, loss G: 9.3087\n",
      "Epoch [0/10] Batch 1333/1583                       Loss D: 0.0002, loss G: 9.3114\n",
      "Epoch [0/10] Batch 1334/1583                       Loss D: 0.0002, loss G: 9.3132\n",
      "Epoch [0/10] Batch 1335/1583                       Loss D: 0.0002, loss G: 9.3123\n",
      "Epoch [0/10] Batch 1336/1583                       Loss D: 0.0002, loss G: 9.3130\n",
      "Epoch [0/10] Batch 1337/1583                       Loss D: 0.0002, loss G: 9.3142\n",
      "Epoch [0/10] Batch 1338/1583                       Loss D: 0.0002, loss G: 9.3164\n",
      "Epoch [0/10] Batch 1339/1583                       Loss D: 0.0002, loss G: 9.3191\n",
      "Epoch [0/10] Batch 1340/1583                       Loss D: 0.0002, loss G: 9.3234\n",
      "Epoch [0/10] Batch 1341/1583                       Loss D: 0.0002, loss G: 9.3247\n",
      "Epoch [0/10] Batch 1342/1583                       Loss D: 0.0002, loss G: 9.3253\n",
      "Epoch [0/10] Batch 1343/1583                       Loss D: 0.0002, loss G: 9.3269\n",
      "Epoch [0/10] Batch 1344/1583                       Loss D: 0.0002, loss G: 9.3294\n",
      "Epoch [0/10] Batch 1345/1583                       Loss D: 0.0002, loss G: 9.3314\n",
      "Epoch [0/10] Batch 1346/1583                       Loss D: 0.0002, loss G: 9.3342\n",
      "Epoch [0/10] Batch 1347/1583                       Loss D: 0.0002, loss G: 9.3363\n",
      "Epoch [0/10] Batch 1348/1583                       Loss D: 0.0002, loss G: 9.3378\n",
      "Epoch [0/10] Batch 1349/1583                       Loss D: 0.0002, loss G: 9.3382\n",
      "Epoch [0/10] Batch 1350/1583                       Loss D: 0.0002, loss G: 9.3386\n",
      "Epoch [0/10] Batch 1351/1583                       Loss D: 0.0002, loss G: 9.3392\n",
      "Epoch [0/10] Batch 1352/1583                       Loss D: 0.0002, loss G: 9.3412\n",
      "Epoch [0/10] Batch 1353/1583                       Loss D: 0.0002, loss G: 9.3429\n",
      "Epoch [0/10] Batch 1354/1583                       Loss D: 0.0002, loss G: 9.3448\n",
      "Epoch [0/10] Batch 1355/1583                       Loss D: 0.0002, loss G: 9.3475\n",
      "Epoch [0/10] Batch 1356/1583                       Loss D: 0.0002, loss G: 9.3506\n",
      "Epoch [0/10] Batch 1357/1583                       Loss D: 0.0002, loss G: 9.3546\n",
      "Epoch [0/10] Batch 1358/1583                       Loss D: 0.0002, loss G: 9.3542\n",
      "Epoch [0/10] Batch 1359/1583                       Loss D: 0.0002, loss G: 9.3540\n",
      "Epoch [0/10] Batch 1360/1583                       Loss D: 0.0002, loss G: 9.3554\n",
      "Epoch [0/10] Batch 1361/1583                       Loss D: 0.0002, loss G: 9.3571\n",
      "Epoch [0/10] Batch 1362/1583                       Loss D: 0.0002, loss G: 9.3587\n",
      "Epoch [0/10] Batch 1363/1583                       Loss D: 0.0002, loss G: 9.3611\n",
      "Epoch [0/10] Batch 1364/1583                       Loss D: 0.0002, loss G: 9.3630\n",
      "Epoch [0/10] Batch 1365/1583                       Loss D: 0.0002, loss G: 9.3651\n",
      "Epoch [0/10] Batch 1366/1583                       Loss D: 0.0002, loss G: 9.3653\n",
      "Epoch [0/10] Batch 1367/1583                       Loss D: 0.0002, loss G: 9.3645\n",
      "Epoch [0/10] Batch 1368/1583                       Loss D: 0.0002, loss G: 9.3660\n",
      "Epoch [0/10] Batch 1369/1583                       Loss D: 0.0002, loss G: 9.3693\n",
      "Epoch [0/10] Batch 1370/1583                       Loss D: 0.0002, loss G: 9.3721\n",
      "Epoch [0/10] Batch 1371/1583                       Loss D: 0.0002, loss G: 9.3747\n",
      "Epoch [0/10] Batch 1372/1583                       Loss D: 0.0002, loss G: 9.3754\n",
      "Epoch [0/10] Batch 1373/1583                       Loss D: 0.0002, loss G: 9.3776\n",
      "Epoch [0/10] Batch 1374/1583                       Loss D: 0.0002, loss G: 9.3772\n",
      "Epoch [0/10] Batch 1375/1583                       Loss D: 0.0002, loss G: 9.3789\n",
      "Epoch [0/10] Batch 1376/1583                       Loss D: 0.0002, loss G: 9.3802\n",
      "Epoch [0/10] Batch 1377/1583                       Loss D: 0.0002, loss G: 9.3840\n",
      "Epoch [0/10] Batch 1378/1583                       Loss D: 0.0001, loss G: 9.3874\n",
      "Epoch [0/10] Batch 1379/1583                       Loss D: 0.0002, loss G: 9.3880\n",
      "Epoch [0/10] Batch 1380/1583                       Loss D: 0.0002, loss G: 9.3881\n",
      "Epoch [0/10] Batch 1381/1583                       Loss D: 0.0002, loss G: 9.3900\n",
      "Epoch [0/10] Batch 1382/1583                       Loss D: 0.0002, loss G: 9.3915\n",
      "Epoch [0/10] Batch 1383/1583                       Loss D: 0.0002, loss G: 9.3944\n",
      "Epoch [0/10] Batch 1384/1583                       Loss D: 0.0002, loss G: 9.3942\n",
      "Epoch [0/10] Batch 1385/1583                       Loss D: 0.0002, loss G: 9.3947\n",
      "Epoch [0/10] Batch 1386/1583                       Loss D: 0.0002, loss G: 9.3946\n",
      "Epoch [0/10] Batch 1387/1583                       Loss D: 0.0001, loss G: 9.3967\n",
      "Epoch [0/10] Batch 1388/1583                       Loss D: 0.0002, loss G: 9.3977\n",
      "Epoch [0/10] Batch 1389/1583                       Loss D: 0.0002, loss G: 9.3996\n",
      "Epoch [0/10] Batch 1390/1583                       Loss D: 0.0002, loss G: 9.4012\n",
      "Epoch [0/10] Batch 1391/1583                       Loss D: 0.0002, loss G: 9.4032\n",
      "Epoch [0/10] Batch 1392/1583                       Loss D: 0.0002, loss G: 9.4051\n",
      "Epoch [0/10] Batch 1393/1583                       Loss D: 0.0002, loss G: 9.4071\n",
      "Epoch [0/10] Batch 1394/1583                       Loss D: 0.0002, loss G: 9.4088\n",
      "Epoch [0/10] Batch 1395/1583                       Loss D: 0.0001, loss G: 9.4112\n",
      "Epoch [0/10] Batch 1396/1583                       Loss D: 0.0002, loss G: 9.4131\n",
      "Epoch [0/10] Batch 1397/1583                       Loss D: 0.0002, loss G: 9.4140\n",
      "Epoch [0/10] Batch 1398/1583                       Loss D: 0.0002, loss G: 9.4157\n",
      "Epoch [0/10] Batch 1399/1583                       Loss D: 0.0002, loss G: 9.4181\n",
      "Epoch [0/10] Batch 1400/1583                       Loss D: 0.0002, loss G: 9.4200\n",
      "Epoch [0/10] Batch 1401/1583                       Loss D: 0.0002, loss G: 9.4208\n",
      "Epoch [0/10] Batch 1402/1583                       Loss D: 0.0002, loss G: 9.4219\n",
      "Epoch [0/10] Batch 1403/1583                       Loss D: 0.0002, loss G: 9.4238\n",
      "Epoch [0/10] Batch 1404/1583                       Loss D: 0.0002, loss G: 9.4258\n",
      "Epoch [0/10] Batch 1405/1583                       Loss D: 0.0001, loss G: 9.4281\n",
      "Epoch [0/10] Batch 1406/1583                       Loss D: 0.0001, loss G: 9.4286\n",
      "Epoch [0/10] Batch 1407/1583                       Loss D: 0.0002, loss G: 9.4302\n",
      "Epoch [0/10] Batch 1408/1583                       Loss D: 0.0002, loss G: 9.4317\n",
      "Epoch [0/10] Batch 1409/1583                       Loss D: 0.0002, loss G: 9.4331\n",
      "Epoch [0/10] Batch 1410/1583                       Loss D: 0.0001, loss G: 9.4357\n",
      "Epoch [0/10] Batch 1411/1583                       Loss D: 0.0001, loss G: 9.4368\n",
      "Epoch [0/10] Batch 1412/1583                       Loss D: 0.0001, loss G: 9.4369\n",
      "Epoch [0/10] Batch 1413/1583                       Loss D: 0.0002, loss G: 9.4374\n",
      "Epoch [0/10] Batch 1414/1583                       Loss D: 0.0002, loss G: 9.4391\n",
      "Epoch [0/10] Batch 1415/1583                       Loss D: 0.0002, loss G: 9.4419\n",
      "Epoch [0/10] Batch 1416/1583                       Loss D: 0.0001, loss G: 9.4443\n",
      "Epoch [0/10] Batch 1417/1583                       Loss D: 0.0002, loss G: 9.4467\n",
      "Epoch [0/10] Batch 1418/1583                       Loss D: 0.0001, loss G: 9.4470\n",
      "Epoch [0/10] Batch 1419/1583                       Loss D: 0.0001, loss G: 9.4494\n",
      "Epoch [0/10] Batch 1420/1583                       Loss D: 0.0002, loss G: 9.4491\n",
      "Epoch [0/10] Batch 1421/1583                       Loss D: 0.0002, loss G: 9.4493\n",
      "Epoch [0/10] Batch 1422/1583                       Loss D: 0.0001, loss G: 9.4497\n",
      "Epoch [0/10] Batch 1423/1583                       Loss D: 0.0002, loss G: 9.4512\n",
      "Epoch [0/10] Batch 1424/1583                       Loss D: 0.0001, loss G: 9.4542\n",
      "Epoch [0/10] Batch 1425/1583                       Loss D: 0.0002, loss G: 9.4576\n",
      "Epoch [0/10] Batch 1426/1583                       Loss D: 0.0001, loss G: 9.4582\n",
      "Epoch [0/10] Batch 1427/1583                       Loss D: 0.0001, loss G: 9.4596\n",
      "Epoch [0/10] Batch 1428/1583                       Loss D: 0.0001, loss G: 9.4621\n",
      "Epoch [0/10] Batch 1429/1583                       Loss D: 0.0001, loss G: 9.4637\n",
      "Epoch [0/10] Batch 1430/1583                       Loss D: 0.0001, loss G: 9.4643\n",
      "Epoch [0/10] Batch 1431/1583                       Loss D: 0.0001, loss G: 9.4647\n",
      "Epoch [0/10] Batch 1432/1583                       Loss D: 0.0002, loss G: 9.4653\n",
      "Epoch [0/10] Batch 1433/1583                       Loss D: 0.0001, loss G: 9.4650\n",
      "Epoch [0/10] Batch 1434/1583                       Loss D: 0.0002, loss G: 9.4661\n",
      "Epoch [0/10] Batch 1435/1583                       Loss D: 0.0001, loss G: 9.4657\n",
      "Epoch [0/10] Batch 1436/1583                       Loss D: 0.0001, loss G: 9.4679\n",
      "Epoch [0/10] Batch 1437/1583                       Loss D: 0.0001, loss G: 9.4698\n",
      "Epoch [0/10] Batch 1438/1583                       Loss D: 0.0001, loss G: 9.4724\n",
      "Epoch [0/10] Batch 1439/1583                       Loss D: 0.0002, loss G: 9.4734\n",
      "Epoch [0/10] Batch 1440/1583                       Loss D: 0.0001, loss G: 9.4756\n",
      "Epoch [0/10] Batch 1441/1583                       Loss D: 0.0001, loss G: 9.4763\n",
      "Epoch [0/10] Batch 1442/1583                       Loss D: 0.0001, loss G: 9.4790\n",
      "Epoch [0/10] Batch 1443/1583                       Loss D: 0.0001, loss G: 9.4808\n",
      "Epoch [0/10] Batch 1444/1583                       Loss D: 0.0001, loss G: 9.4823\n",
      "Epoch [0/10] Batch 1445/1583                       Loss D: 0.0001, loss G: 9.4826\n",
      "Epoch [0/10] Batch 1446/1583                       Loss D: 0.0001, loss G: 9.4839\n",
      "Epoch [0/10] Batch 1447/1583                       Loss D: 0.0001, loss G: 9.4852\n",
      "Epoch [0/10] Batch 1448/1583                       Loss D: 0.0001, loss G: 9.4838\n",
      "Epoch [0/10] Batch 1449/1583                       Loss D: 0.0001, loss G: 9.4837\n",
      "Epoch [0/10] Batch 1450/1583                       Loss D: 0.0001, loss G: 9.4842\n",
      "Epoch [0/10] Batch 1451/1583                       Loss D: 0.0001, loss G: 9.4853\n",
      "Epoch [0/10] Batch 1452/1583                       Loss D: 0.0002, loss G: 9.4880\n",
      "Epoch [0/10] Batch 1453/1583                       Loss D: 0.0001, loss G: 9.4888\n",
      "Epoch [0/10] Batch 1454/1583                       Loss D: 0.0001, loss G: 9.4892\n",
      "Epoch [0/10] Batch 1455/1583                       Loss D: 0.0001, loss G: 9.4887\n",
      "Epoch [0/10] Batch 1456/1583                       Loss D: 0.0001, loss G: 9.4889\n",
      "Epoch [0/10] Batch 1457/1583                       Loss D: 0.0001, loss G: 9.4899\n",
      "Epoch [0/10] Batch 1458/1583                       Loss D: 0.0001, loss G: 9.4906\n",
      "Epoch [0/10] Batch 1459/1583                       Loss D: 0.0001, loss G: 9.4911\n",
      "Epoch [0/10] Batch 1460/1583                       Loss D: 0.0001, loss G: 9.4930\n",
      "Epoch [0/10] Batch 1461/1583                       Loss D: 0.0001, loss G: 9.4952\n",
      "Epoch [0/10] Batch 1462/1583                       Loss D: 0.0001, loss G: 9.4962\n",
      "Epoch [0/10] Batch 1463/1583                       Loss D: 0.0001, loss G: 9.4953\n",
      "Epoch [0/10] Batch 1464/1583                       Loss D: 0.0001, loss G: 9.4958\n",
      "Epoch [0/10] Batch 1465/1583                       Loss D: 0.0002, loss G: 9.4959\n",
      "Epoch [0/10] Batch 1466/1583                       Loss D: 0.0001, loss G: 9.4953\n",
      "Epoch [0/10] Batch 1467/1583                       Loss D: 0.0001, loss G: 9.4978\n",
      "Epoch [0/10] Batch 1468/1583                       Loss D: 0.0001, loss G: 9.4993\n",
      "Epoch [0/10] Batch 1469/1583                       Loss D: 0.0001, loss G: 9.5004\n",
      "Epoch [0/10] Batch 1470/1583                       Loss D: 0.0001, loss G: 9.5003\n",
      "Epoch [0/10] Batch 1471/1583                       Loss D: 0.0001, loss G: 9.5009\n",
      "Epoch [0/10] Batch 1472/1583                       Loss D: 0.0001, loss G: 9.5040\n",
      "Epoch [0/10] Batch 1473/1583                       Loss D: 0.0001, loss G: 9.5072\n",
      "Epoch [0/10] Batch 1474/1583                       Loss D: 0.0001, loss G: 9.5110\n",
      "Epoch [0/10] Batch 1475/1583                       Loss D: 0.0001, loss G: 9.5130\n",
      "Epoch [0/10] Batch 1476/1583                       Loss D: 0.0001, loss G: 9.5121\n",
      "Epoch [0/10] Batch 1477/1583                       Loss D: 0.0001, loss G: 9.5110\n",
      "Epoch [0/10] Batch 1478/1583                       Loss D: 0.0001, loss G: 9.5095\n",
      "Epoch [0/10] Batch 1479/1583                       Loss D: 0.0001, loss G: 9.5106\n",
      "Epoch [0/10] Batch 1480/1583                       Loss D: 0.0001, loss G: 9.5109\n",
      "Epoch [0/10] Batch 1481/1583                       Loss D: 0.0001, loss G: 9.5126\n",
      "Epoch [0/10] Batch 1482/1583                       Loss D: 0.0001, loss G: 9.5142\n",
      "Epoch [0/10] Batch 1483/1583                       Loss D: 0.0001, loss G: 9.5163\n",
      "Epoch [0/10] Batch 1484/1583                       Loss D: 0.0001, loss G: 9.5173\n",
      "Epoch [0/10] Batch 1485/1583                       Loss D: 0.0001, loss G: 9.5166\n",
      "Epoch [0/10] Batch 1486/1583                       Loss D: 0.0001, loss G: 9.5176\n",
      "Epoch [0/10] Batch 1487/1583                       Loss D: 0.0001, loss G: 9.5181\n",
      "Epoch [0/10] Batch 1488/1583                       Loss D: 0.0001, loss G: 9.5185\n",
      "Epoch [0/10] Batch 1489/1583                       Loss D: 0.0001, loss G: 9.5200\n",
      "Epoch [0/10] Batch 1490/1583                       Loss D: 0.0001, loss G: 9.5214\n",
      "Epoch [0/10] Batch 1491/1583                       Loss D: 0.0001, loss G: 9.5230\n",
      "Epoch [0/10] Batch 1492/1583                       Loss D: 0.0001, loss G: 9.5246\n",
      "Epoch [0/10] Batch 1493/1583                       Loss D: 0.0001, loss G: 9.5279\n",
      "Epoch [0/10] Batch 1494/1583                       Loss D: 0.0001, loss G: 9.5276\n",
      "Epoch [0/10] Batch 1495/1583                       Loss D: 0.0001, loss G: 9.5283\n",
      "Epoch [0/10] Batch 1496/1583                       Loss D: 0.0001, loss G: 9.5282\n",
      "Epoch [0/10] Batch 1497/1583                       Loss D: 0.0001, loss G: 9.5294\n",
      "Epoch [0/10] Batch 1498/1583                       Loss D: 0.0001, loss G: 9.5311\n",
      "Epoch [0/10] Batch 1499/1583                       Loss D: 0.0001, loss G: 9.5337\n",
      "Epoch [0/10] Batch 1500/1583                       Loss D: 0.0001, loss G: 9.5351\n",
      "Epoch [0/10] Batch 1501/1583                       Loss D: 0.0001, loss G: 9.5375\n",
      "Epoch [0/10] Batch 1502/1583                       Loss D: 0.0001, loss G: 9.5378\n",
      "Epoch [0/10] Batch 1503/1583                       Loss D: 0.0001, loss G: 9.5376\n",
      "Epoch [0/10] Batch 1504/1583                       Loss D: 0.0001, loss G: 9.5379\n",
      "Epoch [0/10] Batch 1505/1583                       Loss D: 0.0001, loss G: 9.5382\n",
      "Epoch [0/10] Batch 1506/1583                       Loss D: 0.0001, loss G: 9.5387\n",
      "Epoch [0/10] Batch 1507/1583                       Loss D: 0.0001, loss G: 9.5394\n",
      "Epoch [0/10] Batch 1508/1583                       Loss D: 0.0001, loss G: 9.5405\n",
      "Epoch [0/10] Batch 1509/1583                       Loss D: 0.0001, loss G: 9.5431\n",
      "Epoch [0/10] Batch 1510/1583                       Loss D: 0.0001, loss G: 9.5443\n",
      "Epoch [0/10] Batch 1511/1583                       Loss D: 0.0001, loss G: 9.5505\n",
      "Epoch [0/10] Batch 1512/1583                       Loss D: 0.0001, loss G: 9.5606\n",
      "Epoch [0/10] Batch 1513/1583                       Loss D: 0.0001, loss G: 9.5575\n",
      "Epoch [0/10] Batch 1514/1583                       Loss D: 0.0001, loss G: 9.5528\n",
      "Epoch [0/10] Batch 1515/1583                       Loss D: 0.0001, loss G: 9.5481\n",
      "Epoch [0/10] Batch 1516/1583                       Loss D: 0.0001, loss G: 9.5467\n",
      "Epoch [0/10] Batch 1517/1583                       Loss D: 0.0001, loss G: 9.5471\n",
      "Epoch [0/10] Batch 1518/1583                       Loss D: 0.0001, loss G: 9.5474\n",
      "Epoch [0/10] Batch 1519/1583                       Loss D: 0.0001, loss G: 9.5495\n",
      "Epoch [0/10] Batch 1520/1583                       Loss D: 0.0001, loss G: 9.5529\n",
      "Epoch [0/10] Batch 1521/1583                       Loss D: 0.0001, loss G: 9.5516\n",
      "Epoch [0/10] Batch 1522/1583                       Loss D: 0.0001, loss G: 9.5527\n",
      "Epoch [0/10] Batch 1523/1583                       Loss D: 0.0001, loss G: 9.5543\n",
      "Epoch [0/10] Batch 1524/1583                       Loss D: 0.0001, loss G: 9.5583\n",
      "Epoch [0/10] Batch 1525/1583                       Loss D: 0.0001, loss G: 9.5610\n",
      "Epoch [0/10] Batch 1526/1583                       Loss D: 0.0001, loss G: 9.5614\n",
      "Epoch [0/10] Batch 1527/1583                       Loss D: 0.0001, loss G: 9.5596\n",
      "Epoch [0/10] Batch 1528/1583                       Loss D: 0.0001, loss G: 9.5558\n",
      "Epoch [0/10] Batch 1529/1583                       Loss D: 0.0001, loss G: 9.5544\n",
      "Epoch [0/10] Batch 1530/1583                       Loss D: 0.0001, loss G: 9.5535\n",
      "Epoch [0/10] Batch 1531/1583                       Loss D: 0.0001, loss G: 9.5545\n",
      "Epoch [0/10] Batch 1532/1583                       Loss D: 0.0001, loss G: 9.5568\n",
      "Epoch [0/10] Batch 1533/1583                       Loss D: 0.0001, loss G: 9.5596\n",
      "Epoch [0/10] Batch 1534/1583                       Loss D: 0.0001, loss G: 9.5617\n",
      "Epoch [0/10] Batch 1535/1583                       Loss D: 0.0001, loss G: 9.5622\n",
      "Epoch [0/10] Batch 1536/1583                       Loss D: 0.0001, loss G: 9.5602\n",
      "Epoch [0/10] Batch 1537/1583                       Loss D: 0.0001, loss G: 9.5592\n",
      "Epoch [0/10] Batch 1538/1583                       Loss D: 0.0001, loss G: 9.5596\n",
      "Epoch [0/10] Batch 1539/1583                       Loss D: 0.0001, loss G: 9.5613\n",
      "Epoch [0/10] Batch 1540/1583                       Loss D: 0.0001, loss G: 9.5644\n",
      "Epoch [0/10] Batch 1541/1583                       Loss D: 0.0001, loss G: 9.5658\n",
      "Epoch [0/10] Batch 1542/1583                       Loss D: 0.0001, loss G: 9.5664\n",
      "Epoch [0/10] Batch 1543/1583                       Loss D: 0.0001, loss G: 9.5659\n",
      "Epoch [0/10] Batch 1544/1583                       Loss D: 0.0001, loss G: 9.5638\n",
      "Epoch [0/10] Batch 1545/1583                       Loss D: 0.0001, loss G: 9.5634\n",
      "Epoch [0/10] Batch 1546/1583                       Loss D: 0.0001, loss G: 9.5643\n",
      "Epoch [0/10] Batch 1547/1583                       Loss D: 0.0001, loss G: 9.5669\n",
      "Epoch [0/10] Batch 1548/1583                       Loss D: 0.0001, loss G: 9.5698\n",
      "Epoch [0/10] Batch 1549/1583                       Loss D: 0.0001, loss G: 9.5714\n",
      "Epoch [0/10] Batch 1550/1583                       Loss D: 0.0001, loss G: 9.5711\n",
      "Epoch [0/10] Batch 1551/1583                       Loss D: 0.0001, loss G: 9.5695\n",
      "Epoch [0/10] Batch 1552/1583                       Loss D: 0.0001, loss G: 9.5684\n",
      "Epoch [0/10] Batch 1553/1583                       Loss D: 0.0001, loss G: 9.5678\n",
      "Epoch [0/10] Batch 1554/1583                       Loss D: 0.0001, loss G: 9.5708\n",
      "Epoch [0/10] Batch 1555/1583                       Loss D: 0.0001, loss G: 9.5734\n",
      "Epoch [0/10] Batch 1556/1583                       Loss D: 0.0001, loss G: 9.5737\n",
      "Epoch [0/10] Batch 1557/1583                       Loss D: 0.0001, loss G: 9.5732\n",
      "Epoch [0/10] Batch 1558/1583                       Loss D: 0.0001, loss G: 9.5726\n",
      "Epoch [0/10] Batch 1559/1583                       Loss D: 0.0001, loss G: 9.5754\n",
      "Epoch [0/10] Batch 1560/1583                       Loss D: 0.0001, loss G: 9.5790\n",
      "Epoch [0/10] Batch 1561/1583                       Loss D: 0.0001, loss G: 9.5818\n",
      "Epoch [0/10] Batch 1562/1583                       Loss D: 0.0001, loss G: 9.5831\n",
      "Epoch [0/10] Batch 1563/1583                       Loss D: 0.0001, loss G: 9.5821\n",
      "Epoch [0/10] Batch 1564/1583                       Loss D: 0.0001, loss G: 9.5789\n",
      "Epoch [0/10] Batch 1565/1583                       Loss D: 0.0001, loss G: 9.5768\n",
      "Epoch [0/10] Batch 1566/1583                       Loss D: 0.0001, loss G: 9.5783\n",
      "Epoch [0/10] Batch 1567/1583                       Loss D: 0.0001, loss G: 9.5817\n",
      "Epoch [0/10] Batch 1568/1583                       Loss D: 0.0001, loss G: 9.5858\n",
      "Epoch [0/10] Batch 1569/1583                       Loss D: 0.0001, loss G: 9.5911\n",
      "Epoch [0/10] Batch 1570/1583                       Loss D: 0.0001, loss G: 9.5937\n",
      "Epoch [0/10] Batch 1571/1583                       Loss D: 0.0001, loss G: 9.5886\n",
      "Epoch [0/10] Batch 1572/1583                       Loss D: 0.0001, loss G: 9.5846\n",
      "Epoch [0/10] Batch 1573/1583                       Loss D: 0.0001, loss G: 9.5828\n",
      "Epoch [0/10] Batch 1574/1583                       Loss D: 0.0001, loss G: 9.5827\n",
      "Epoch [0/10] Batch 1575/1583                       Loss D: 0.0001, loss G: 9.5829\n",
      "Epoch [0/10] Batch 1576/1583                       Loss D: 0.0001, loss G: 9.5857\n",
      "Epoch [0/10] Batch 1577/1583                       Loss D: 0.0001, loss G: 9.5878\n",
      "Epoch [0/10] Batch 1578/1583                       Loss D: 0.0001, loss G: 9.5882\n",
      "Epoch [0/10] Batch 1579/1583                       Loss D: 0.0001, loss G: 9.5892\n",
      "Epoch [0/10] Batch 1580/1583                       Loss D: 0.0001, loss G: 9.5888\n",
      "Epoch [0/10] Batch 1581/1583                       Loss D: 0.0001, loss G: 9.5892\n",
      "Epoch [0/10] Batch 1582/1583                       Loss D: 0.0001, loss G: 9.5893\n",
      "Epoch [1/10] Batch 0/1583                       Loss D: 0.0001, loss G: 9.5910\n",
      "Epoch [1/10] Batch 1/1583                       Loss D: 0.0001, loss G: 9.5925\n",
      "Epoch [1/10] Batch 2/1583                       Loss D: 0.0001, loss G: 9.5951\n",
      "Epoch [1/10] Batch 3/1583                       Loss D: 0.0001, loss G: 9.5970\n",
      "Epoch [1/10] Batch 4/1583                       Loss D: 0.0001, loss G: 9.5997\n",
      "Epoch [1/10] Batch 5/1583                       Loss D: 0.0001, loss G: 9.6014\n",
      "Epoch [1/10] Batch 6/1583                       Loss D: 0.0001, loss G: 9.5992\n",
      "Epoch [1/10] Batch 7/1583                       Loss D: 0.0001, loss G: 9.5985\n",
      "Epoch [1/10] Batch 8/1583                       Loss D: 0.0001, loss G: 9.6003\n",
      "Epoch [1/10] Batch 9/1583                       Loss D: 0.0001, loss G: 9.6044\n",
      "Epoch [1/10] Batch 10/1583                       Loss D: 0.0001, loss G: 9.6077\n",
      "Epoch [1/10] Batch 11/1583                       Loss D: 0.0001, loss G: 9.6060\n",
      "Epoch [1/10] Batch 12/1583                       Loss D: 0.0001, loss G: 9.6049\n",
      "Epoch [1/10] Batch 13/1583                       Loss D: 0.0001, loss G: 9.6040\n",
      "Epoch [1/10] Batch 14/1583                       Loss D: 0.0001, loss G: 9.6057\n",
      "Epoch [1/10] Batch 15/1583                       Loss D: 0.0001, loss G: 9.6095\n",
      "Epoch [1/10] Batch 16/1583                       Loss D: 0.0001, loss G: 9.6155\n",
      "Epoch [1/10] Batch 17/1583                       Loss D: 0.0001, loss G: 9.6152\n",
      "Epoch [1/10] Batch 18/1583                       Loss D: 0.0001, loss G: 9.6168\n",
      "Epoch [1/10] Batch 19/1583                       Loss D: 0.0001, loss G: 9.6154\n",
      "Epoch [1/10] Batch 20/1583                       Loss D: 0.0001, loss G: 9.6160\n",
      "Epoch [1/10] Batch 21/1583                       Loss D: 0.0001, loss G: 9.6168\n",
      "Epoch [1/10] Batch 22/1583                       Loss D: 0.0001, loss G: 9.6184\n",
      "Epoch [1/10] Batch 23/1583                       Loss D: 0.0001, loss G: 9.6188\n",
      "Epoch [1/10] Batch 24/1583                       Loss D: 0.0001, loss G: 9.6202\n",
      "Epoch [1/10] Batch 25/1583                       Loss D: 0.0001, loss G: 9.6204\n",
      "Epoch [1/10] Batch 26/1583                       Loss D: 0.0001, loss G: 9.6218\n",
      "Epoch [1/10] Batch 27/1583                       Loss D: 0.0001, loss G: 9.6240\n",
      "Epoch [1/10] Batch 28/1583                       Loss D: 0.0001, loss G: 9.6247\n",
      "Epoch [1/10] Batch 29/1583                       Loss D: 0.0001, loss G: 9.6247\n",
      "Epoch [1/10] Batch 30/1583                       Loss D: 0.0001, loss G: 9.6248\n",
      "Epoch [1/10] Batch 31/1583                       Loss D: 0.0001, loss G: 9.6269\n",
      "Epoch [1/10] Batch 32/1583                       Loss D: 0.0001, loss G: 9.6290\n",
      "Epoch [1/10] Batch 33/1583                       Loss D: 0.0001, loss G: 9.6302\n",
      "Epoch [1/10] Batch 34/1583                       Loss D: 0.0001, loss G: 9.6309\n",
      "Epoch [1/10] Batch 35/1583                       Loss D: 0.0001, loss G: 9.6320\n",
      "Epoch [1/10] Batch 36/1583                       Loss D: 0.0001, loss G: 9.6329\n",
      "Epoch [1/10] Batch 37/1583                       Loss D: 0.0001, loss G: 9.6365\n",
      "Epoch [1/10] Batch 38/1583                       Loss D: 0.0001, loss G: 9.6380\n",
      "Epoch [1/10] Batch 39/1583                       Loss D: 0.0001, loss G: 9.6408\n",
      "Epoch [1/10] Batch 40/1583                       Loss D: 0.0001, loss G: 9.6388\n",
      "Epoch [1/10] Batch 41/1583                       Loss D: 0.0001, loss G: 9.6374\n",
      "Epoch [1/10] Batch 42/1583                       Loss D: 0.0001, loss G: 9.6374\n",
      "Epoch [1/10] Batch 43/1583                       Loss D: 0.0001, loss G: 9.6366\n",
      "Epoch [1/10] Batch 44/1583                       Loss D: 0.0001, loss G: 9.6358\n",
      "Epoch [1/10] Batch 45/1583                       Loss D: 0.0001, loss G: 9.6374\n",
      "Epoch [1/10] Batch 46/1583                       Loss D: 0.0001, loss G: 9.6395\n",
      "Epoch [1/10] Batch 47/1583                       Loss D: 0.0001, loss G: 9.6418\n",
      "Epoch [1/10] Batch 48/1583                       Loss D: 0.0001, loss G: 9.6440\n",
      "Epoch [1/10] Batch 49/1583                       Loss D: 0.0001, loss G: 9.6435\n",
      "Epoch [1/10] Batch 50/1583                       Loss D: 0.0001, loss G: 9.6438\n",
      "Epoch [1/10] Batch 51/1583                       Loss D: 0.0001, loss G: 9.6440\n",
      "Epoch [1/10] Batch 52/1583                       Loss D: 0.0001, loss G: 9.6474\n",
      "Epoch [1/10] Batch 53/1583                       Loss D: 0.0001, loss G: 9.6476\n",
      "Epoch [1/10] Batch 54/1583                       Loss D: 0.0001, loss G: 9.6461\n",
      "Epoch [1/10] Batch 55/1583                       Loss D: 0.0001, loss G: 9.6438\n",
      "Epoch [1/10] Batch 56/1583                       Loss D: 0.0001, loss G: 9.6423\n",
      "Epoch [1/10] Batch 57/1583                       Loss D: 0.0001, loss G: 9.6431\n",
      "Epoch [1/10] Batch 58/1583                       Loss D: 0.0001, loss G: 9.6434\n",
      "Epoch [1/10] Batch 59/1583                       Loss D: 0.0001, loss G: 9.6437\n",
      "Epoch [1/10] Batch 60/1583                       Loss D: 0.0001, loss G: 9.6419\n",
      "Epoch [1/10] Batch 61/1583                       Loss D: 0.0001, loss G: 9.6401\n",
      "Epoch [1/10] Batch 62/1583                       Loss D: 0.0001, loss G: 9.6389\n",
      "Epoch [1/10] Batch 63/1583                       Loss D: 0.0001, loss G: 9.6387\n",
      "Epoch [1/10] Batch 64/1583                       Loss D: 0.0001, loss G: 9.6387\n",
      "Epoch [1/10] Batch 65/1583                       Loss D: 0.0001, loss G: 9.6377\n",
      "Epoch [1/10] Batch 66/1583                       Loss D: 0.0001, loss G: 9.6346\n",
      "Epoch [1/10] Batch 67/1583                       Loss D: 0.0001, loss G: 9.6327\n",
      "Epoch [1/10] Batch 68/1583                       Loss D: 0.0001, loss G: 9.6302\n",
      "Epoch [1/10] Batch 69/1583                       Loss D: 0.0001, loss G: 9.6307\n",
      "Epoch [1/10] Batch 70/1583                       Loss D: 0.0001, loss G: 9.6333\n",
      "Epoch [1/10] Batch 71/1583                       Loss D: 0.0001, loss G: 9.6346\n",
      "Epoch [1/10] Batch 72/1583                       Loss D: 0.0001, loss G: 9.6338\n",
      "Epoch [1/10] Batch 73/1583                       Loss D: 0.0001, loss G: 9.6312\n",
      "Epoch [1/10] Batch 74/1583                       Loss D: 0.0001, loss G: 9.6292\n",
      "Epoch [1/10] Batch 75/1583                       Loss D: 0.0001, loss G: 9.6294\n",
      "Epoch [1/10] Batch 76/1583                       Loss D: 0.0001, loss G: 9.6289\n",
      "Epoch [1/10] Batch 77/1583                       Loss D: 0.0001, loss G: 9.6278\n",
      "Epoch [1/10] Batch 78/1583                       Loss D: 0.0001, loss G: 9.6256\n",
      "Epoch [1/10] Batch 79/1583                       Loss D: 0.0001, loss G: 9.6243\n",
      "Epoch [1/10] Batch 80/1583                       Loss D: 0.0001, loss G: 9.6232\n",
      "Epoch [1/10] Batch 81/1583                       Loss D: 0.0001, loss G: 9.6235\n",
      "Epoch [1/10] Batch 82/1583                       Loss D: 0.0001, loss G: 9.6215\n",
      "Epoch [1/10] Batch 83/1583                       Loss D: 0.0001, loss G: 9.6167\n",
      "Epoch [1/10] Batch 84/1583                       Loss D: 0.0001, loss G: 9.6134\n",
      "Epoch [1/10] Batch 85/1583                       Loss D: 0.0001, loss G: 9.6111\n",
      "Epoch [1/10] Batch 86/1583                       Loss D: 0.0001, loss G: 9.6109\n",
      "Epoch [1/10] Batch 87/1583                       Loss D: 0.0001, loss G: 9.6119\n",
      "Epoch [1/10] Batch 88/1583                       Loss D: 0.0001, loss G: 9.6152\n",
      "Epoch [1/10] Batch 89/1583                       Loss D: 0.0001, loss G: 9.6147\n",
      "Epoch [1/10] Batch 90/1583                       Loss D: 0.0001, loss G: 9.6093\n",
      "Epoch [1/10] Batch 91/1583                       Loss D: 0.0001, loss G: 9.6064\n",
      "Epoch [1/10] Batch 92/1583                       Loss D: 0.0001, loss G: 9.6047\n",
      "Epoch [1/10] Batch 93/1583                       Loss D: 0.0001, loss G: 9.6034\n",
      "Epoch [1/10] Batch 94/1583                       Loss D: 0.0001, loss G: 9.6040\n",
      "Epoch [1/10] Batch 95/1583                       Loss D: 0.0001, loss G: 9.6023\n",
      "Epoch [1/10] Batch 96/1583                       Loss D: 0.0001, loss G: 9.6022\n",
      "Epoch [1/10] Batch 97/1583                       Loss D: 0.0001, loss G: 9.6011\n",
      "Epoch [1/10] Batch 98/1583                       Loss D: 0.0001, loss G: 9.5977\n",
      "Epoch [1/10] Batch 99/1583                       Loss D: 0.0001, loss G: 9.5966\n",
      "Epoch [1/10] Batch 100/1583                       Loss D: 0.0001, loss G: 9.5959\n",
      "Epoch [1/10] Batch 101/1583                       Loss D: 0.0001, loss G: 9.5964\n",
      "Epoch [1/10] Batch 102/1583                       Loss D: 0.0001, loss G: 9.5975\n",
      "Epoch [1/10] Batch 103/1583                       Loss D: 0.0001, loss G: 9.5985\n",
      "Epoch [1/10] Batch 104/1583                       Loss D: 0.0001, loss G: 9.5996\n",
      "Epoch [1/10] Batch 105/1583                       Loss D: 0.0001, loss G: 9.5962\n",
      "Epoch [1/10] Batch 106/1583                       Loss D: 0.0001, loss G: 9.5934\n",
      "Epoch [1/10] Batch 107/1583                       Loss D: 0.0001, loss G: 9.5907\n",
      "Epoch [1/10] Batch 108/1583                       Loss D: 0.0001, loss G: 9.5902\n",
      "Epoch [1/10] Batch 109/1583                       Loss D: 0.0001, loss G: 9.5907\n",
      "Epoch [1/10] Batch 110/1583                       Loss D: 0.0001, loss G: 9.5939\n",
      "Epoch [1/10] Batch 111/1583                       Loss D: 0.0001, loss G: 9.5925\n",
      "Epoch [1/10] Batch 112/1583                       Loss D: 0.0001, loss G: 9.5881\n",
      "Epoch [1/10] Batch 113/1583                       Loss D: 0.0001, loss G: 9.5834\n",
      "Epoch [1/10] Batch 114/1583                       Loss D: 0.0001, loss G: 9.5814\n",
      "Epoch [1/10] Batch 115/1583                       Loss D: 0.0001, loss G: 9.5812\n",
      "Epoch [1/10] Batch 116/1583                       Loss D: 0.0001, loss G: 9.5823\n",
      "Epoch [1/10] Batch 117/1583                       Loss D: 0.0001, loss G: 9.5839\n",
      "Epoch [1/10] Batch 118/1583                       Loss D: 0.0001, loss G: 9.5825\n",
      "Epoch [1/10] Batch 119/1583                       Loss D: 0.0001, loss G: 9.5787\n",
      "Epoch [1/10] Batch 120/1583                       Loss D: 0.0001, loss G: 9.5762\n",
      "Epoch [1/10] Batch 121/1583                       Loss D: 0.0001, loss G: 9.5749\n",
      "Epoch [1/10] Batch 122/1583                       Loss D: 0.0001, loss G: 9.5748\n",
      "Epoch [1/10] Batch 123/1583                       Loss D: 0.0001, loss G: 9.5764\n",
      "Epoch [1/10] Batch 124/1583                       Loss D: 0.0001, loss G: 9.5799\n",
      "Epoch [1/10] Batch 125/1583                       Loss D: 0.0001, loss G: 9.5805\n",
      "Epoch [1/10] Batch 126/1583                       Loss D: 0.0001, loss G: 9.5777\n",
      "Epoch [1/10] Batch 127/1583                       Loss D: 0.0001, loss G: 9.5748\n",
      "Epoch [1/10] Batch 128/1583                       Loss D: 0.0001, loss G: 9.5719\n",
      "Epoch [1/10] Batch 129/1583                       Loss D: 0.0001, loss G: 9.5699\n",
      "Epoch [1/10] Batch 130/1583                       Loss D: 0.0001, loss G: 9.5693\n",
      "Epoch [1/10] Batch 131/1583                       Loss D: 0.0001, loss G: 9.5724\n",
      "Epoch [1/10] Batch 132/1583                       Loss D: 0.0001, loss G: 9.5749\n",
      "Epoch [1/10] Batch 133/1583                       Loss D: 0.0001, loss G: 9.5770\n",
      "Epoch [1/10] Batch 134/1583                       Loss D: 0.0001, loss G: 9.5722\n",
      "Epoch [1/10] Batch 135/1583                       Loss D: 0.0001, loss G: 9.5675\n",
      "Epoch [1/10] Batch 136/1583                       Loss D: 0.0001, loss G: 9.5617\n",
      "Epoch [1/10] Batch 137/1583                       Loss D: 0.0001, loss G: 9.5602\n",
      "Epoch [1/10] Batch 138/1583                       Loss D: 0.0001, loss G: 9.5622\n",
      "Epoch [1/10] Batch 139/1583                       Loss D: 0.0001, loss G: 9.5647\n",
      "Epoch [1/10] Batch 140/1583                       Loss D: 0.0001, loss G: 9.5679\n",
      "Epoch [1/10] Batch 141/1583                       Loss D: 0.0001, loss G: 9.5689\n",
      "Epoch [1/10] Batch 142/1583                       Loss D: 0.0001, loss G: 9.5659\n",
      "Epoch [1/10] Batch 143/1583                       Loss D: 0.0001, loss G: 9.5596\n",
      "Epoch [1/10] Batch 144/1583                       Loss D: 0.0001, loss G: 9.5582\n",
      "Epoch [1/10] Batch 145/1583                       Loss D: 0.0001, loss G: 9.5592\n",
      "Epoch [1/10] Batch 146/1583                       Loss D: 0.0001, loss G: 9.5607\n",
      "Epoch [1/10] Batch 147/1583                       Loss D: 0.0001, loss G: 9.5613\n",
      "Epoch [1/10] Batch 148/1583                       Loss D: 0.0001, loss G: 9.5646\n",
      "Epoch [1/10] Batch 149/1583                       Loss D: 0.0001, loss G: 9.5639\n",
      "Epoch [1/10] Batch 150/1583                       Loss D: 0.0001, loss G: 9.5626\n",
      "Epoch [1/10] Batch 151/1583                       Loss D: 0.0001, loss G: 9.5602\n",
      "Epoch [1/10] Batch 152/1583                       Loss D: 0.0001, loss G: 9.5589\n",
      "Epoch [1/10] Batch 153/1583                       Loss D: 0.0001, loss G: 9.5584\n",
      "Epoch [1/10] Batch 154/1583                       Loss D: 0.0001, loss G: 9.5574\n",
      "Epoch [1/10] Batch 155/1583                       Loss D: 0.0001, loss G: 9.5589\n",
      "Epoch [1/10] Batch 156/1583                       Loss D: 0.0001, loss G: 9.5605\n",
      "Epoch [1/10] Batch 157/1583                       Loss D: 0.0001, loss G: 9.5608\n",
      "Epoch [1/10] Batch 158/1583                       Loss D: 0.0001, loss G: 9.5604\n",
      "Epoch [1/10] Batch 159/1583                       Loss D: 0.0001, loss G: 9.5579\n",
      "Epoch [1/10] Batch 160/1583                       Loss D: 0.0001, loss G: 9.5521\n",
      "Epoch [1/10] Batch 161/1583                       Loss D: 0.0001, loss G: 9.5508\n",
      "Epoch [1/10] Batch 162/1583                       Loss D: 0.0001, loss G: 9.5503\n",
      "Epoch [1/10] Batch 163/1583                       Loss D: 0.0001, loss G: 9.5504\n",
      "Epoch [1/10] Batch 164/1583                       Loss D: 0.0001, loss G: 9.5508\n",
      "Epoch [1/10] Batch 165/1583                       Loss D: 0.0001, loss G: 9.5542\n",
      "Epoch [1/10] Batch 166/1583                       Loss D: 0.0001, loss G: 9.5528\n",
      "Epoch [1/10] Batch 167/1583                       Loss D: 0.0001, loss G: 9.5498\n",
      "Epoch [1/10] Batch 168/1583                       Loss D: 0.0001, loss G: 9.5461\n",
      "Epoch [1/10] Batch 169/1583                       Loss D: 0.0001, loss G: 9.5442\n",
      "Epoch [1/10] Batch 170/1583                       Loss D: 0.0001, loss G: 9.5427\n",
      "Epoch [1/10] Batch 171/1583                       Loss D: 0.0001, loss G: 9.5435\n",
      "Epoch [1/10] Batch 172/1583                       Loss D: 0.0001, loss G: 9.5445\n",
      "Epoch [1/10] Batch 173/1583                       Loss D: 0.0001, loss G: 9.5506\n",
      "Epoch [1/10] Batch 174/1583                       Loss D: 0.0001, loss G: 9.5482\n",
      "Epoch [1/10] Batch 175/1583                       Loss D: 0.0001, loss G: 9.5458\n",
      "Epoch [1/10] Batch 176/1583                       Loss D: 0.0001, loss G: 9.5433\n",
      "Epoch [1/10] Batch 177/1583                       Loss D: 0.0001, loss G: 9.5392\n",
      "Epoch [1/10] Batch 178/1583                       Loss D: 0.0001, loss G: 9.5347\n",
      "Epoch [1/10] Batch 179/1583                       Loss D: 0.0001, loss G: 9.5339\n",
      "Epoch [1/10] Batch 180/1583                       Loss D: 0.0001, loss G: 9.5313\n",
      "Epoch [1/10] Batch 181/1583                       Loss D: 0.0001, loss G: 9.5338\n",
      "Epoch [1/10] Batch 182/1583                       Loss D: 0.0001, loss G: 9.5330\n",
      "Epoch [1/10] Batch 183/1583                       Loss D: 0.0001, loss G: 9.5311\n",
      "Epoch [1/10] Batch 184/1583                       Loss D: 0.0001, loss G: 9.5306\n",
      "Epoch [1/10] Batch 185/1583                       Loss D: 0.0001, loss G: 9.5303\n",
      "Epoch [1/10] Batch 186/1583                       Loss D: 0.0001, loss G: 9.5250\n",
      "Epoch [1/10] Batch 187/1583                       Loss D: 0.0001, loss G: 9.5242\n",
      "Epoch [1/10] Batch 188/1583                       Loss D: 0.0001, loss G: 9.5214\n",
      "Epoch [1/10] Batch 189/1583                       Loss D: 0.0001, loss G: 9.5228\n",
      "Epoch [1/10] Batch 190/1583                       Loss D: 0.0001, loss G: 9.5249\n",
      "Epoch [1/10] Batch 191/1583                       Loss D: 0.0001, loss G: 9.5247\n",
      "Epoch [1/10] Batch 192/1583                       Loss D: 0.0001, loss G: 9.5190\n",
      "Epoch [1/10] Batch 193/1583                       Loss D: 0.0001, loss G: 9.5123\n",
      "Epoch [1/10] Batch 194/1583                       Loss D: 0.0001, loss G: 9.5098\n",
      "Epoch [1/10] Batch 195/1583                       Loss D: 0.0001, loss G: 9.5092\n",
      "Epoch [1/10] Batch 196/1583                       Loss D: 0.0001, loss G: 9.5135\n",
      "Epoch [1/10] Batch 197/1583                       Loss D: 0.0001, loss G: 9.5222\n",
      "Epoch [1/10] Batch 198/1583                       Loss D: 0.0001, loss G: 9.5304\n",
      "Epoch [1/10] Batch 199/1583                       Loss D: 0.0001, loss G: 9.5291\n",
      "Epoch [1/10] Batch 200/1583                       Loss D: 0.0001, loss G: 9.5241\n",
      "Epoch [1/10] Batch 201/1583                       Loss D: 0.0001, loss G: 9.5197\n",
      "Epoch [1/10] Batch 202/1583                       Loss D: 0.0001, loss G: 9.5158\n",
      "Epoch [1/10] Batch 203/1583                       Loss D: 0.0001, loss G: 9.5164\n",
      "Epoch [1/10] Batch 204/1583                       Loss D: 0.0001, loss G: 9.5146\n",
      "Epoch [1/10] Batch 205/1583                       Loss D: 0.0001, loss G: 9.5141\n",
      "Epoch [1/10] Batch 206/1583                       Loss D: 0.0001, loss G: 9.5155\n",
      "Epoch [1/10] Batch 207/1583                       Loss D: 0.0001, loss G: 9.5200\n",
      "Epoch [1/10] Batch 208/1583                       Loss D: 0.0001, loss G: 9.5205\n",
      "Epoch [1/10] Batch 209/1583                       Loss D: 0.0001, loss G: 9.5164\n",
      "Epoch [1/10] Batch 210/1583                       Loss D: 0.0001, loss G: 9.5117\n",
      "Epoch [1/10] Batch 211/1583                       Loss D: 0.0001, loss G: 9.5130\n",
      "Epoch [1/10] Batch 212/1583                       Loss D: 0.0001, loss G: 9.5182\n",
      "Epoch [1/10] Batch 213/1583                       Loss D: 0.0001, loss G: 9.5233\n",
      "Epoch [1/10] Batch 214/1583                       Loss D: 0.0001, loss G: 9.5248\n",
      "Epoch [1/10] Batch 215/1583                       Loss D: 0.0001, loss G: 9.5240\n",
      "Epoch [1/10] Batch 216/1583                       Loss D: 0.0001, loss G: 9.5204\n",
      "Epoch [1/10] Batch 217/1583                       Loss D: 0.0001, loss G: 9.5225\n",
      "Epoch [1/10] Batch 218/1583                       Loss D: 0.0001, loss G: 9.5243\n",
      "Epoch [1/10] Batch 219/1583                       Loss D: 0.0001, loss G: 9.5251\n",
      "Epoch [1/10] Batch 220/1583                       Loss D: 0.0001, loss G: 9.5172\n",
      "Epoch [1/10] Batch 221/1583                       Loss D: 0.0001, loss G: 9.5141\n",
      "Epoch [1/10] Batch 222/1583                       Loss D: 0.0001, loss G: 9.5182\n",
      "Epoch [1/10] Batch 223/1583                       Loss D: 0.0001, loss G: 9.5243\n",
      "Epoch [1/10] Batch 224/1583                       Loss D: 0.0001, loss G: 9.5296\n",
      "Epoch [1/10] Batch 225/1583                       Loss D: 0.0001, loss G: 9.5377\n",
      "Epoch [1/10] Batch 226/1583                       Loss D: 0.0001, loss G: 9.5322\n",
      "Epoch [1/10] Batch 227/1583                       Loss D: 0.0001, loss G: 9.5268\n",
      "Epoch [1/10] Batch 228/1583                       Loss D: 0.0001, loss G: 9.5202\n",
      "Epoch [1/10] Batch 229/1583                       Loss D: 0.0001, loss G: 9.5199\n",
      "Epoch [1/10] Batch 230/1583                       Loss D: 0.0001, loss G: 9.5240\n",
      "Epoch [1/10] Batch 231/1583                       Loss D: 0.0001, loss G: 9.5281\n",
      "Epoch [1/10] Batch 232/1583                       Loss D: 0.0001, loss G: 9.5320\n",
      "Epoch [1/10] Batch 233/1583                       Loss D: 0.0001, loss G: 9.5352\n",
      "Epoch [1/10] Batch 234/1583                       Loss D: 0.0001, loss G: 9.5313\n",
      "Epoch [1/10] Batch 235/1583                       Loss D: 0.0001, loss G: 9.5287\n",
      "Epoch [1/10] Batch 236/1583                       Loss D: 0.0001, loss G: 9.5335\n",
      "Epoch [1/10] Batch 237/1583                       Loss D: 0.0001, loss G: 9.5448\n",
      "Epoch [1/10] Batch 238/1583                       Loss D: 0.0001, loss G: 9.5484\n",
      "Epoch [1/10] Batch 239/1583                       Loss D: 0.0001, loss G: 9.5507\n",
      "Epoch [1/10] Batch 240/1583                       Loss D: 0.0001, loss G: 9.5415\n",
      "Epoch [1/10] Batch 241/1583                       Loss D: 0.0001, loss G: 9.5390\n",
      "Epoch [1/10] Batch 242/1583                       Loss D: 0.0001, loss G: 9.5368\n",
      "Epoch [1/10] Batch 243/1583                       Loss D: 0.0001, loss G: 9.5374\n",
      "Epoch [1/10] Batch 244/1583                       Loss D: 0.0001, loss G: 9.5397\n",
      "Epoch [1/10] Batch 245/1583                       Loss D: 0.0001, loss G: 9.5466\n",
      "Epoch [1/10] Batch 246/1583                       Loss D: 0.0001, loss G: 9.5534\n",
      "Epoch [1/10] Batch 247/1583                       Loss D: 0.0001, loss G: 9.5543\n",
      "Epoch [1/10] Batch 248/1583                       Loss D: 0.0001, loss G: 9.5565\n",
      "Epoch [1/10] Batch 249/1583                       Loss D: 0.0001, loss G: 9.5544\n",
      "Epoch [1/10] Batch 250/1583                       Loss D: 0.0001, loss G: 9.5554\n",
      "Epoch [1/10] Batch 251/1583                       Loss D: 0.0001, loss G: 9.5494\n",
      "Epoch [1/10] Batch 252/1583                       Loss D: 0.0001, loss G: 9.5489\n",
      "Epoch [1/10] Batch 253/1583                       Loss D: 0.0001, loss G: 9.5502\n",
      "Epoch [1/10] Batch 254/1583                       Loss D: 0.0001, loss G: 9.5537\n",
      "Epoch [1/10] Batch 255/1583                       Loss D: 0.0001, loss G: 9.5578\n",
      "Epoch [1/10] Batch 256/1583                       Loss D: 0.0001, loss G: 9.5669\n",
      "Epoch [1/10] Batch 257/1583                       Loss D: 0.0001, loss G: 9.5673\n",
      "Epoch [1/10] Batch 258/1583                       Loss D: 0.0001, loss G: 9.5707\n",
      "Epoch [1/10] Batch 259/1583                       Loss D: 0.0001, loss G: 9.5678\n",
      "Epoch [1/10] Batch 260/1583                       Loss D: 0.0001, loss G: 9.5687\n",
      "Epoch [1/10] Batch 261/1583                       Loss D: 0.0001, loss G: 9.5696\n",
      "Epoch [1/10] Batch 262/1583                       Loss D: 0.0001, loss G: 9.5698\n",
      "Epoch [1/10] Batch 263/1583                       Loss D: 0.0001, loss G: 9.5710\n",
      "Epoch [1/10] Batch 264/1583                       Loss D: 0.0001, loss G: 9.5720\n",
      "Epoch [1/10] Batch 265/1583                       Loss D: 0.0001, loss G: 9.5721\n",
      "Epoch [1/10] Batch 266/1583                       Loss D: 0.0001, loss G: 9.5734\n",
      "Epoch [1/10] Batch 267/1583                       Loss D: 0.0001, loss G: 9.5790\n",
      "Epoch [1/10] Batch 268/1583                       Loss D: 0.0001, loss G: 9.5818\n",
      "Epoch [1/10] Batch 269/1583                       Loss D: 0.0001, loss G: 9.5804\n",
      "Epoch [1/10] Batch 270/1583                       Loss D: 0.0001, loss G: 9.5775\n",
      "Epoch [1/10] Batch 271/1583                       Loss D: 0.0001, loss G: 9.5756\n",
      "Epoch [1/10] Batch 272/1583                       Loss D: 0.0001, loss G: 9.5759\n",
      "Epoch [1/10] Batch 273/1583                       Loss D: 0.0001, loss G: 9.5824\n",
      "Epoch [1/10] Batch 274/1583                       Loss D: 0.0001, loss G: 9.5904\n",
      "Epoch [1/10] Batch 275/1583                       Loss D: 0.0001, loss G: 9.5959\n",
      "Epoch [1/10] Batch 276/1583                       Loss D: 0.0001, loss G: 9.6010\n",
      "Epoch [1/10] Batch 277/1583                       Loss D: 0.0001, loss G: 9.6001\n",
      "Epoch [1/10] Batch 278/1583                       Loss D: 0.0001, loss G: 9.5980\n",
      "Epoch [1/10] Batch 279/1583                       Loss D: 0.0001, loss G: 9.5986\n",
      "Epoch [1/10] Batch 280/1583                       Loss D: 0.0001, loss G: 9.6061\n",
      "Epoch [1/10] Batch 281/1583                       Loss D: 0.0001, loss G: 9.6100\n",
      "Epoch [1/10] Batch 282/1583                       Loss D: 0.0001, loss G: 9.6153\n",
      "Epoch [1/10] Batch 283/1583                       Loss D: 0.0001, loss G: 9.6141\n",
      "Epoch [1/10] Batch 284/1583                       Loss D: 0.0001, loss G: 9.6184\n",
      "Epoch [1/10] Batch 285/1583                       Loss D: 0.0001, loss G: 9.6177\n",
      "Epoch [1/10] Batch 286/1583                       Loss D: 0.0001, loss G: 9.6208\n",
      "Epoch [1/10] Batch 287/1583                       Loss D: 0.0001, loss G: 9.6230\n",
      "Epoch [1/10] Batch 288/1583                       Loss D: 0.0001, loss G: 9.6262\n",
      "Epoch [1/10] Batch 289/1583                       Loss D: 0.0001, loss G: 9.6268\n",
      "Epoch [1/10] Batch 290/1583                       Loss D: 0.0001, loss G: 9.6273\n",
      "Epoch [1/10] Batch 291/1583                       Loss D: 0.0001, loss G: 9.6325\n",
      "Epoch [1/10] Batch 292/1583                       Loss D: 0.0001, loss G: 9.6355\n",
      "Epoch [1/10] Batch 293/1583                       Loss D: 0.0001, loss G: 9.6410\n",
      "Epoch [1/10] Batch 294/1583                       Loss D: 0.0001, loss G: 9.6415\n",
      "Epoch [1/10] Batch 295/1583                       Loss D: 0.0001, loss G: 9.6448\n",
      "Epoch [1/10] Batch 296/1583                       Loss D: 0.0001, loss G: 9.6435\n",
      "Epoch [1/10] Batch 297/1583                       Loss D: 0.0001, loss G: 9.6457\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for idx, (real, _) in enumerate(loader):\n",
    "        real = real.to(DEVICE)\n",
    "        z_noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1, device=DEVICE)\n",
    "\n",
    "        \"\"\"\n",
    "        Train Discriminator\n",
    "        -> maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        \"\"\"\n",
    "        real_d = d(real).reshape(-1)\n",
    "        loss_real_d = criteon(real_d, torch.ones_like(real_d))  # log(D(x)) + 0\n",
    "        fake = g(z_noise)\n",
    "        fake_d = d(fake).reshape(-1)\n",
    "        loss_fake_d = criteon(fake_d, torch.zeros_like(fake_d)) # 0 + log(1 - D(G(z)))\n",
    "        loss_d = loss_real_d + loss_fake_d # log(D(x)) + log(1 - D(G(z)))\n",
    "        d_optim.zero_grad()\n",
    "        loss_d.backward(retain_graph=True)\n",
    "        d_optim.step()\n",
    "\n",
    "        \"\"\"\n",
    "        Train Discriminator\n",
    "        -> minimize log(1 - D(G(z))) but leads to vanishing gradient problem\n",
    "        --> the workaround is to maximize log(D(G(z)))\n",
    "        \"\"\"\n",
    "        out = d(fake).reshape(-1)\n",
    "        loss_g = criteon(out, torch.ones_like(out))\n",
    "        g_optim.zero_grad()\n",
    "        loss_g.backward()\n",
    "        g_optim.step()\n",
    "\n",
    "        # Logging\n",
    "        if idx == 0:\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = g(fixed_noise)\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:32], normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:32], normalize=True)\n",
    "\n",
    "                sum_writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                sum_writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1        \n",
    "        print(\n",
    "            f\"Epoch [{epoch}/{EPOCH}] Batch {idx}/{len(loader)} \\\n",
    "                      Loss D: {loss_d:.4f}, loss G: {loss_g:.4f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wah](hasil_1_epoch.png)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2358c7718a06ce74613d3ca23af7963e3dc3a98263e194a13e6cf112cb830610"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('DataScience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
